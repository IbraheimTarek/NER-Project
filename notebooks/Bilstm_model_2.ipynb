{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    # Extract tokens: parentheses or sequences of non-whitespace, non-parenthesis characters.\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    # Parse tokens into a nested list structure\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "\n",
    "def normalize_structure(tree):\n",
    "    if not isinstance(tree, list):\n",
    "        return None\n",
    "\n",
    "    def is_key(token):\n",
    "        return token in [\n",
    "            \"ORDER\", \"PIZZAORDER\", \"DRINKORDER\", \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\",\n",
    "            \"COMPLEX_TOPPING\", \"QUANTITY\", \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"NOT\"\n",
    "        ]\n",
    "\n",
    "    # Clean the list by keeping sublists and tokens as-is for further analysis\n",
    "    cleaned = []\n",
    "    for el in tree:\n",
    "        cleaned.append(el)\n",
    "\n",
    "    if len(cleaned) > 0 and isinstance(cleaned[0], str) and is_key(cleaned[0]):\n",
    "        key = cleaned[0]\n",
    "        if key == \"ORDER\":\n",
    "            pizzaorders = []\n",
    "            drinkorders = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    if \"PIZZAORDER\" in node:\n",
    "                        if isinstance(node[\"PIZZAORDER\"], list):\n",
    "                            pizzaorders.extend(node[\"PIZZAORDER\"])\n",
    "                        else:\n",
    "                            pizzaorders.append(node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in node:\n",
    "                        if isinstance(node[\"DRINKORDER\"], list):\n",
    "                            drinkorders.extend(node[\"DRINKORDER\"])\n",
    "                        else:\n",
    "                            drinkorders.append(node[\"DRINKORDER\"])\n",
    "                    if node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                        pizzaorders.append(node)\n",
    "                    if node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                        drinkorders.append(node)\n",
    "            result = {}\n",
    "            if pizzaorders:\n",
    "                result[\"PIZZAORDER\"] = pizzaorders\n",
    "            if drinkorders:\n",
    "                result[\"DRINKORDER\"] = drinkorders\n",
    "            if result:\n",
    "                return {\"ORDER\": result}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "        elif key == \"PIZZAORDER\":\n",
    "            number = None\n",
    "            size = None\n",
    "            style = None\n",
    "            toppings = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"SIZE\":\n",
    "                        size = node[\"VALUE\"]\n",
    "                    elif t == \"STYLE\":\n",
    "                        style = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        toppings.append(node)\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if size is not None:\n",
    "                result[\"SIZE\"] = size\n",
    "            if style is not None:\n",
    "                result[\"STYLE\"] = style\n",
    "            if toppings:\n",
    "                result[\"AllTopping\"] = toppings\n",
    "            # Mark type internally, will remove later\n",
    "            result[\"TYPE\"] = \"PIZZAORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key == \"DRINKORDER\":\n",
    "            number = None\n",
    "            volume = None\n",
    "            drinktype = None\n",
    "            containertype = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"VOLUME\":\n",
    "                        volume = node[\"VALUE\"]\n",
    "                    elif t == \"DRINKTYPE\":\n",
    "                        drinktype = node[\"VALUE\"]\n",
    "                    elif t == \"CONTAINERTYPE\":\n",
    "                        containertype = node[\"VALUE\"]\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if volume is not None:\n",
    "                result[\"VOLUME\"] = volume\n",
    "            if drinktype is not None:\n",
    "                result[\"DRINKTYPE\"] = drinktype\n",
    "            if containertype is not None:\n",
    "                result[\"CONTAINERTYPE\"] = containertype\n",
    "            result[\"TYPE\"] = \"DRINKORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key in [\"NUMBER\",\"SIZE\",\"STYLE\",\"VOLUME\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"QUANTITY\"]:\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            value_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": key,\n",
    "                \"VALUE\": value_str\n",
    "            }\n",
    "\n",
    "        elif key == \"TOPPING\":\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            topping_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": None,\n",
    "                \"Topping\": topping_str\n",
    "            }\n",
    "\n",
    "        elif key == \"COMPLEX_TOPPING\":\n",
    "            quantity = None\n",
    "            topping = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"QUANTITY\":\n",
    "                        quantity = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        topping = node[\"Topping\"]\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": quantity,\n",
    "                \"Topping\": topping\n",
    "            }\n",
    "\n",
    "        elif key == \"NOT\":\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict) and node.get(\"TYPE\") == \"TOPPING\":\n",
    "                    node[\"NOT\"] = True\n",
    "                    if \"Quantity\" not in node:\n",
    "                        node[\"Quantity\"] = None\n",
    "                    return node\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Try to parse sublists and combine orders found\n",
    "        combined_order = {\"PIZZAORDER\": [], \"DRINKORDER\": []}\n",
    "        found_order = False\n",
    "\n",
    "        for el in cleaned:\n",
    "            node = normalize_structure(el)\n",
    "            if isinstance(node, dict):\n",
    "                if \"ORDER\" in node:\n",
    "                    found_order = True\n",
    "                    order_node = node[\"ORDER\"]\n",
    "                    if \"PIZZAORDER\" in order_node:\n",
    "                        combined_order[\"PIZZAORDER\"].extend(order_node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in order_node:\n",
    "                        combined_order[\"DRINKORDER\"].extend(order_node[\"DRINKORDER\"])\n",
    "                elif node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"PIZZAORDER\"].append(node)\n",
    "                elif node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"DRINKORDER\"].append(node)\n",
    "\n",
    "        if found_order:\n",
    "            final = {}\n",
    "            if combined_order[\"PIZZAORDER\"]:\n",
    "                final[\"PIZZAORDER\"] = combined_order[\"PIZZAORDER\"]\n",
    "            if combined_order[\"DRINKORDER\"]:\n",
    "                final[\"DRINKORDER\"] = combined_order[\"DRINKORDER\"]\n",
    "            return {\"ORDER\": final} if final else {}\n",
    "\n",
    "        return None\n",
    "\n",
    "def remove_type_keys(obj):\n",
    "    # Recursively remove \"TYPE\" keys from all dictionaries\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(\"TYPE\", None)\n",
    "        for k, v in obj.items():\n",
    "            remove_type_keys(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            remove_type_keys(item)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenize(text)\n",
    "    parsed = parse_tokens(tokens)\n",
    "    result = normalize_structure(parsed)\n",
    "    remove_type_keys(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=(dropout if n_layers>1 else 0), batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded) \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=(dropout if n_layers>1 else 0), batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch] single token\n",
    "        input = input.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input)) # [batch, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell)) # output: [batch, 1, hid_dim]\n",
    "        prediction = self.fc_out(output.squeeze(1)) # [batch, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # tgt: [batch, tgt_len]\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = tgt[:,0]  # first token <sos>\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            # Teacher forcing\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_freq=2, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]):\n",
    "    word_freq = {}\n",
    "    for text in texts:\n",
    "        for tok in text.split():\n",
    "            word_freq[tok] = word_freq.get(tok, 0) + 1\n",
    "    # Build vocab\n",
    "    vocab = special_tokens[:]\n",
    "    for w, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):\n",
    "        if freq >= min_freq and w not in special_tokens:\n",
    "            vocab.append(w)\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def numericalize(text, word2idx, max_len, sos_token=\"<sos>\", eos_token=\"<eos>\", pad_token=\"<pad>\"):\n",
    "    tokens = text.split()\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    # truncate or pad\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "        tokens[-1] = eos_token  # ensure ends with EOS\n",
    "    else:\n",
    "        tokens = tokens + [pad_token] * (max_len - len(tokens))\n",
    "    indices = [word2idx.get(t, word2idx[\"<unk>\"]) for t in tokens]\n",
    "    return indices\n",
    "\n",
    "def collate_fn(batch, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    src_indices = [numericalize(s, src_word2idx, max_src_len) for s in srcs]\n",
    "    tgt_indices = [numericalize(t, tgt_word2idx, max_tgt_len) for t in tgts]\n",
    "    \n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "    tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "    \n",
    "    return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for src, tgt in progress_bar:\n",
    "        src = src.to(model.device)\n",
    "        tgt = tgt.to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:].contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.3f}\"})\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in progress_bar:\n",
    "            src = src.to(model.device)\n",
    "            tgt = tgt.to(model.device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:,1:].contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": f\"{loss.item():.3f}\"})\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_output(model, src_tokens, tgt_word2idx, idx2tgt):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tokens)\n",
    "        sos_idx = tgt_word2idx[\"<sos>\"]\n",
    "        eos_idx = tgt_word2idx[\"<eos>\"]\n",
    "        input_token = torch.tensor([sos_idx], device=model.device)\n",
    "        decoded_tokens = []\n",
    "        max_tgt_len = src_tokens.shape[1]*2  # heuristic limit\n",
    "\n",
    "        for _ in range(max_tgt_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == eos_idx:\n",
    "                break\n",
    "            decoded_tokens.append(top1.item())\n",
    "            input_token = top1\n",
    "        \n",
    "        decoded_text = \" \".join([idx2tgt[i] for i in decoded_tokens])\n",
    "        return decoded_text\n",
    "\n",
    "def exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, idx2tgt, max_src_len, batch_size=32):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for srcs, tgts in dev_loader:\n",
    "        # Convert srcs to tensors\n",
    "        src_indices = [numericalize(s, src_word2idx, max_src_len) for s in srcs]\n",
    "        src_tensor = torch.tensor(src_indices, dtype=torch.long, device=model.device)\n",
    "\n",
    "        # Decode each sample in the batch\n",
    "        for i in range(len(srcs)):\n",
    "            single_src = src_tensor[i].unsqueeze(0)  # [1, max_src_len]\n",
    "            predicted_text = generate_output(model, single_src, tgt_word2idx, idx2tgt)\n",
    "            # Convert both predicted and reference TOP into JSON using preprocess\n",
    "            pred_json = preprocess(predicted_text)\n",
    "            ref_json = preprocess(tgts[i])\n",
    "            if pred_json == ref_json:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, jsonl_path,SRC_NAME, TOP_NAME, max_samples=None, max_src_len=128, max_tgt_len=256):\n",
    "        self.src_texts = []\n",
    "        self.tgt_texts = []\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        \n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples is not None and i >= max_samples:\n",
    "                    break\n",
    "                entry = json.loads(line.strip())\n",
    "                src = entry.get(SRC_NAME, \"\").strip()\n",
    "                tgt = entry.get(TOP_NAME, \"\").strip()\n",
    "                if src and tgt:\n",
    "                    self.src_texts.append(src)\n",
    "                    self.tgt_texts.append(tgt)\n",
    "                    \n",
    "        # Shuffle is not applied here for dev datasets.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_texts[idx], self.tgt_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsonl_path = \"../dataset/PIZZA_train.json\"  \n",
    "dev_jsonl_path = \"../dataset/PIZZA_dev.json\"      \n",
    "\n",
    "train_dataset = PizzaDataset(train_jsonl_path,\"train.SRC\", \"train.TOP-DECOUPLED\", max_samples=2500000) \n",
    "dev_dataset = PizzaDataset(dev_jsonl_path,\"dev.SRC\", \"dev.TOP\", max_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('large pie with green pepper and with extra peperonni',\n",
       " '(ORDER (PIZZAORDER (SIZE large ) (TOPPING green pepper ) (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING peperonni ) ) ) )')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2456446, 348)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset), len(dev_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies from train data\n",
    "src_word2idx, src_idx2word = build_vocab(train_dataset.src_texts, min_freq=1)\n",
    "tgt_word2idx, tgt_idx2word = build_vocab(train_dataset.tgt_texts, min_freq=1)\n",
    "\n",
    "pad_idx = tgt_word2idx[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0,\n",
       "  '<sos>': 1,\n",
       "  '<eos>': 2,\n",
       "  '<unk>': 3,\n",
       "  'and': 4,\n",
       "  'with': 5,\n",
       "  'a': 6,\n",
       "  'three': 7,\n",
       "  'pizzas': 8,\n",
       "  'pizza': 9,\n",
       "  \"i'd\": 10,\n",
       "  'like': 11,\n",
       "  'cheese': 12,\n",
       "  'four': 13,\n",
       "  'pies': 14,\n",
       "  'party': 15,\n",
       "  'five': 16,\n",
       "  'american': 17,\n",
       "  'sized': 18,\n",
       "  'one': 19,\n",
       "  'no': 20,\n",
       "  'of': 21,\n",
       "  'two': 22,\n",
       "  'i': 23,\n",
       "  'size': 24,\n",
       "  'sprite': 25,\n",
       "  'pepper': 26,\n",
       "  'glaze': 27,\n",
       "  'without': 28,\n",
       "  'ice': 29,\n",
       "  '-': 30,\n",
       "  'large': 31,\n",
       "  'balsamic': 32,\n",
       "  'peppers': 33,\n",
       "  'ounce': 34,\n",
       "  'pie': 35,\n",
       "  'crust': 36,\n",
       "  'tea': 37,\n",
       "  'thin': 38,\n",
       "  'sauce': 39,\n",
       "  'ups': 40,\n",
       "  'extra': 41,\n",
       "  'diet': 42,\n",
       "  'green': 43,\n",
       "  'seven': 44,\n",
       "  'medium': 45,\n",
       "  'also': 46,\n",
       "  'personal': 47,\n",
       "  'roasted': 48,\n",
       "  'red': 49,\n",
       "  'teas': 50,\n",
       "  'ginger': 51,\n",
       "  'pecorino': 52,\n",
       "  'peperonni': 53,\n",
       "  'cans': 54,\n",
       "  'chicken': 55,\n",
       "  'banana': 56,\n",
       "  'need': 57,\n",
       "  'fantas': 58,\n",
       "  'little': 59,\n",
       "  'ale': 60,\n",
       "  'lunch': 61,\n",
       "  'bottle': 62,\n",
       "  'any': 63,\n",
       "  '500': 64,\n",
       "  'sprites': 65,\n",
       "  '20': 66,\n",
       "  'coke': 67,\n",
       "  'can': 68,\n",
       "  'mozzarella': 69,\n",
       "  'onions': 70,\n",
       "  'want': 71,\n",
       "  'the': 72,\n",
       "  'onion': 73,\n",
       "  'have': 74,\n",
       "  'bit': 75,\n",
       "  'milliliter': 76,\n",
       "  'hold': 77,\n",
       "  'olive': 78,\n",
       "  'just': 79,\n",
       "  'pellegrino': 80,\n",
       "  'pineapple': 81,\n",
       "  'san': 82,\n",
       "  'regular': 83,\n",
       "  'fl': 84,\n",
       "  'liter': 85,\n",
       "  'yellow': 86,\n",
       "  'tomato': 87,\n",
       "  'caramelized': 88,\n",
       "  'ml': 89,\n",
       "  'iced': 90,\n",
       "  'not': 91,\n",
       "  'olives': 92,\n",
       "  'buffalo': 93,\n",
       "  'dried': 94,\n",
       "  'pellegrinos': 95,\n",
       "  'tomatoes': 96,\n",
       "  'eight': 97,\n",
       "  'dr': 98,\n",
       "  'lot': 99,\n",
       "  'fluid': 100,\n",
       "  'pepperoni': 101,\n",
       "  'in': 102,\n",
       "  'small': 103,\n",
       "  'bacon': 104,\n",
       "  '200': 105,\n",
       "  'bbq': 106,\n",
       "  'mountain': 107,\n",
       "  'garlic': 108,\n",
       "  'lemon': 109,\n",
       "  'barbecue': 110,\n",
       "  'cherry': 111,\n",
       "  '12': 112,\n",
       "  'meatball': 113,\n",
       "  'sausage': 114,\n",
       "  'pesto': 115,\n",
       "  '8': 116,\n",
       "  '16': 117,\n",
       "  'waters': 118,\n",
       "  'pepsis': 119,\n",
       "  'jalapeno': 120,\n",
       "  'much': 121,\n",
       "  'pestos': 122,\n",
       "  'fried': 123,\n",
       "  'vegan': 124,\n",
       "  'kalamata': 125,\n",
       "  '7': 126,\n",
       "  'only': 127,\n",
       "  'peppperoni': 128,\n",
       "  'fanta': 129,\n",
       "  'feta': 130,\n",
       "  'zeros': 131,\n",
       "  'mozarella': 132,\n",
       "  'doctor': 133,\n",
       "  'pulled': 134,\n",
       "  'pork': 135,\n",
       "  'beef': 136,\n",
       "  'grilled': 137,\n",
       "  'hot': 138,\n",
       "  'mushrooms': 139,\n",
       "  'many': 140,\n",
       "  'pepsi': 141,\n",
       "  'parmesan': 142,\n",
       "  'crusts': 143,\n",
       "  'sodas': 144,\n",
       "  'low': 145,\n",
       "  'fat': 146,\n",
       "  'stuffed': 147,\n",
       "  'peper': 148,\n",
       "  'lots': 149,\n",
       "  'perriers': 150,\n",
       "  'ricotta': 151,\n",
       "  'pepers': 152,\n",
       "  'dews': 153,\n",
       "  'pickles': 154,\n",
       "  'white': 155,\n",
       "  'tiny': 156,\n",
       "  'soda': 157,\n",
       "  'high': 158,\n",
       "  'rise': 159,\n",
       "  'dough': 160,\n",
       "  'zeroes': 161,\n",
       "  'peperronni': 162,\n",
       "  'arugula': 163,\n",
       "  'dew': 164,\n",
       "  'black': 165,\n",
       "  'balzamic': 166,\n",
       "  'ales': 167,\n",
       "  'perrier': 168,\n",
       "  'ranch': 169,\n",
       "  '500-ml': 170,\n",
       "  'ham': 171,\n",
       "  'hams': 172,\n",
       "  'oz': 173,\n",
       "  'artichoke': 174,\n",
       "  'hate': 175,\n",
       "  'bay': 176,\n",
       "  'leaves': 177,\n",
       "  'alfredo': 178,\n",
       "  'oregano': 179,\n",
       "  'broccoli': 180,\n",
       "  '500-milliliter': 181,\n",
       "  'artichokes': 182,\n",
       "  'bottles': 183,\n",
       "  'carrots': 184,\n",
       "  'two-liter': 185,\n",
       "  '200-milliliter': 186,\n",
       "  'three-liter': 187,\n",
       "  'powder': 188,\n",
       "  'mushroom': 189,\n",
       "  'peperoni': 190,\n",
       "  'cheddar': 191,\n",
       "  'basil': 192,\n",
       "  'sixteen': 193,\n",
       "  'applewood': 194,\n",
       "  'avoid': 195,\n",
       "  'anchovy': 196,\n",
       "  'chickens': 197,\n",
       "  'cumin': 198,\n",
       "  'apple': 199,\n",
       "  'wood': 200,\n",
       "  'carrot': 201,\n",
       "  'meat': 202,\n",
       "  'peperroni': 203,\n",
       "  'chorizo': 204,\n",
       "  'ground': 205,\n",
       "  'anchovies': 206,\n",
       "  'cheeseburger': 207,\n",
       "  'pickle': 208,\n",
       "  'chorrizo': 209,\n",
       "  'bean': 210,\n",
       "  'brocoli': 211,\n",
       "  'bacons': 212,\n",
       "  'beans': 213,\n",
       "  'big': 214,\n",
       "  'spiced': 215,\n",
       "  'peppperonis': 216,\n",
       "  'tuna': 217,\n",
       "  'spicy': 218,\n",
       "  'salami': 219,\n",
       "  'spinach': 220,\n",
       "  'pepperonis': 221,\n",
       "  'lettuce': 222,\n",
       "  'shrimps': 223,\n",
       "  'italian': 224,\n",
       "  'flakes': 225,\n",
       "  'shrimp': 226,\n",
       "  'rosemary': 227,\n",
       "  'flake': 228,\n",
       "  'pineaples': 229,\n",
       "  'pineapples': 230,\n",
       "  'meatlover': 231,\n",
       "  'pineaple': 232,\n",
       "  'jalapenos': 233,\n",
       "  'peperonis': 234,\n",
       "  'pea': 235,\n",
       "  'parsley': 236,\n",
       "  'peas': 237,\n",
       "  'oil': 238,\n",
       "  'all': 239,\n",
       "  'up': 240,\n",
       "  'thick': 241,\n",
       "  'water': 242,\n",
       "  'new': 243,\n",
       "  'sausages': 244,\n",
       "  'meatballs': 245,\n",
       "  'tunas': 246,\n",
       "  '4': 247,\n",
       "  '14': 248,\n",
       "  'fifteen': 249,\n",
       "  '3': 250,\n",
       "  '13': 251,\n",
       "  '5': 252,\n",
       "  'twelve': 253,\n",
       "  'six': 254,\n",
       "  'ten': 255,\n",
       "  'thirteen': 256,\n",
       "  '2': 257,\n",
       "  '11': 258,\n",
       "  'eleven': 259,\n",
       "  '15': 260,\n",
       "  '9': 261,\n",
       "  '6': 262,\n",
       "  'nine': 263,\n",
       "  'fourteen': 264,\n",
       "  '10': 265,\n",
       "  '1': 266,\n",
       "  'an': 267,\n",
       "  'yorker': 268,\n",
       "  'cokes': 269,\n",
       "  'gluten': 270,\n",
       "  'free': 271,\n",
       "  'sourdough': 272,\n",
       "  'keto': 273,\n",
       "  'combination': 274,\n",
       "  'style': 275,\n",
       "  'chicago': 276,\n",
       "  'lover': 277,\n",
       "  'every': 278,\n",
       "  'lovers': 279,\n",
       "  'cauliflower': 280,\n",
       "  'gluten-free': 281,\n",
       "  'everything': 282,\n",
       "  'deepdish': 283,\n",
       "  'vegetarian': 284,\n",
       "  'zero': 285,\n",
       "  'napolitana': 286,\n",
       "  'margarita': 287,\n",
       "  'neapolitan': 288,\n",
       "  'york': 289,\n",
       "  'veggies': 290,\n",
       "  'meatlovers': 291,\n",
       "  'veggie': 292,\n",
       "  'topping': 293,\n",
       "  'works': 294,\n",
       "  'supreme': 295,\n",
       "  'hawaiian': 296,\n",
       "  'margherita': 297,\n",
       "  'deep': 298,\n",
       "  'dish': 299,\n",
       "  'mexican': 300,\n",
       "  'vegetables': 301,\n",
       "  'toppings': 302,\n",
       "  'mediterranean': 303,\n",
       "  'coffee': 304,\n",
       "  'coffees': 305,\n",
       "  'pan': 306,\n",
       "  'med': 307,\n",
       "  'napolitan': 308},\n",
       " {0: '<pad>',\n",
       "  1: '<sos>',\n",
       "  2: '<eos>',\n",
       "  3: '<unk>',\n",
       "  4: 'and',\n",
       "  5: 'with',\n",
       "  6: 'a',\n",
       "  7: 'three',\n",
       "  8: 'pizzas',\n",
       "  9: 'pizza',\n",
       "  10: \"i'd\",\n",
       "  11: 'like',\n",
       "  12: 'cheese',\n",
       "  13: 'four',\n",
       "  14: 'pies',\n",
       "  15: 'party',\n",
       "  16: 'five',\n",
       "  17: 'american',\n",
       "  18: 'sized',\n",
       "  19: 'one',\n",
       "  20: 'no',\n",
       "  21: 'of',\n",
       "  22: 'two',\n",
       "  23: 'i',\n",
       "  24: 'size',\n",
       "  25: 'sprite',\n",
       "  26: 'pepper',\n",
       "  27: 'glaze',\n",
       "  28: 'without',\n",
       "  29: 'ice',\n",
       "  30: '-',\n",
       "  31: 'large',\n",
       "  32: 'balsamic',\n",
       "  33: 'peppers',\n",
       "  34: 'ounce',\n",
       "  35: 'pie',\n",
       "  36: 'crust',\n",
       "  37: 'tea',\n",
       "  38: 'thin',\n",
       "  39: 'sauce',\n",
       "  40: 'ups',\n",
       "  41: 'extra',\n",
       "  42: 'diet',\n",
       "  43: 'green',\n",
       "  44: 'seven',\n",
       "  45: 'medium',\n",
       "  46: 'also',\n",
       "  47: 'personal',\n",
       "  48: 'roasted',\n",
       "  49: 'red',\n",
       "  50: 'teas',\n",
       "  51: 'ginger',\n",
       "  52: 'pecorino',\n",
       "  53: 'peperonni',\n",
       "  54: 'cans',\n",
       "  55: 'chicken',\n",
       "  56: 'banana',\n",
       "  57: 'need',\n",
       "  58: 'fantas',\n",
       "  59: 'little',\n",
       "  60: 'ale',\n",
       "  61: 'lunch',\n",
       "  62: 'bottle',\n",
       "  63: 'any',\n",
       "  64: '500',\n",
       "  65: 'sprites',\n",
       "  66: '20',\n",
       "  67: 'coke',\n",
       "  68: 'can',\n",
       "  69: 'mozzarella',\n",
       "  70: 'onions',\n",
       "  71: 'want',\n",
       "  72: 'the',\n",
       "  73: 'onion',\n",
       "  74: 'have',\n",
       "  75: 'bit',\n",
       "  76: 'milliliter',\n",
       "  77: 'hold',\n",
       "  78: 'olive',\n",
       "  79: 'just',\n",
       "  80: 'pellegrino',\n",
       "  81: 'pineapple',\n",
       "  82: 'san',\n",
       "  83: 'regular',\n",
       "  84: 'fl',\n",
       "  85: 'liter',\n",
       "  86: 'yellow',\n",
       "  87: 'tomato',\n",
       "  88: 'caramelized',\n",
       "  89: 'ml',\n",
       "  90: 'iced',\n",
       "  91: 'not',\n",
       "  92: 'olives',\n",
       "  93: 'buffalo',\n",
       "  94: 'dried',\n",
       "  95: 'pellegrinos',\n",
       "  96: 'tomatoes',\n",
       "  97: 'eight',\n",
       "  98: 'dr',\n",
       "  99: 'lot',\n",
       "  100: 'fluid',\n",
       "  101: 'pepperoni',\n",
       "  102: 'in',\n",
       "  103: 'small',\n",
       "  104: 'bacon',\n",
       "  105: '200',\n",
       "  106: 'bbq',\n",
       "  107: 'mountain',\n",
       "  108: 'garlic',\n",
       "  109: 'lemon',\n",
       "  110: 'barbecue',\n",
       "  111: 'cherry',\n",
       "  112: '12',\n",
       "  113: 'meatball',\n",
       "  114: 'sausage',\n",
       "  115: 'pesto',\n",
       "  116: '8',\n",
       "  117: '16',\n",
       "  118: 'waters',\n",
       "  119: 'pepsis',\n",
       "  120: 'jalapeno',\n",
       "  121: 'much',\n",
       "  122: 'pestos',\n",
       "  123: 'fried',\n",
       "  124: 'vegan',\n",
       "  125: 'kalamata',\n",
       "  126: '7',\n",
       "  127: 'only',\n",
       "  128: 'peppperoni',\n",
       "  129: 'fanta',\n",
       "  130: 'feta',\n",
       "  131: 'zeros',\n",
       "  132: 'mozarella',\n",
       "  133: 'doctor',\n",
       "  134: 'pulled',\n",
       "  135: 'pork',\n",
       "  136: 'beef',\n",
       "  137: 'grilled',\n",
       "  138: 'hot',\n",
       "  139: 'mushrooms',\n",
       "  140: 'many',\n",
       "  141: 'pepsi',\n",
       "  142: 'parmesan',\n",
       "  143: 'crusts',\n",
       "  144: 'sodas',\n",
       "  145: 'low',\n",
       "  146: 'fat',\n",
       "  147: 'stuffed',\n",
       "  148: 'peper',\n",
       "  149: 'lots',\n",
       "  150: 'perriers',\n",
       "  151: 'ricotta',\n",
       "  152: 'pepers',\n",
       "  153: 'dews',\n",
       "  154: 'pickles',\n",
       "  155: 'white',\n",
       "  156: 'tiny',\n",
       "  157: 'soda',\n",
       "  158: 'high',\n",
       "  159: 'rise',\n",
       "  160: 'dough',\n",
       "  161: 'zeroes',\n",
       "  162: 'peperronni',\n",
       "  163: 'arugula',\n",
       "  164: 'dew',\n",
       "  165: 'black',\n",
       "  166: 'balzamic',\n",
       "  167: 'ales',\n",
       "  168: 'perrier',\n",
       "  169: 'ranch',\n",
       "  170: '500-ml',\n",
       "  171: 'ham',\n",
       "  172: 'hams',\n",
       "  173: 'oz',\n",
       "  174: 'artichoke',\n",
       "  175: 'hate',\n",
       "  176: 'bay',\n",
       "  177: 'leaves',\n",
       "  178: 'alfredo',\n",
       "  179: 'oregano',\n",
       "  180: 'broccoli',\n",
       "  181: '500-milliliter',\n",
       "  182: 'artichokes',\n",
       "  183: 'bottles',\n",
       "  184: 'carrots',\n",
       "  185: 'two-liter',\n",
       "  186: '200-milliliter',\n",
       "  187: 'three-liter',\n",
       "  188: 'powder',\n",
       "  189: 'mushroom',\n",
       "  190: 'peperoni',\n",
       "  191: 'cheddar',\n",
       "  192: 'basil',\n",
       "  193: 'sixteen',\n",
       "  194: 'applewood',\n",
       "  195: 'avoid',\n",
       "  196: 'anchovy',\n",
       "  197: 'chickens',\n",
       "  198: 'cumin',\n",
       "  199: 'apple',\n",
       "  200: 'wood',\n",
       "  201: 'carrot',\n",
       "  202: 'meat',\n",
       "  203: 'peperroni',\n",
       "  204: 'chorizo',\n",
       "  205: 'ground',\n",
       "  206: 'anchovies',\n",
       "  207: 'cheeseburger',\n",
       "  208: 'pickle',\n",
       "  209: 'chorrizo',\n",
       "  210: 'bean',\n",
       "  211: 'brocoli',\n",
       "  212: 'bacons',\n",
       "  213: 'beans',\n",
       "  214: 'big',\n",
       "  215: 'spiced',\n",
       "  216: 'peppperonis',\n",
       "  217: 'tuna',\n",
       "  218: 'spicy',\n",
       "  219: 'salami',\n",
       "  220: 'spinach',\n",
       "  221: 'pepperonis',\n",
       "  222: 'lettuce',\n",
       "  223: 'shrimps',\n",
       "  224: 'italian',\n",
       "  225: 'flakes',\n",
       "  226: 'shrimp',\n",
       "  227: 'rosemary',\n",
       "  228: 'flake',\n",
       "  229: 'pineaples',\n",
       "  230: 'pineapples',\n",
       "  231: 'meatlover',\n",
       "  232: 'pineaple',\n",
       "  233: 'jalapenos',\n",
       "  234: 'peperonis',\n",
       "  235: 'pea',\n",
       "  236: 'parsley',\n",
       "  237: 'peas',\n",
       "  238: 'oil',\n",
       "  239: 'all',\n",
       "  240: 'up',\n",
       "  241: 'thick',\n",
       "  242: 'water',\n",
       "  243: 'new',\n",
       "  244: 'sausages',\n",
       "  245: 'meatballs',\n",
       "  246: 'tunas',\n",
       "  247: '4',\n",
       "  248: '14',\n",
       "  249: 'fifteen',\n",
       "  250: '3',\n",
       "  251: '13',\n",
       "  252: '5',\n",
       "  253: 'twelve',\n",
       "  254: 'six',\n",
       "  255: 'ten',\n",
       "  256: 'thirteen',\n",
       "  257: '2',\n",
       "  258: '11',\n",
       "  259: 'eleven',\n",
       "  260: '15',\n",
       "  261: '9',\n",
       "  262: '6',\n",
       "  263: 'nine',\n",
       "  264: 'fourteen',\n",
       "  265: '10',\n",
       "  266: '1',\n",
       "  267: 'an',\n",
       "  268: 'yorker',\n",
       "  269: 'cokes',\n",
       "  270: 'gluten',\n",
       "  271: 'free',\n",
       "  272: 'sourdough',\n",
       "  273: 'keto',\n",
       "  274: 'combination',\n",
       "  275: 'style',\n",
       "  276: 'chicago',\n",
       "  277: 'lover',\n",
       "  278: 'every',\n",
       "  279: 'lovers',\n",
       "  280: 'cauliflower',\n",
       "  281: 'gluten-free',\n",
       "  282: 'everything',\n",
       "  283: 'deepdish',\n",
       "  284: 'vegetarian',\n",
       "  285: 'zero',\n",
       "  286: 'napolitana',\n",
       "  287: 'margarita',\n",
       "  288: 'neapolitan',\n",
       "  289: 'york',\n",
       "  290: 'veggies',\n",
       "  291: 'meatlovers',\n",
       "  292: 'veggie',\n",
       "  293: 'topping',\n",
       "  294: 'works',\n",
       "  295: 'supreme',\n",
       "  296: 'hawaiian',\n",
       "  297: 'margherita',\n",
       "  298: 'deep',\n",
       "  299: 'dish',\n",
       "  300: 'mexican',\n",
       "  301: 'vegetables',\n",
       "  302: 'toppings',\n",
       "  303: 'mediterranean',\n",
       "  304: 'coffee',\n",
       "  305: 'coffees',\n",
       "  306: 'pan',\n",
       "  307: 'med',\n",
       "  308: 'napolitan'})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word2idx, src_idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0,\n",
       "  '<sos>': 1,\n",
       "  '<eos>': 2,\n",
       "  '<unk>': 3,\n",
       "  ')': 4,\n",
       "  '(NUMBER': 5,\n",
       "  '(TOPPING': 6,\n",
       "  '(PIZZAORDER': 7,\n",
       "  '(ORDER': 8,\n",
       "  '(DRINKORDER': 9,\n",
       "  '(DRINKTYPE': 10,\n",
       "  'a': 11,\n",
       "  '(SIZE': 12,\n",
       "  'three': 13,\n",
       "  '(NOT': 14,\n",
       "  'cheese': 15,\n",
       "  '(VOLUME': 16,\n",
       "  'four': 17,\n",
       "  'party': 18,\n",
       "  'five': 19,\n",
       "  'american': 20,\n",
       "  'sized': 21,\n",
       "  '(COMPLEX_TOPPING': 22,\n",
       "  '(QUANTITY': 23,\n",
       "  'one': 24,\n",
       "  '(STYLE': 25,\n",
       "  'two': 26,\n",
       "  'size': 27,\n",
       "  'sprite': 28,\n",
       "  'pepper': 29,\n",
       "  'glaze': 30,\n",
       "  'ice': 31,\n",
       "  '-': 32,\n",
       "  'large': 33,\n",
       "  'balsamic': 34,\n",
       "  'peppers': 35,\n",
       "  '(CONTAINERTYPE': 36,\n",
       "  'ounce': 37,\n",
       "  'crust': 38,\n",
       "  'tea': 39,\n",
       "  'thin': 40,\n",
       "  'sauce': 41,\n",
       "  'ups': 42,\n",
       "  'extra': 43,\n",
       "  'diet': 44,\n",
       "  'green': 45,\n",
       "  'seven': 46,\n",
       "  'medium': 47,\n",
       "  'personal': 48,\n",
       "  'roasted': 49,\n",
       "  'red': 50,\n",
       "  'teas': 51,\n",
       "  'of': 52,\n",
       "  'ginger': 53,\n",
       "  'pecorino': 54,\n",
       "  'peperonni': 55,\n",
       "  'cans': 56,\n",
       "  'chicken': 57,\n",
       "  'banana': 58,\n",
       "  'fantas': 59,\n",
       "  'little': 60,\n",
       "  'ale': 61,\n",
       "  'lunch': 62,\n",
       "  'bottle': 63,\n",
       "  '500': 64,\n",
       "  'sprites': 65,\n",
       "  '20': 66,\n",
       "  'coke': 67,\n",
       "  'mozzarella': 68,\n",
       "  'onions': 69,\n",
       "  'onion': 70,\n",
       "  'bit': 71,\n",
       "  'milliliter': 72,\n",
       "  'olive': 73,\n",
       "  'just': 74,\n",
       "  'pellegrino': 75,\n",
       "  'pineapple': 76,\n",
       "  'san': 77,\n",
       "  'regular': 78,\n",
       "  'fl': 79,\n",
       "  'liter': 80,\n",
       "  'yellow': 81,\n",
       "  'tomato': 82,\n",
       "  'caramelized': 83,\n",
       "  'ml': 84,\n",
       "  'iced': 85,\n",
       "  'not': 86,\n",
       "  'olives': 87,\n",
       "  'buffalo': 88,\n",
       "  'dried': 89,\n",
       "  'pellegrinos': 90,\n",
       "  'tomatoes': 91,\n",
       "  'eight': 92,\n",
       "  'dr': 93,\n",
       "  'lot': 94,\n",
       "  'fluid': 95,\n",
       "  'pepperoni': 96,\n",
       "  'in': 97,\n",
       "  'small': 98,\n",
       "  'bacon': 99,\n",
       "  '200': 100,\n",
       "  'bbq': 101,\n",
       "  'mountain': 102,\n",
       "  'garlic': 103,\n",
       "  'lemon': 104,\n",
       "  'barbecue': 105,\n",
       "  'cherry': 106,\n",
       "  '12': 107,\n",
       "  'meatball': 108,\n",
       "  'sausage': 109,\n",
       "  'pesto': 110,\n",
       "  '8': 111,\n",
       "  '16': 112,\n",
       "  'waters': 113,\n",
       "  'pepsis': 114,\n",
       "  'jalapeno': 115,\n",
       "  'much': 116,\n",
       "  'pestos': 117,\n",
       "  'fried': 118,\n",
       "  'vegan': 119,\n",
       "  'kalamata': 120,\n",
       "  '7': 121,\n",
       "  'only': 122,\n",
       "  'peppperoni': 123,\n",
       "  'fanta': 124,\n",
       "  'feta': 125,\n",
       "  'zeros': 126,\n",
       "  'mozarella': 127,\n",
       "  'doctor': 128,\n",
       "  'pulled': 129,\n",
       "  'pork': 130,\n",
       "  'beef': 131,\n",
       "  'grilled': 132,\n",
       "  'hot': 133,\n",
       "  'mushrooms': 134,\n",
       "  'many': 135,\n",
       "  'pepsi': 136,\n",
       "  'parmesan': 137,\n",
       "  'crusts': 138,\n",
       "  'sodas': 139,\n",
       "  'low': 140,\n",
       "  'fat': 141,\n",
       "  'stuffed': 142,\n",
       "  'peper': 143,\n",
       "  'lots': 144,\n",
       "  'perriers': 145,\n",
       "  'ricotta': 146,\n",
       "  'pepers': 147,\n",
       "  'dews': 148,\n",
       "  'pickles': 149,\n",
       "  'white': 150,\n",
       "  'tiny': 151,\n",
       "  'soda': 152,\n",
       "  'high': 153,\n",
       "  'rise': 154,\n",
       "  'dough': 155,\n",
       "  'zeroes': 156,\n",
       "  'peperronni': 157,\n",
       "  'arugula': 158,\n",
       "  'dew': 159,\n",
       "  'black': 160,\n",
       "  'balzamic': 161,\n",
       "  'ales': 162,\n",
       "  'perrier': 163,\n",
       "  'ranch': 164,\n",
       "  '500-ml': 165,\n",
       "  'ham': 166,\n",
       "  'hams': 167,\n",
       "  'oz': 168,\n",
       "  'artichoke': 169,\n",
       "  'bay': 170,\n",
       "  'leaves': 171,\n",
       "  'alfredo': 172,\n",
       "  'oregano': 173,\n",
       "  'broccoli': 174,\n",
       "  '500-milliliter': 175,\n",
       "  'artichokes': 176,\n",
       "  'bottles': 177,\n",
       "  'carrots': 178,\n",
       "  'two-liter': 179,\n",
       "  '200-milliliter': 180,\n",
       "  'three-liter': 181,\n",
       "  'powder': 182,\n",
       "  'mushroom': 183,\n",
       "  'peperoni': 184,\n",
       "  'cheddar': 185,\n",
       "  'basil': 186,\n",
       "  'sixteen': 187,\n",
       "  'applewood': 188,\n",
       "  'anchovy': 189,\n",
       "  'chickens': 190,\n",
       "  'cumin': 191,\n",
       "  'apple': 192,\n",
       "  'wood': 193,\n",
       "  'carrot': 194,\n",
       "  'meat': 195,\n",
       "  'peperroni': 196,\n",
       "  'chorizo': 197,\n",
       "  'ground': 198,\n",
       "  'anchovies': 199,\n",
       "  'cheeseburger': 200,\n",
       "  'pickle': 201,\n",
       "  'chorrizo': 202,\n",
       "  'bean': 203,\n",
       "  'brocoli': 204,\n",
       "  'bacons': 205,\n",
       "  'beans': 206,\n",
       "  'big': 207,\n",
       "  'spiced': 208,\n",
       "  'peppperonis': 209,\n",
       "  'tuna': 210,\n",
       "  'spicy': 211,\n",
       "  'salami': 212,\n",
       "  'spinach': 213,\n",
       "  'pepperonis': 214,\n",
       "  'lettuce': 215,\n",
       "  'shrimps': 216,\n",
       "  'italian': 217,\n",
       "  'flakes': 218,\n",
       "  'shrimp': 219,\n",
       "  'rosemary': 220,\n",
       "  'flake': 221,\n",
       "  'pineaples': 222,\n",
       "  'pineapples': 223,\n",
       "  'meatlover': 224,\n",
       "  'pineaple': 225,\n",
       "  'jalapenos': 226,\n",
       "  'peperonis': 227,\n",
       "  'pea': 228,\n",
       "  'parsley': 229,\n",
       "  'peas': 230,\n",
       "  'oil': 231,\n",
       "  'all': 232,\n",
       "  'can': 233,\n",
       "  'up': 234,\n",
       "  'thick': 235,\n",
       "  'water': 236,\n",
       "  'new': 237,\n",
       "  'sausages': 238,\n",
       "  'meatballs': 239,\n",
       "  'tunas': 240,\n",
       "  '4': 241,\n",
       "  '14': 242,\n",
       "  'fifteen': 243,\n",
       "  '3': 244,\n",
       "  '13': 245,\n",
       "  '5': 246,\n",
       "  'twelve': 247,\n",
       "  'six': 248,\n",
       "  'ten': 249,\n",
       "  'thirteen': 250,\n",
       "  '2': 251,\n",
       "  '11': 252,\n",
       "  'eleven': 253,\n",
       "  '15': 254,\n",
       "  '9': 255,\n",
       "  'the': 256,\n",
       "  '6': 257,\n",
       "  'nine': 258,\n",
       "  'fourteen': 259,\n",
       "  '10': 260,\n",
       "  '1': 261,\n",
       "  'an': 262,\n",
       "  'yorker': 263,\n",
       "  'cokes': 264,\n",
       "  'gluten': 265,\n",
       "  'free': 266,\n",
       "  'sourdough': 267,\n",
       "  'keto': 268,\n",
       "  'combination': 269,\n",
       "  'style': 270,\n",
       "  'chicago': 271,\n",
       "  'lover': 272,\n",
       "  'every': 273,\n",
       "  'lovers': 274,\n",
       "  'cauliflower': 275,\n",
       "  'gluten-free': 276,\n",
       "  'everything': 277,\n",
       "  'deepdish': 278,\n",
       "  'vegetarian': 279,\n",
       "  'zero': 280,\n",
       "  'napolitana': 281,\n",
       "  'margarita': 282,\n",
       "  'neapolitan': 283,\n",
       "  'york': 284,\n",
       "  'veggies': 285,\n",
       "  'meatlovers': 286,\n",
       "  'veggie': 287,\n",
       "  'topping': 288,\n",
       "  'with': 289,\n",
       "  'works': 290,\n",
       "  'supreme': 291,\n",
       "  'hawaiian': 292,\n",
       "  'margherita': 293,\n",
       "  'deep': 294,\n",
       "  'dish': 295,\n",
       "  'mexican': 296,\n",
       "  'vegetables': 297,\n",
       "  'toppings': 298,\n",
       "  'mediterranean': 299,\n",
       "  'coffee': 300,\n",
       "  'coffees': 301,\n",
       "  'pan': 302,\n",
       "  'med': 303,\n",
       "  'napolitan': 304},\n",
       " {0: '<pad>',\n",
       "  1: '<sos>',\n",
       "  2: '<eos>',\n",
       "  3: '<unk>',\n",
       "  4: ')',\n",
       "  5: '(NUMBER',\n",
       "  6: '(TOPPING',\n",
       "  7: '(PIZZAORDER',\n",
       "  8: '(ORDER',\n",
       "  9: '(DRINKORDER',\n",
       "  10: '(DRINKTYPE',\n",
       "  11: 'a',\n",
       "  12: '(SIZE',\n",
       "  13: 'three',\n",
       "  14: '(NOT',\n",
       "  15: 'cheese',\n",
       "  16: '(VOLUME',\n",
       "  17: 'four',\n",
       "  18: 'party',\n",
       "  19: 'five',\n",
       "  20: 'american',\n",
       "  21: 'sized',\n",
       "  22: '(COMPLEX_TOPPING',\n",
       "  23: '(QUANTITY',\n",
       "  24: 'one',\n",
       "  25: '(STYLE',\n",
       "  26: 'two',\n",
       "  27: 'size',\n",
       "  28: 'sprite',\n",
       "  29: 'pepper',\n",
       "  30: 'glaze',\n",
       "  31: 'ice',\n",
       "  32: '-',\n",
       "  33: 'large',\n",
       "  34: 'balsamic',\n",
       "  35: 'peppers',\n",
       "  36: '(CONTAINERTYPE',\n",
       "  37: 'ounce',\n",
       "  38: 'crust',\n",
       "  39: 'tea',\n",
       "  40: 'thin',\n",
       "  41: 'sauce',\n",
       "  42: 'ups',\n",
       "  43: 'extra',\n",
       "  44: 'diet',\n",
       "  45: 'green',\n",
       "  46: 'seven',\n",
       "  47: 'medium',\n",
       "  48: 'personal',\n",
       "  49: 'roasted',\n",
       "  50: 'red',\n",
       "  51: 'teas',\n",
       "  52: 'of',\n",
       "  53: 'ginger',\n",
       "  54: 'pecorino',\n",
       "  55: 'peperonni',\n",
       "  56: 'cans',\n",
       "  57: 'chicken',\n",
       "  58: 'banana',\n",
       "  59: 'fantas',\n",
       "  60: 'little',\n",
       "  61: 'ale',\n",
       "  62: 'lunch',\n",
       "  63: 'bottle',\n",
       "  64: '500',\n",
       "  65: 'sprites',\n",
       "  66: '20',\n",
       "  67: 'coke',\n",
       "  68: 'mozzarella',\n",
       "  69: 'onions',\n",
       "  70: 'onion',\n",
       "  71: 'bit',\n",
       "  72: 'milliliter',\n",
       "  73: 'olive',\n",
       "  74: 'just',\n",
       "  75: 'pellegrino',\n",
       "  76: 'pineapple',\n",
       "  77: 'san',\n",
       "  78: 'regular',\n",
       "  79: 'fl',\n",
       "  80: 'liter',\n",
       "  81: 'yellow',\n",
       "  82: 'tomato',\n",
       "  83: 'caramelized',\n",
       "  84: 'ml',\n",
       "  85: 'iced',\n",
       "  86: 'not',\n",
       "  87: 'olives',\n",
       "  88: 'buffalo',\n",
       "  89: 'dried',\n",
       "  90: 'pellegrinos',\n",
       "  91: 'tomatoes',\n",
       "  92: 'eight',\n",
       "  93: 'dr',\n",
       "  94: 'lot',\n",
       "  95: 'fluid',\n",
       "  96: 'pepperoni',\n",
       "  97: 'in',\n",
       "  98: 'small',\n",
       "  99: 'bacon',\n",
       "  100: '200',\n",
       "  101: 'bbq',\n",
       "  102: 'mountain',\n",
       "  103: 'garlic',\n",
       "  104: 'lemon',\n",
       "  105: 'barbecue',\n",
       "  106: 'cherry',\n",
       "  107: '12',\n",
       "  108: 'meatball',\n",
       "  109: 'sausage',\n",
       "  110: 'pesto',\n",
       "  111: '8',\n",
       "  112: '16',\n",
       "  113: 'waters',\n",
       "  114: 'pepsis',\n",
       "  115: 'jalapeno',\n",
       "  116: 'much',\n",
       "  117: 'pestos',\n",
       "  118: 'fried',\n",
       "  119: 'vegan',\n",
       "  120: 'kalamata',\n",
       "  121: '7',\n",
       "  122: 'only',\n",
       "  123: 'peppperoni',\n",
       "  124: 'fanta',\n",
       "  125: 'feta',\n",
       "  126: 'zeros',\n",
       "  127: 'mozarella',\n",
       "  128: 'doctor',\n",
       "  129: 'pulled',\n",
       "  130: 'pork',\n",
       "  131: 'beef',\n",
       "  132: 'grilled',\n",
       "  133: 'hot',\n",
       "  134: 'mushrooms',\n",
       "  135: 'many',\n",
       "  136: 'pepsi',\n",
       "  137: 'parmesan',\n",
       "  138: 'crusts',\n",
       "  139: 'sodas',\n",
       "  140: 'low',\n",
       "  141: 'fat',\n",
       "  142: 'stuffed',\n",
       "  143: 'peper',\n",
       "  144: 'lots',\n",
       "  145: 'perriers',\n",
       "  146: 'ricotta',\n",
       "  147: 'pepers',\n",
       "  148: 'dews',\n",
       "  149: 'pickles',\n",
       "  150: 'white',\n",
       "  151: 'tiny',\n",
       "  152: 'soda',\n",
       "  153: 'high',\n",
       "  154: 'rise',\n",
       "  155: 'dough',\n",
       "  156: 'zeroes',\n",
       "  157: 'peperronni',\n",
       "  158: 'arugula',\n",
       "  159: 'dew',\n",
       "  160: 'black',\n",
       "  161: 'balzamic',\n",
       "  162: 'ales',\n",
       "  163: 'perrier',\n",
       "  164: 'ranch',\n",
       "  165: '500-ml',\n",
       "  166: 'ham',\n",
       "  167: 'hams',\n",
       "  168: 'oz',\n",
       "  169: 'artichoke',\n",
       "  170: 'bay',\n",
       "  171: 'leaves',\n",
       "  172: 'alfredo',\n",
       "  173: 'oregano',\n",
       "  174: 'broccoli',\n",
       "  175: '500-milliliter',\n",
       "  176: 'artichokes',\n",
       "  177: 'bottles',\n",
       "  178: 'carrots',\n",
       "  179: 'two-liter',\n",
       "  180: '200-milliliter',\n",
       "  181: 'three-liter',\n",
       "  182: 'powder',\n",
       "  183: 'mushroom',\n",
       "  184: 'peperoni',\n",
       "  185: 'cheddar',\n",
       "  186: 'basil',\n",
       "  187: 'sixteen',\n",
       "  188: 'applewood',\n",
       "  189: 'anchovy',\n",
       "  190: 'chickens',\n",
       "  191: 'cumin',\n",
       "  192: 'apple',\n",
       "  193: 'wood',\n",
       "  194: 'carrot',\n",
       "  195: 'meat',\n",
       "  196: 'peperroni',\n",
       "  197: 'chorizo',\n",
       "  198: 'ground',\n",
       "  199: 'anchovies',\n",
       "  200: 'cheeseburger',\n",
       "  201: 'pickle',\n",
       "  202: 'chorrizo',\n",
       "  203: 'bean',\n",
       "  204: 'brocoli',\n",
       "  205: 'bacons',\n",
       "  206: 'beans',\n",
       "  207: 'big',\n",
       "  208: 'spiced',\n",
       "  209: 'peppperonis',\n",
       "  210: 'tuna',\n",
       "  211: 'spicy',\n",
       "  212: 'salami',\n",
       "  213: 'spinach',\n",
       "  214: 'pepperonis',\n",
       "  215: 'lettuce',\n",
       "  216: 'shrimps',\n",
       "  217: 'italian',\n",
       "  218: 'flakes',\n",
       "  219: 'shrimp',\n",
       "  220: 'rosemary',\n",
       "  221: 'flake',\n",
       "  222: 'pineaples',\n",
       "  223: 'pineapples',\n",
       "  224: 'meatlover',\n",
       "  225: 'pineaple',\n",
       "  226: 'jalapenos',\n",
       "  227: 'peperonis',\n",
       "  228: 'pea',\n",
       "  229: 'parsley',\n",
       "  230: 'peas',\n",
       "  231: 'oil',\n",
       "  232: 'all',\n",
       "  233: 'can',\n",
       "  234: 'up',\n",
       "  235: 'thick',\n",
       "  236: 'water',\n",
       "  237: 'new',\n",
       "  238: 'sausages',\n",
       "  239: 'meatballs',\n",
       "  240: 'tunas',\n",
       "  241: '4',\n",
       "  242: '14',\n",
       "  243: 'fifteen',\n",
       "  244: '3',\n",
       "  245: '13',\n",
       "  246: '5',\n",
       "  247: 'twelve',\n",
       "  248: 'six',\n",
       "  249: 'ten',\n",
       "  250: 'thirteen',\n",
       "  251: '2',\n",
       "  252: '11',\n",
       "  253: 'eleven',\n",
       "  254: '15',\n",
       "  255: '9',\n",
       "  256: 'the',\n",
       "  257: '6',\n",
       "  258: 'nine',\n",
       "  259: 'fourteen',\n",
       "  260: '10',\n",
       "  261: '1',\n",
       "  262: 'an',\n",
       "  263: 'yorker',\n",
       "  264: 'cokes',\n",
       "  265: 'gluten',\n",
       "  266: 'free',\n",
       "  267: 'sourdough',\n",
       "  268: 'keto',\n",
       "  269: 'combination',\n",
       "  270: 'style',\n",
       "  271: 'chicago',\n",
       "  272: 'lover',\n",
       "  273: 'every',\n",
       "  274: 'lovers',\n",
       "  275: 'cauliflower',\n",
       "  276: 'gluten-free',\n",
       "  277: 'everything',\n",
       "  278: 'deepdish',\n",
       "  279: 'vegetarian',\n",
       "  280: 'zero',\n",
       "  281: 'napolitana',\n",
       "  282: 'margarita',\n",
       "  283: 'neapolitan',\n",
       "  284: 'york',\n",
       "  285: 'veggies',\n",
       "  286: 'meatlovers',\n",
       "  287: 'veggie',\n",
       "  288: 'topping',\n",
       "  289: 'with',\n",
       "  290: 'works',\n",
       "  291: 'supreme',\n",
       "  292: 'hawaiian',\n",
       "  293: 'margherita',\n",
       "  294: 'deep',\n",
       "  295: 'dish',\n",
       "  296: 'mexican',\n",
       "  297: 'vegetables',\n",
       "  298: 'toppings',\n",
       "  299: 'mediterranean',\n",
       "  300: 'coffee',\n",
       "  301: 'coffees',\n",
       "  302: 'pan',\n",
       "  303: 'med',\n",
       "  304: 'napolitan'})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_word2idx, tgt_idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## max lenght without preprocessing (133, 335)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_src_len = 133\n",
    "max_tgt_len = 335\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=lambda b: collate_fn(b, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=lambda b: collate_fn(b, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_dim = len(src_word2idx)\n",
    "output_dim = len(tgt_word2idx)\n",
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m----> 3\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_loader, optimizer, criterion)\n\u001b[0;32m      4\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate_loss(model, val_loader, criterion)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Compute exact match accuracy on dev set\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[5], line 17\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, criterion, clip)\u001b[0m\n\u001b[0;32m     14\u001b[0m tgt \u001b[38;5;241m=\u001b[39m tgt[:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[1;32m---> 17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     19\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    521\u001b[0m     )\n\u001b[1;32m--> 522\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    523\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    524\u001b[0m )\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    267\u001b[0m     tensors,\n\u001b[0;32m    268\u001b[0m     grad_tensors_,\n\u001b[0;32m    269\u001b[0m     retain_graph,\n\u001b[0;32m    270\u001b[0m     create_graph,\n\u001b[0;32m    271\u001b[0m     inputs,\n\u001b[0;32m    272\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    273\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    274\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate_loss(model, val_loader, criterion)\n",
    "\n",
    "    # Compute exact match accuracy on dev set\n",
    "    exact_match = exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Dev Exact Match: {exact_match:.2%}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'exact_match_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Final evaluation on dev\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m exact_match_final \u001b[38;5;241m=\u001b[39m exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinal Dev Exact Match Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exact_match_final)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'exact_match_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Final evaluation on dev\n",
    "exact_match_final = exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n",
    "print(\"Final Dev Exact Match Accuracy:\", exact_match_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
