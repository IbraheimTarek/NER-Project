{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "from utils.data_preprocessing import preprocess_text\n",
    "from utils.feature_extraction import bag_of_words, tfidf_features, extract_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records so far...\n",
      "Processed 20000 records so far...\n",
      "Processed 30000 records so far...\n",
      "Processed 40000 records so far...\n",
      "Processed 50000 records so far...\n",
      "Processed 60000 records so far...\n",
      "Processed 70000 records so far...\n",
      "Processed 80000 records so far...\n",
      "Processed 90000 records so far...\n",
      "Processed 100000 records so far...\n",
      "Processed 110000 records so far...\n",
      "Processed 120000 records so far...\n",
      "Processed 130000 records so far...\n",
      "Processed 140000 records so far...\n",
      "Processed 150000 records so far...\n",
      "Processed 160000 records so far...\n",
      "Processed 170000 records so far...\n",
      "Processed 180000 records so far...\n",
      "Processed 190000 records so far...\n",
      "Processed 200000 records so far...\n",
      "Processed 210000 records so far...\n",
      "Processed 220000 records so far...\n",
      "Processed 230000 records so far...\n",
      "Processed 240000 records so far...\n",
      "Processed 250000 records so far...\n",
      "Processed 260000 records so far...\n",
      "Processed 270000 records so far...\n",
      "Processed 280000 records so far...\n",
      "Processed 290000 records so far...\n",
      "Processed 300000 records so far...\n",
      "Processed 310000 records so far...\n",
      "Processed 320000 records so far...\n",
      "Processed 330000 records so far...\n",
      "Processed 340000 records so far...\n",
      "Processed 350000 records so far...\n",
      "Processed 360000 records so far...\n",
      "Processed 370000 records so far...\n",
      "Processed 380000 records so far...\n",
      "Processed 390000 records so far...\n",
      "Processed 400000 records so far...\n",
      "Processed 410000 records so far...\n",
      "Processed 420000 records so far...\n",
      "Processed 430000 records so far...\n",
      "Processed 440000 records so far...\n",
      "Processed 450000 records so far...\n",
      "Processed 460000 records so far...\n",
      "Processed 470000 records so far...\n",
      "Processed 480000 records so far...\n",
      "Processed 490000 records so far...\n",
      "Processed 500000 records so far...\n",
      "Processed 510000 records so far...\n",
      "Processed 520000 records so far...\n",
      "Processed 530000 records so far...\n",
      "Processed 540000 records so far...\n",
      "Processed 550000 records so far...\n",
      "Processed 560000 records so far...\n",
      "Processed 570000 records so far...\n",
      "Processed 580000 records so far...\n",
      "Processed 590000 records so far...\n",
      "Processed 600000 records so far...\n",
      "Processed 610000 records so far...\n",
      "Processed 620000 records so far...\n",
      "Processed 630000 records so far...\n",
      "Processed 640000 records so far...\n",
      "Processed 650000 records so far...\n",
      "Processed 660000 records so far...\n",
      "Processed 670000 records so far...\n",
      "Processed 680000 records so far...\n",
      "Processed 690000 records so far...\n",
      "Processed 700000 records so far...\n",
      "Processed 710000 records so far...\n",
      "Processed 720000 records so far...\n",
      "Processed 730000 records so far...\n",
      "Processed 740000 records so far...\n",
      "Processed 750000 records so far...\n",
      "Processed 760000 records so far...\n",
      "Processed 770000 records so far...\n",
      "Processed 780000 records so far...\n",
      "Processed 790000 records so far...\n",
      "Processed 800000 records so far...\n",
      "Processed 810000 records so far...\n",
      "Processed 820000 records so far...\n",
      "Processed 830000 records so far...\n",
      "Processed 840000 records so far...\n",
      "Processed 850000 records so far...\n",
      "Processed 860000 records so far...\n",
      "Processed 870000 records so far...\n",
      "Processed 880000 records so far...\n",
      "Processed 890000 records so far...\n",
      "Processed 900000 records so far...\n",
      "Processed 910000 records so far...\n",
      "Processed 920000 records so far...\n",
      "Processed 930000 records so far...\n",
      "Processed 940000 records so far...\n",
      "Processed 950000 records so far...\n",
      "Processed 960000 records so far...\n",
      "Processed 970000 records so far...\n",
      "Processed 980000 records so far...\n",
      "Processed 990000 records so far...\n",
      "Processed 1000000 records so far...\n",
      "Processed 1010000 records so far...\n",
      "Processed 1020000 records so far...\n",
      "Processed 1030000 records so far...\n",
      "Processed 1040000 records so far...\n",
      "Processed 1050000 records so far...\n",
      "Processed 1060000 records so far...\n",
      "Processed 1070000 records so far...\n",
      "Processed 1080000 records so far...\n",
      "Processed 1090000 records so far...\n",
      "Processed 1100000 records so far...\n",
      "Processed 1110000 records so far...\n",
      "Processed 1120000 records so far...\n",
      "Processed 1130000 records so far...\n",
      "Processed 1140000 records so far...\n",
      "Processed 1150000 records so far...\n",
      "Processed 1160000 records so far...\n",
      "Processed 1170000 records so far...\n",
      "Processed 1180000 records so far...\n",
      "Processed 1190000 records so far...\n",
      "Processed 1200000 records so far...\n",
      "Processed 1210000 records so far...\n",
      "Processed 1220000 records so far...\n",
      "Processed 1230000 records so far...\n",
      "Processed 1240000 records so far...\n",
      "Processed 1250000 records so far...\n",
      "Processed 1260000 records so far...\n",
      "Processed 1270000 records so far...\n",
      "Processed 1280000 records so far...\n",
      "Processed 1290000 records so far...\n",
      "Processed 1300000 records so far...\n",
      "Processed 1310000 records so far...\n",
      "Processed 1320000 records so far...\n",
      "Processed 1330000 records so far...\n",
      "Processed 1340000 records so far...\n",
      "Processed 1350000 records so far...\n",
      "Processed 1360000 records so far...\n",
      "Processed 1370000 records so far...\n",
      "Processed 1380000 records so far...\n",
      "Processed 1390000 records so far...\n",
      "Processed 1400000 records so far...\n",
      "Processed 1410000 records so far...\n",
      "Processed 1420000 records so far...\n",
      "Processed 1430000 records so far...\n",
      "Processed 1440000 records so far...\n",
      "Processed 1450000 records so far...\n",
      "Processed 1460000 records so far...\n",
      "Processed 1470000 records so far...\n",
      "Processed 1480000 records so far...\n",
      "Processed 1490000 records so far...\n",
      "Processed 1500000 records so far...\n",
      "Processed 1510000 records so far...\n",
      "Processed 1520000 records so far...\n",
      "Processed 1530000 records so far...\n",
      "Processed 1540000 records so far...\n",
      "Processed 1550000 records so far...\n",
      "Processed 1560000 records so far...\n",
      "Processed 1570000 records so far...\n",
      "Processed 1580000 records so far...\n",
      "Processed 1590000 records so far...\n",
      "Processed 1600000 records so far...\n",
      "Processed 1610000 records so far...\n",
      "Processed 1620000 records so far...\n",
      "Processed 1630000 records so far...\n",
      "Processed 1640000 records so far...\n",
      "Processed 1650000 records so far...\n",
      "Processed 1660000 records so far...\n",
      "Processed 1670000 records so far...\n",
      "Processed 1680000 records so far...\n",
      "Processed 1690000 records so far...\n",
      "Processed 1700000 records so far...\n",
      "Processed 1710000 records so far...\n",
      "Processed 1720000 records so far...\n",
      "Processed 1730000 records so far...\n",
      "Processed 1740000 records so far...\n",
      "Processed 1750000 records so far...\n",
      "Processed 1760000 records so far...\n",
      "Processed 1770000 records so far...\n",
      "Processed 1780000 records so far...\n",
      "Processed 1790000 records so far...\n",
      "Processed 1800000 records so far...\n",
      "Processed 1810000 records so far...\n",
      "Processed 1820000 records so far...\n",
      "Processed 1830000 records so far...\n",
      "Processed 1840000 records so far...\n",
      "Processed 1850000 records so far...\n",
      "Processed 1860000 records so far...\n",
      "Processed 1870000 records so far...\n",
      "Processed 1880000 records so far...\n",
      "Processed 1890000 records so far...\n",
      "Processed 1900000 records so far...\n",
      "Processed 1910000 records so far...\n",
      "Processed 1920000 records so far...\n",
      "Processed 1930000 records so far...\n",
      "Processed 1940000 records so far...\n",
      "Processed 1950000 records so far...\n",
      "Processed 1960000 records so far...\n",
      "Processed 1970000 records so far...\n",
      "Processed 1980000 records so far...\n",
      "Processed 1990000 records so far...\n",
      "Processed 2000000 records so far...\n",
      "Processed 2010000 records so far...\n",
      "Processed 2020000 records so far...\n",
      "Processed 2030000 records so far...\n",
      "Processed 2040000 records so far...\n",
      "Processed 2050000 records so far...\n",
      "Processed 2060000 records so far...\n",
      "Processed 2070000 records so far...\n",
      "Processed 2080000 records so far...\n",
      "Processed 2090000 records so far...\n",
      "Processed 2100000 records so far...\n",
      "Processed 2110000 records so far...\n",
      "Processed 2120000 records so far...\n",
      "Processed 2130000 records so far...\n",
      "Processed 2140000 records so far...\n",
      "Processed 2150000 records so far...\n",
      "Processed 2160000 records so far...\n",
      "Processed 2170000 records so far...\n",
      "Processed 2180000 records so far...\n",
      "Processed 2190000 records so far...\n",
      "Processed 2200000 records so far...\n",
      "Processed 2210000 records so far...\n",
      "Processed 2220000 records so far...\n",
      "Processed 2230000 records so far...\n",
      "Processed 2240000 records so far...\n",
      "Processed 2250000 records so far...\n",
      "Processed 2260000 records so far...\n",
      "Processed 2270000 records so far...\n",
      "Processed 2280000 records so far...\n",
      "Processed 2290000 records so far...\n",
      "Processed 2300000 records so far...\n",
      "Processed 2310000 records so far...\n",
      "Processed 2320000 records so far...\n",
      "Processed 2330000 records so far...\n",
      "Processed 2340000 records so far...\n",
      "Processed 2350000 records so far...\n",
      "Processed 2360000 records so far...\n",
      "Processed 2370000 records so far...\n",
      "Processed 2380000 records so far...\n",
      "Processed 2390000 records so far...\n",
      "Processed 2400000 records so far...\n",
      "Processed 2410000 records so far...\n",
      "Processed 2420000 records so far...\n",
      "Processed 2430000 records so far...\n",
      "Processed 2440000 records so far...\n",
      "Processed 2450000 records so far...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = '../dataset/PIZZA_train.json'\n",
    "test_path = '../dataset/PIZZA_dev.json'\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                data.append(record)\n",
    "                \n",
    "                # Process in chunks of 10,000 records\n",
    "                if i > 0 and i % 10000 == 0:\n",
    "                    print(f\"Processed {i} records so far...\")\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Convert remaining data to DataFrame\n",
    "data = read_data(train_path)\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "data = read_data(test_path)\n",
    "if data:\n",
    "    dev = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hima\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2064,  1045,  2031,  1037,  2312, 22861,  4160,  2766, 15960,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  101,  1006,  2344,  2064,  1045,  2031,  1006, 10733,  8551,  2121,\n",
      "          1006,  2193,  1037,  1007,  1006,  2946,  2312,  1007,  1006, 22286,\n",
      "         22861,  4160,  2766, 15960,  1007,  1007,  1007,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Example: Using a pre-trained tokenizer like BERT\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "src_text = \"can i have a large bbq pulled pork\"\n",
    "tgt_text = \"(ORDER can i have (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\"\n",
    "\n",
    "# Tokenize the input and output\n",
    "src_tokens = src_tokenizer(src_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "tgt_tokens = tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "print(src_tokens)\n",
    "print(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[\"train.SRC\"][idx]\n",
    "        tgt_text = self.data[\"train.TOP\"][idx]\n",
    "\n",
    "        src_tokens = self.src_tokenizer(src_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "        tgt_tokens = self.tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"src_input_ids\": src_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"src_attention_mask\": src_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"tgt_input_ids\": tgt_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"tgt_attention_mask\": tgt_tokens[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "dataset = PizzaDataset(df, src_tokenizer, tgt_tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([  101,  2312, 11345,  2007,  2665, 11565,  1998,  2007,  4469, 27233,\n",
       "         26534,  3490,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'src_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'tgt_input_ids': tensor([  101,  1006,  2344,  1006, 10733,  8551,  2121,  1006,  2946,  2312,\n",
       "          1007, 11345,  2007,  1006, 22286,  2665, 11565,  1007,  1998,  2007,\n",
       "          1006,  3375,  1035, 22286,  1006, 11712,  4469,  1007,  1006, 22286,\n",
       "         27233, 26534,  3490,  1007,  1007,  1007,  1007,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'tgt_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0])}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration\n",
    "\n",
    "# Initialize a seq2seq model\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76764"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Initialize variables for tracking\n",
    "running_loss = 0.0\n",
    "total_batches = len(dataloader)\n",
    "total_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 10/76764 [00:15<31:42:15,  1.49s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 10/76764, Average Loss: 9.6387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 20/76764 [00:31<34:03:10,  1.60s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20/76764, Average Loss: 9.6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 30/76764 [00:48<37:56:17,  1.78s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 30/76764, Average Loss: 9.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 40/76764 [01:05<34:27:48,  1.62s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 40/76764, Average Loss: 9.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 50/76764 [01:21<36:00:03,  1.69s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 50/76764, Average Loss: 9.6180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 60/76764 [01:37<33:42:32,  1.58s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 60/76764, Average Loss: 9.6251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 70/76764 [01:53<33:15:36,  1.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 70/76764, Average Loss: 9.6260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 80/76764 [02:08<32:41:20,  1.53s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 80/76764, Average Loss: 9.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 90/76764 [02:24<34:31:38,  1.62s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 90/76764, Average Loss: 9.6176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 100/76764 [02:40<35:36:34,  1.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/76764, Average Loss: 9.6190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 110/76764 [02:56<32:57:27,  1.55s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 110/76764, Average Loss: 9.6228\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 120/76764 [03:13<38:17:29,  1.80s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 120/76764, Average Loss: 9.6312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 130/76764 [03:32<38:42:09,  1.82s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 130/76764, Average Loss: 9.6256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 140/76764 [03:50<39:53:45,  1.87s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 140/76764, Average Loss: 9.6268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 150/76764 [04:07<35:10:20,  1.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 150/76764, Average Loss: 9.6272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 160/76764 [04:23<32:40:49,  1.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 160/76764, Average Loss: 9.6261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 170/76764 [04:40<38:17:02,  1.80s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 170/76764, Average Loss: 9.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 180/76764 [04:58<36:05:22,  1.70s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 180/76764, Average Loss: 9.6254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 190/76764 [05:14<34:09:53,  1.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 190/76764, Average Loss: 9.6242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 200/76764 [05:31<33:50:56,  1.59s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 200/76764, Average Loss: 9.6199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 210/76764 [05:47<35:06:07,  1.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 210/76764, Average Loss: 9.6176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 220/76764 [06:04<37:57:22,  1.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 220/76764, Average Loss: 9.6175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 230/76764 [06:23<38:45:54,  1.82s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 230/76764, Average Loss: 9.6183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 240/76764 [06:41<38:02:36,  1.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 240/76764, Average Loss: 9.6177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 250/76764 [06:57<33:46:24,  1.59s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 250/76764, Average Loss: 9.6180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 260/76764 [07:13<33:26:43,  1.57s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 260/76764, Average Loss: 9.6174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 270/76764 [07:28<31:20:07,  1.47s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 270/76764, Average Loss: 9.6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 280/76764 [07:44<34:59:35,  1.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 280/76764, Average Loss: 9.6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 290/76764 [08:00<34:20:18,  1.62s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 290/76764, Average Loss: 9.6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 300/76764 [08:14<31:07:30,  1.47s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 300/76764, Average Loss: 9.6191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 310/76764 [08:31<35:10:04,  1.66s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 310/76764, Average Loss: 9.6189\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 320/76764 [08:47<34:15:21,  1.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 320/76764, Average Loss: 9.6205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 330/76764 [09:04<35:18:03,  1.66s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 330/76764, Average Loss: 9.6204\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 340/76764 [09:20<35:21:17,  1.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 340/76764, Average Loss: 9.6243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 350/76764 [09:37<32:31:52,  1.53s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 350/76764, Average Loss: 9.6239\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 360/76764 [09:51<29:57:35,  1.41s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 360/76764, Average Loss: 9.6252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 370/76764 [10:05<29:05:40,  1.37s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 370/76764, Average Loss: 9.6233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 380/76764 [10:20<32:17:33,  1.52s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 380/76764, Average Loss: 9.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 390/76764 [10:35<30:44:22,  1.45s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 390/76764, Average Loss: 9.6211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 400/76764 [10:51<34:53:10,  1.64s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 400/76764, Average Loss: 9.6212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 410/76764 [11:08<35:27:05,  1.67s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 410/76764, Average Loss: 9.6222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 420/76764 [11:23<31:58:06,  1.51s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 420/76764, Average Loss: 9.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 430/76764 [11:39<33:06:54,  1.56s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 430/76764, Average Loss: 9.6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 440/76764 [11:55<36:44:52,  1.73s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 440/76764, Average Loss: 9.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 450/76764 [12:11<32:37:23,  1.54s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 450/76764, Average Loss: 9.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 460/76764 [12:27<32:52:38,  1.55s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 460/76764, Average Loss: 9.6220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 470/76764 [12:42<31:40:35,  1.49s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 470/76764, Average Loss: 9.6207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 480/76764 [12:57<33:13:15,  1.57s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 480/76764, Average Loss: 9.6194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 490/76764 [13:13<32:53:31,  1.55s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 490/76764, Average Loss: 9.6213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 500/76764 [13:29<34:17:01,  1.62s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 500/76764, Average Loss: 9.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 510/76764 [13:45<34:05:23,  1.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 510/76764, Average Loss: 9.6220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 520/76764 [14:02<36:08:07,  1.71s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 520/76764, Average Loss: 9.6234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 530/76764 [14:18<33:30:56,  1.58s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 530/76764, Average Loss: 9.6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 540/76764 [14:36<39:26:55,  1.86s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 540/76764, Average Loss: 9.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 550/76764 [14:53<35:13:09,  1.66s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 550/76764, Average Loss: 9.6221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 560/76764 [15:09<34:57:31,  1.65s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 560/76764, Average Loss: 9.6218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 570/76764 [15:26<34:30:21,  1.63s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 570/76764, Average Loss: 9.6229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 580/76764 [15:41<34:07:55,  1.61s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 580/76764, Average Loss: 9.6234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 590/76764 [15:59<37:57:58,  1.79s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 590/76764, Average Loss: 9.6231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   1%|          | 597/76764 [16:13<34:29:06,  1.63s/batch]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m labels \u001b[38;5;241m=\u001b[39m tgt_input_ids\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m      9\u001b[0m labels[labels \u001b[38;5;241m==\u001b[39m tgt_tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m  \u001b[38;5;66;03m# Ignore padding for loss\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[0;32m     12\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39msrc_input_ids,\n\u001b[0;32m     13\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39msrc_attention_mask,\n\u001b[0;32m     14\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[0;32m     17\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\models\\bart\\modeling_bart.py:1659\u001b[0m, in \u001b[0;36mBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1637\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(\n\u001b[0;32m   1638\u001b[0m             labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mdecoder_start_token_id\n\u001b[0;32m   1639\u001b[0m         )\n\u001b[0;32m   1641\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\n\u001b[0;32m   1642\u001b[0m     input_ids,\n\u001b[0;32m   1643\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1656\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[0;32m   1657\u001b[0m )\n\u001b[1;32m-> 1659\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m   1660\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m lm_logits \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\u001b[38;5;241m.\u001b[39mto(lm_logits\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   1662\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Training Loop with Loss Tracking\n",
    "for batch_idx, batch in enumerate(tqdm(dataloader, desc=\"Training Progress\", unit=\"batch\")):\n",
    "    src_input_ids = batch[\"src_input_ids\"]\n",
    "    src_attention_mask = batch[\"src_attention_mask\"]\n",
    "    tgt_input_ids = batch[\"tgt_input_ids\"]\n",
    "\n",
    "    # Shift target for decoder input\n",
    "    labels = tgt_input_ids.clone()\n",
    "    labels[labels == tgt_tokenizer.pad_token_id] = -100  # Ignore padding for loss\n",
    "\n",
    "    outputs = model(\n",
    "        input_ids=src_input_ids,\n",
    "        attention_mask=src_attention_mask,\n",
    "        labels=labels\n",
    "    )\n",
    "    loss = outputs.loss\n",
    "    running_loss += loss.item()\n",
    "    \n",
    "    # Print progress every 10 batches\n",
    "    if (batch_idx + 1) % 10 == 0:\n",
    "        avg_loss = running_loss / (batch_idx + 1)\n",
    "        print(f\"Batch {batch_idx + 1}/{total_batches}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# Print final loss at the end\n",
    "print(f\"Final Average Loss: {running_loss / total_batches:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [unused1] can i have a large bbq pulled pork ship [unused410] a large mates large bb caucasian pulled pork [unused9] ship [unused410] initiatives large bb large bbhearted movement pork management jose pork ship § [unused410] [unused1]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"can i have a large bbq pulled pork\"\n",
    "input_tokens = src_tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Generate prediction\n",
    "output_tokens = model.generate(input_ids=input_tokens[\"input_ids\"], attention_mask=input_tokens[\"attention_mask\"], max_length=50)\n",
    "predicted_text = tgt_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "print(f\"Predicted: {predicted_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevPizzaDataset(Dataset):\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[idx][\"dev.SRC\"]\n",
    "        tgt_text = self.data[idx][\"dev.TOP\"]\n",
    "\n",
    "        src_tokens = self.src_tokenizer(src_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "        tgt_tokens = self.tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"src_input_ids\": src_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"src_attention_mask\": src_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"tgt_input_ids\": tgt_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"tgt_attention_mask\": tgt_tokens[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "dev_dataset = DevPizzaDataset(dev, src_tokenizer, tgt_tokenizer)\n",
    "dev_dataloader = DataLoader(dev_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval() \n",
    "\n",
    "predictions = []\n",
    "ground_truths = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in dev_dataloader:\n",
    "        src_input_ids = batch[\"src_input_ids\"]\n",
    "        src_attention_mask = batch[\"src_attention_mask\"]\n",
    "\n",
    "        # Generate output sequence\n",
    "        output_tokens = model.generate(\n",
    "            input_ids=src_input_ids,\n",
    "            attention_mask=src_attention_mask,\n",
    "            max_length=100,  # Adjust as needed\n",
    "            num_beams=4      # Use beam search for better predictions\n",
    "        )\n",
    "\n",
    "        # Decode predictions and ground truth\n",
    "        predicted_text = tgt_tokenizer.decode(output_tokens[0], skip_special_tokens=True)\n",
    "        ground_truth_text = tgt_tokenizer.decode(batch[\"tgt_input_ids\"][0], skip_special_tokens=True)\n",
    "\n",
    "        predictions.append(predicted_text)\n",
    "        ground_truths.append(ground_truth_text)\n",
    "\n",
    "        print(f\"Source: {src_tokenizer.decode(src_input_ids[0], skip_special_tokens=True)}\")\n",
    "        print(f\"Prediction: {predicted_text}\")\n",
    "        print(f\"Ground Truth: {ground_truth_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_matches = sum([1 for pred, gt in zip(predictions, ground_truths) if pred == gt])\n",
    "accuracy = exact_matches / len(ground_truths)\n",
    "\n",
    "print(f\"Exact Match Accuracy: {accuracy * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "# Tokenize predictions and ground truths\n",
    "predictions_tokens = [pred.split() for pred in predictions]\n",
    "ground_truths_tokens = [[gt.split()] for gt in ground_truths]  # Nested list for corpus_bleu\n",
    "\n",
    "bleu_score = corpus_bleu(ground_truths_tokens, predictions_tokens)\n",
    "print(f\"BLEU Score: {bleu_score * 100:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):\n",
    "    if pred != gt:\n",
    "        print(f\"Mismatch {i + 1}:\")\n",
    "        print(f\"Prediction: {pred}\")\n",
    "        print(f\"Ground Truth: {gt}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
