{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "ENTITY_KEYS = {\n",
    "    \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\", \"COMPLEX_TOPPING\", \"QUANTITY\",\n",
    "    \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\"\n",
    "}\n",
    "ORDER_KEYS = {\"PIZZAORDER\", \"DRINKORDER\"}\n",
    "\n",
    "def tokenize(s):\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "def extract_orders(structure, order_index=1):\n",
    "\n",
    "    results = []\n",
    "\n",
    "    if not isinstance(structure, list) or len(structure) == 0:\n",
    "        return results, order_index\n",
    "\n",
    "    first = structure[0]\n",
    "    if isinstance(first, list):\n",
    "        for elem in structure:\n",
    "            sub_results, order_index = extract_orders(elem, order_index)\n",
    "            results.extend(sub_results)\n",
    "        return results, order_index\n",
    "\n",
    "    if isinstance(first, str) and first in ORDER_KEYS:\n",
    "        order_type = \"PIZZAORDER\" if first == \"PIZZAORDER\" else \"DRINKORDER\"\n",
    "        current_order_sequence = order_index\n",
    "        order_index += 1\n",
    "        content_tokens = []\n",
    "        for elem in structure[1:]:\n",
    "            content_tokens.extend(collect_tokens(elem))\n",
    "        for tok in content_tokens:\n",
    "            results.append((tok, order_type, current_order_sequence))\n",
    "\n",
    "        return results, order_index\n",
    "    else:\n",
    "        for elem in structure:\n",
    "            sub_results, order_index = extract_orders(elem, order_index)\n",
    "            results.extend(sub_results)\n",
    "        return results, order_index\n",
    "def collect_tokens(node):\n",
    "    collected = []\n",
    "    if isinstance(node, list):\n",
    "        for sub in node:\n",
    "            sub_tokens = collect_tokens(sub)\n",
    "            collected.extend(sub_tokens)\n",
    "    else:\n",
    "        if node not in [\"(\", \")\"] and not is_structural_key(node):\n",
    "            collected.append(node)\n",
    "    return collected\n",
    "\n",
    "def is_structural_key(token):\n",
    "    return token in [\n",
    "        \"ORDER\",\"PIZZAORDER\",\"DRINKORDER\",\"NUMBER\",\"SIZE\",\"STYLE\",\"TOPPING\",\n",
    "        \"COMPLEX_TOPPING\",\"QUANTITY\",\"VOLUME\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"NOT\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'O', None), ('need', 'O', None), ('a', 'PIZZAORDER', 1), ('medium', 'PIZZAORDER', 1), ('ham', 'PIZZAORDER', 1), ('and', 'PIZZAORDER', 1), ('pineapple', 'PIZZAORDER', 1), ('pizza', 'PIZZAORDER', 1), ('and', 'O', None), ('a', 'DRINKORDER', 2), ('small', 'DRINKORDER', 2), ('iced', 'DRINKORDER', 2), ('tea', 'DRINKORDER', 2)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def label_input(input_text, top):\n",
    "    tokens = tokenize(top)\n",
    "    parsed = parse_tokens(tokens)\n",
    "\n",
    "    order_info, _ = extract_orders(parsed)\n",
    "    label_dict = defaultdict(list)\n",
    "    for tok, lbl, num in order_info:\n",
    "        label_dict[tok.lower()].append((lbl, num))\n",
    "\n",
    "    input_tokens = input_text.split()\n",
    "    labeled_input = []\n",
    "\n",
    "    used_labels = defaultdict(int)\n",
    "\n",
    "    for token in input_tokens:\n",
    "        token_lower = token.lower()\n",
    "        if token_lower in label_dict:\n",
    "\n",
    "            label_index = used_labels[token_lower]\n",
    "            if label_index < len(label_dict[token_lower]):\n",
    "                token_label, sequence_number = label_dict[token_lower][label_index]\n",
    "                used_labels[token_lower] += 1 \n",
    "            else:\n",
    "                token_label, sequence_number = 'O', None  \n",
    "        else:\n",
    "            token_label, sequence_number = 'O', None  \n",
    "\n",
    "        labeled_input.append((token, token_label, sequence_number))\n",
    "    return labeled_input\n",
    "\n",
    "top = \"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE medium ) (TOPPING ham ) and (TOPPING pineapple ) pizza ) and (DRINKORDER (NUMBER a ) (VOLUME small ) (DRINKTYPE iced tea ) ) )\"\n",
    "\n",
    "input_text = \"i need a medium ham and pineapple pizza and a small iced tea\"\n",
    "input_label_sequence = label_input(input_text, top)\n",
    "print(input_label_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 11, 11, 11, 11, 11, 11, 0, 22, 22, 22, 22]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def transform_to_labels(input_array):\n",
    "    labeled_numbers = []\n",
    "\n",
    "    for _, label, sequence in input_array:\n",
    "        # Compute the numerical label\n",
    "        if label == 'O' and sequence is None:\n",
    "            numerical_label = 0  # Neutral/irrelevant\n",
    "        elif label == 'PIZZAORDER':\n",
    "            numerical_label = 10 + sequence  # Unique range for pizza orders\n",
    "        elif label == 'DRINKORDER':\n",
    "            numerical_label = 20 + sequence  # Unique range for drink orders\n",
    "        else:\n",
    "            numerical_label = 0  # Default fallback\n",
    "\n",
    "        labeled_numbers.append(numerical_label)\n",
    "\n",
    "    return labeled_numbers\n",
    "\n",
    "transform_to_labels(input_label_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(input_file: str, output_file: str):\n",
    "\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            src = record[\"dev.SRC\"]\n",
    "            top = record[\"dev.TOP\"]\n",
    "\n",
    "            labeled_input = label_input(src, top)\n",
    "            numerical_labels = transform_to_labels(labeled_input)\n",
    "\n",
    "            training_instance = {\n",
    "                \"text\": src,\n",
    "                \"labels\": numerical_labels\n",
    "            }\n",
    "\n",
    "            outfile.write(json.dumps(training_instance) + \"\\n\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"../dataset/PIZZA_dev.json\"\n",
    "output_file = \"../dataset/dev_data_model1.json\"\n",
    "\n",
    "# Generate the training data\n",
    "#create_training_data(input_file,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2456446</td>\n",
       "      <td>2456446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2456446</td>\n",
       "      <td>1929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>can i have a large bbq pulled pork</td>\n",
       "      <td>[0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>110156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      text  \\\n",
       "count                              2456446   \n",
       "unique                             2456446   \n",
       "top     can i have a large bbq pulled pork   \n",
       "freq                                     1   \n",
       "\n",
       "                                                   labels  \n",
       "count                                             2456446  \n",
       "unique                                               1929  \n",
       "top     [0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11,...  \n",
       "freq                                               110156  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_path = '../dataset/training_data_model1.json'\n",
    "dev_path = \"../dataset/dev_data_model1.json\"\n",
    "df = pd.read_json(train_path, lines=True)\n",
    "dev = pd.read_json(dev_path, lines=True)\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>348</td>\n",
       "      <td>348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>348</td>\n",
       "      <td>176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>i want to order two medium pizzas with sausage...</td>\n",
       "      <td>[0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "count                                                 348   \n",
       "unique                                                348   \n",
       "top     i want to order two medium pizzas with sausage...   \n",
       "freq                                                    1   \n",
       "\n",
       "                                            labels  \n",
       "count                                          348  \n",
       "unique                                         176  \n",
       "top     [0, 0, 11, 11, 11, 11, 11, 11, 11, 11, 11]  \n",
       "freq                                            24  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df['text']\n",
    "y_train = df['labels']\n",
    "X_test = dev['text']\n",
    "y_test = dev['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('can i have a large bbq pulled pork', [0, 0, 0, 11, 11, 11, 11, 11])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0], y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}  # Special tokens\n",
    "\n",
    "def tokenize_output(output):\n",
    "    \"\"\"\n",
    "    Tokenizes the structured output into meaningful tokens.\n",
    "    Example:\n",
    "        Input: \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\"\n",
    "        Output: [\"(ORDER\", \"(PIZZAORDER\", \"(NUMBER\", \"a\", \"(SIZE\", \"large\", \"(TOPPING\", \"bbq\", \"pulled\", \"pork\", \")\", \")\", \")\", \")\"]\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"\\(|\\)|\\w+|[^\\s()]+\", output)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(outputs, index):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from tokenized outputs.\n",
    "    \"\"\"\n",
    "    i = index\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = i\n",
    "                i += 1\n",
    "    return vocab, i\n",
    "def encode_outputs(outputs, vocab):\n",
    "    encoded = []\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)  # Tokenize the output\n",
    "        sequence = [vocab.get(token, vocab.get(\"<UNK>\", 0)) for token in tokens] # [vocab.get(\"<SOS>\", 0)] + \\+ \\[vocab.get(\"<EOS>\", 0)]\n",
    "        encoded.append(sequence)\n",
    "    return encoded\n",
    "\n",
    "def pad_sequences_to_fixed_length(sequences, max_len):\n",
    "    \"\"\"\n",
    "    Pads sequences to a fixed length.\n",
    "    \"\"\"\n",
    "    return pad_sequences(sequences, maxlen=max_len, padding=\"post\", value=0)\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of integers back into the structured output string.\n",
    "    \"\"\"\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}  # Reverse the vocabulary\n",
    "    tokens = [inv_vocab[idx] for idx in sequence if idx in inv_vocab and idx not in { vocab[\"<SOS>\"], vocab[\"<EOS>\"],vocab[\"<PAD>\"]} ] \n",
    "    output = \" \".join(tokens)\n",
    "    output = output.replace(\" ( \", \" (\").replace(\"( \", \"(\") #.replace(\" )\", \")\")\n",
    "    return output\n",
    "\n",
    "def decode_sequence_2(sequence, vocab):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of integers back into the structured output string.\n",
    "    \"\"\"\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}  # Reverse the vocabulary\n",
    "    # sequence = sequence.cpu().tolist()  # Convert tensor to a list of integers\n",
    "    tokens = [inv_vocab.get(idx, \"\") for idx in sequence if idx > 0]  # Ignore unknown and put empty char\n",
    "    return \"\".join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data(\n",
    "    X_train, y_train, X_test, y_test, max_len_1=20, max_len_2 = 20\n",
    "):\n",
    "\n",
    "    index = 4\n",
    "    X_vocab, index = build_vocab(X_train,index)  # Build vocabulary from training outputs\n",
    "    X_train_encoded = encode_outputs(X_train, X_vocab)  # Encode training outputs\n",
    "    X_test_encoded = encode_outputs(X_test, X_vocab)  # Encode testing outputs\n",
    "    X_train_processed = pad_sequences_to_fixed_length(X_train_encoded, max_len_1)\n",
    "    X_test_processed = pad_sequences_to_fixed_length(X_test_encoded, max_len_1)\n",
    "\n",
    "    y_train_processed = pad_sequences_to_fixed_length(y_train, max_len_2)\n",
    "    y_test_processed = pad_sequences_to_fixed_length(y_test, max_len_2)\n",
    "\n",
    "\n",
    "    return (\n",
    "        X_train_processed,\n",
    "        X_test_processed,\n",
    "        y_train_processed,\n",
    "        y_test_processed,\n",
    "        X_vocab,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed, X_test_processed, y_train_processed, y_test_processed, vocab  = prepare_data( X_train, y_train, X_test, y_test, max_len_1=40, max_len_2=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  5,  6, ...,  0,  0,  0],\n",
       "       [ 8, 12, 13, ...,  0,  0,  0],\n",
       "       [ 5, 19, 20, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 5, 19, 20, ...,  0,  0,  0],\n",
       "       [ 5, 19, 20, ...,  0,  0,  0],\n",
       "       [ 5, 19, 20, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11, 11, 11, 11, 11, 11, 11, 11, 11,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "        0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_processed[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<UNK>': 3,\n",
       " 'can': 4,\n",
       " 'i': 5,\n",
       " 'have': 6,\n",
       " 'a': 7,\n",
       " 'large': 8,\n",
       " 'bbq': 9,\n",
       " 'pulled': 10,\n",
       " 'pork': 11,\n",
       " 'pie': 12,\n",
       " 'with': 13,\n",
       " 'green': 14,\n",
       " 'pepper': 15,\n",
       " 'and': 16,\n",
       " 'extra': 17,\n",
       " 'peperonni': 18,\n",
       " \"'d\": 19,\n",
       " 'like': 20,\n",
       " 'vegetarian': 21,\n",
       " 'pizza': 22,\n",
       " 'party': 23,\n",
       " 'size': 24,\n",
       " 'stuffed': 25,\n",
       " 'crust': 26,\n",
       " 'american': 27,\n",
       " 'cheese': 28,\n",
       " 'mushroom': 29,\n",
       " 'one': 30,\n",
       " 'personal': 31,\n",
       " 'sized': 32,\n",
       " 'artichoke': 33,\n",
       " 'banana': 34,\n",
       " 'peppperonis': 35,\n",
       " 'low': 36,\n",
       " 'fat': 37,\n",
       " 'want': 38,\n",
       " 'regular': 39,\n",
       " 'without': 40,\n",
       " 'any': 41,\n",
       " 'fried': 42,\n",
       " 'onions': 43,\n",
       " 'little': 44,\n",
       " 'bit': 45,\n",
       " 'of': 46,\n",
       " 'high': 47,\n",
       " 'rise': 48,\n",
       " 'dough': 49,\n",
       " 'lot': 50,\n",
       " 'olive': 51,\n",
       " 'pesto': 52,\n",
       " 'sauce': 53,\n",
       " 'peperonis': 54,\n",
       " 'yellow': 55,\n",
       " 'meatball': 56,\n",
       " '-': 57,\n",
       " 'bean': 58,\n",
       " 'big': 59,\n",
       " 'meat': 60,\n",
       " 'mushrooms': 61,\n",
       " 'pecorino': 62,\n",
       " 'balsamic': 63,\n",
       " 'glaze': 64,\n",
       " 'black': 65,\n",
       " 'chicken': 66,\n",
       " 'mozzarella': 67,\n",
       " 'italian': 68,\n",
       " 'sausage': 69,\n",
       " 'olives': 70,\n",
       " 'pestos': 71,\n",
       " 'lunch': 72,\n",
       " 'alfredo': 73,\n",
       " 'need': 74,\n",
       " 'cheeseburger': 75,\n",
       " 'combination': 76,\n",
       " 'spiced': 77,\n",
       " 'mediterranean': 78,\n",
       " 'caramelized': 79,\n",
       " 'red': 80,\n",
       " 'no': 81,\n",
       " 'apple': 82,\n",
       " 'wood': 83,\n",
       " 'bacon': 84,\n",
       " 'chorrizo': 85,\n",
       " 'vegan': 86,\n",
       " 'pepperoni': 87,\n",
       " 'peperroni': 88,\n",
       " 'peppers': 89,\n",
       " 'not': 90,\n",
       " 'much': 91,\n",
       " 'basil': 92,\n",
       " 'feta': 93,\n",
       " 'medium': 94,\n",
       " 'cumin': 95,\n",
       " 'pickles': 96,\n",
       " 'peperronni': 97,\n",
       " 'meatlover': 98,\n",
       " 'small': 99,\n",
       " 'anchovies': 100,\n",
       " 'buffalo': 101,\n",
       " 'peppperoni': 102,\n",
       " 'hot': 103,\n",
       " 'carrot': 104,\n",
       " 'dried': 105,\n",
       " 'all': 106,\n",
       " 'artichokes': 107,\n",
       " 'peperoni': 108,\n",
       " 'parmesan': 109,\n",
       " 'gluten': 110,\n",
       " '-free': 111,\n",
       " 'garlic': 112,\n",
       " 'powder': 113,\n",
       " 'broccoli': 114,\n",
       " 'grilled': 115,\n",
       " 'deepdish': 116,\n",
       " 'mozarella': 117,\n",
       " 'crusts': 118,\n",
       " 'meatlovers': 119,\n",
       " 'everything': 120,\n",
       " 'carrots': 121,\n",
       " 'arugula': 122,\n",
       " 'pineapple': 123,\n",
       " 'kalamata': 124,\n",
       " 'thick': 125,\n",
       " 'beef': 126,\n",
       " 'bacons': 127,\n",
       " 'balzamic': 128,\n",
       " 'new': 129,\n",
       " 'york': 130,\n",
       " 'style': 131,\n",
       " 'every': 132,\n",
       " 'just': 133,\n",
       " 'pepperonis': 134,\n",
       " 'yorker': 135,\n",
       " 'tomato': 136,\n",
       " 'applewood': 137,\n",
       " 'ham': 138,\n",
       " 'barbecue': 139,\n",
       " 'margherita': 140,\n",
       " 'cheddar': 141,\n",
       " 'onion': 142,\n",
       " 'flakes': 143,\n",
       " 'white': 144,\n",
       " 'anchovy': 145,\n",
       " 'only': 146,\n",
       " 'bay': 147,\n",
       " 'leaves': 148,\n",
       " 'chickens': 149,\n",
       " 'roasted': 150,\n",
       " 'cauliflower': 151,\n",
       " 'lettuce': 152,\n",
       " 'salami': 153,\n",
       " 'free': 154,\n",
       " 'pickle': 155,\n",
       " 'veggie': 156,\n",
       " 'lots': 157,\n",
       " 'hams': 158,\n",
       " 'neapolitan': 159,\n",
       " 'brocoli': 160,\n",
       " 'tomatoes': 161,\n",
       " 'sourdough': 162,\n",
       " 'oregano': 163,\n",
       " 'ranch': 164,\n",
       " 'margarita': 165,\n",
       " 'pea': 166,\n",
       " 'the': 167,\n",
       " 'vegetables': 168,\n",
       " 'ground': 169,\n",
       " 'peas': 170,\n",
       " 'lovers': 171,\n",
       " 'works': 172,\n",
       " 'cherry': 173,\n",
       " 'jalapeno': 174,\n",
       " 'chicago': 175,\n",
       " 'hawaiian': 176,\n",
       " 'tiny': 177,\n",
       " 'pineapples': 178,\n",
       " 'thin': 179,\n",
       " 'spicy': 180,\n",
       " 'chorizo': 181,\n",
       " 'supreme': 182,\n",
       " 'tuna': 183,\n",
       " 'shrimps': 184,\n",
       " 'jalapenos': 185,\n",
       " 'veggies': 186,\n",
       " 'napolitana': 187,\n",
       " 'parsley': 188,\n",
       " 'rosemary': 189,\n",
       " 'keto': 190,\n",
       " 'ricotta': 191,\n",
       " 'mexican': 192,\n",
       " 'spinach': 193,\n",
       " 'pineaple': 194,\n",
       " 'oil': 195,\n",
       " 'beans': 196,\n",
       " 'shrimp': 197,\n",
       " 'pineaples': 198,\n",
       " 'lover': 199,\n",
       " 'topping': 200,\n",
       " 'flake': 201,\n",
       " 'many': 202,\n",
       " 'deep': 203,\n",
       " 'dish': 204,\n",
       " 'toppings': 205,\n",
       " 'two': 206,\n",
       " 'pizzas': 207,\n",
       " 'avoid': 208,\n",
       " 'three': 209,\n",
       " 'hold': 210,\n",
       " 'four': 211,\n",
       " 'pies': 212,\n",
       " 'hate': 213,\n",
       " 'five': 214,\n",
       " 'liter': 215,\n",
       " 'ice': 216,\n",
       " 'teas': 217,\n",
       " 'in': 218,\n",
       " 'cans': 219,\n",
       " '500': 220,\n",
       " '-milliliter': 221,\n",
       " '7': 222,\n",
       " 'ups': 223,\n",
       " 'bottles': 224,\n",
       " '20': 225,\n",
       " 'fluid': 226,\n",
       " 'ounce': 227,\n",
       " 'iced': 228,\n",
       " 'coke': 229,\n",
       " '200': 230,\n",
       " 'ml': 231,\n",
       " 'mountain': 232,\n",
       " 'dews': 233,\n",
       " 'lemon': 234,\n",
       " 'tea': 235,\n",
       " 'doctor': 236,\n",
       " 'sprite': 237,\n",
       " 'fl': 238,\n",
       " 'oz': 239,\n",
       " 'diet': 240,\n",
       " 'coffees': 241,\n",
       " '8': 242,\n",
       " 'pepsi': 243,\n",
       " 'pellegrino': 244,\n",
       " '-liter': 245,\n",
       " 'san': 246,\n",
       " 'bottle': 247,\n",
       " 'dr': 248,\n",
       " 'peper': 249,\n",
       " 'soda': 250,\n",
       " 'sixteen': 251,\n",
       " 'cokes': 252,\n",
       " '-ml': 253,\n",
       " 'eight': 254,\n",
       " 'fanta': 255,\n",
       " '12': 256,\n",
       " 'up': 257,\n",
       " 'sprites': 258,\n",
       " 'fantas': 259,\n",
       " 'zeros': 260,\n",
       " 'coffee': 261,\n",
       " 'pepsis': 262,\n",
       " 'pepers': 263,\n",
       " 'milliliter': 264,\n",
       " 'dew': 265,\n",
       " 'zero': 266,\n",
       " 'perriers': 267,\n",
       " 'perrier': 268,\n",
       " 'water': 269,\n",
       " 'sodas': 270,\n",
       " 'pellegrinos': 271,\n",
       " '16': 272,\n",
       " 'seven': 273,\n",
       " 'ginger': 274,\n",
       " 'ale': 275,\n",
       " 'ales': 276,\n",
       " 'waters': 277,\n",
       " 'zeroes': 278,\n",
       " 'also': 279,\n",
       " 'meatballs': 280,\n",
       " 'sausages': 281,\n",
       " 'tunas': 282,\n",
       " 'napolitan': 283,\n",
       " 'pan': 284,\n",
       " 'med': 285,\n",
       " 'nine': 286,\n",
       " '2': 287,\n",
       " 'six': 288,\n",
       " '13': 289,\n",
       " 'ten': 290,\n",
       " '6': 291,\n",
       " '14': 292,\n",
       " 'eleven': 293,\n",
       " '10': 294,\n",
       " 'an': 295,\n",
       " 'twelve': 296,\n",
       " '3': 297,\n",
       " 'fifteen': 298,\n",
       " '1': 299,\n",
       " 'thirteen': 300,\n",
       " '4': 301,\n",
       " '5': 302,\n",
       " 'fourteen': 303,\n",
       " '11': 304,\n",
       " '9': 305,\n",
       " '15': 306}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  5,  38,   3, ...,   0,   0,   0],\n",
       "       [214,  94, 207, ...,   0,   0,   0],\n",
       "       [  5,  74,   3, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  3,   7,  22, ...,   0,   0,   0],\n",
       "       [  3,   7,   3, ...,   0,   0,   0],\n",
       "       [  3,   5,  74, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [11, 11, 11, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       ...,\n",
       "       [ 0, 11, 11, ...,  0,  0,  0],\n",
       "       [ 0,  0,  0, ...,  0,  0,  0],\n",
       "       [ 0, 11,  0, ...,  0,  0,  0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348, 40), (348, 40))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_processed.shape, y_test_processed.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2456446, 40), (2456446, 40))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape, y_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.long) \n",
    "        self.targets = torch.tensor(targets, dtype=torch.long) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src_input_ids\": self.inputs[idx],\n",
    "            \"tgt_input_ids\": self.targets[idx],\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "train_dataset = SequenceDataset(X_train_processed, y_train_processed)\n",
    "test_dataset = SequenceDataset(X_test_processed, y_test_processed)\n",
    "\n",
    "batch_size = 128  # Adjust based GPU ;-;  memory\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import BatchNorm1d\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # Bidirectional LSTM\n",
    "        self.bilstm_1 = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "\n",
    "        # Batch normalization\n",
    "        #self.batchnorm_1 = BatchNorm1d(hidden_dim * 2)\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc2 = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        # BiLSTM layer\n",
    "        lstm_out, _ = self.bilstm_1(embedded)\n",
    "\n",
    "        # # # Batch normalization\n",
    "        # lstm_out = lstm_out.permute(0, 2, 1)\n",
    "        # lstm_out = self.batchnorm_1(lstm_out)\n",
    "        # lstm_out = lstm_out.permute(0, 2, 1)\n",
    "\n",
    "        output = self.fc2(lstm_out)\n",
    "        return F.log_softmax(output, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_dim =  X_train_processed.shape[1] \n",
    "embedding_dim = 128\n",
    "hidden_dim = 128  \n",
    "output_dim = y_train_processed.shape[1]  \n",
    "num_layers = 2  \n",
    "dropout = 0.2 \n",
    "\n",
    "device = torch.device(\"cpu\") #\"cuda\" if torch.cuda.is_available() else\n",
    "model = BiLSTMModel(input_dim, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_accuracy(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch[\"src_input_ids\"].to(device)\n",
    "            tgt = batch[\"tgt_input_ids\"].to(device)\n",
    "\n",
    "            output = model(src)\n",
    "            output_dim = output.shape[-1]\n",
    "\n",
    "            # Flatten outputs and targets\n",
    "            output = output.view(-1, output_dim)  # Shape: (batch_size * seq_len, output_dim)\n",
    "            tgt = tgt.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "            # # Apply mask to remove padding tokens\n",
    "            # mask = tgt != 0  # Mask to ignore padding indices\n",
    "            # output = output[mask]  # Filter model outputs\n",
    "            # tgt = tgt[mask]  # Filter targets\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Calculate accuracy\n",
    "            predictions = output.argmax(dim=1)  # Get the index of the max log-probability\n",
    "            correct_tokens += (predictions == tgt).sum().item()\n",
    "            total_tokens += tgt.size(0)\n",
    "\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "    return epoch_loss / len(dataloader), accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress:   0%|          | 0/19191 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m tgt \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtgt_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Target tokens\u001b[39;00m\n\u001b[0;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 18\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src)  \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[0;32m     19\u001b[0m output_dim \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Flatten outputs and targets for loss computation\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[16], line 29\u001b[0m, in \u001b[0;36mBiLSTMModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;66;03m# Embedding layer\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m     embedded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding(x)\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;66;03m# BiLSTM layer\u001b[39;00m\n\u001b[0;32m     32\u001b[0m     lstm_out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbilstm_1(embedded)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py:163\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 163\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39membedding(\n\u001b[0;32m    164\u001b[0m         \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_idx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_norm,\n\u001b[0;32m    165\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_type, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale_grad_by_freq, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparse)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:2237\u001b[0m, in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2231\u001b[0m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[0;32m   2232\u001b[0m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[0;32m   2234\u001b[0m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[0;32m   2235\u001b[0m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[0;32m   2236\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[1;32m-> 2237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39membedding(weight, \u001b[38;5;28minput\u001b[39m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Use for multi-class classification ignore_index=0 for padding\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0071, weight_decay=1e-5)\n",
    "\n",
    "for epoch in range(20):  # Number of epochs\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(train_dataloader)\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training Progress\", unit=\"batch\", leave=True)\n",
    "\n",
    "    for batch_idx, batch in enumerate(progress_bar):  # Assuming a DataLoader is used\n",
    "        src = batch[\"src_input_ids\"].to(device)  # Input tokens\n",
    "        tgt = batch[\"tgt_input_ids\"].to(device)  # Target tokens\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)  # Forward pass\n",
    "        output_dim = output.shape[-1]\n",
    "\n",
    "        # Flatten outputs and targets for loss computation\n",
    "        output = output.view(-1, output_dim)  # Shape: (batch_size * seq_len, output_dim)\n",
    "        tgt = tgt.view(-1)  # Shape: (batch_size * seq_len)\n",
    "\n",
    "        # Apply mask to remove padding tokens\n",
    "        # mask = tgt != 0  # Mask to ignore padding indices\n",
    "        # output = output[mask]  # Filter model outputs\n",
    "        # tgt = tgt[mask]  # Filter targets\n",
    "        print(output.shape)\n",
    "        print(tgt.shape)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / (batch_idx + 1)\n",
    "        progress_bar.set_description(f\"Training Progress: Batch {batch_idx + 1}/{total_batches}, Avg Loss: {avg_loss:.8f}\")\n",
    "\n",
    "    val_loss, accuracy = evaluate_model_with_accuracy(model, test_dataloader, criterion, device)\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_dataloader):.8f}, Val Loss: {val_loss:.8f}, Accuracy: {accuracy * 100:.4f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"../weights/Bilstm_topdecoupled.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
