{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data saved to ../dataset/training_data_bilstm.json.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\"Tokenizes a sentence into words.\"\"\"\n",
    "    return sentence.split()\n",
    "\n",
    "def extract_labels(src: str, top_decoupled: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts labels for each token in the input sentence based on the TOP-DECOUPLED field.\n",
    "\n",
    "    Args:\n",
    "        src (str): Input sentence.\n",
    "        top_decoupled (str): Decoupled TOP structure.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of labels for each token in the input.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(src)\n",
    "    labels = [\"NONE\"] * len(tokens)\n",
    "\n",
    "    # Map of tags to patterns\n",
    "    tag_patterns = {\n",
    "        \"NUMBER\": r\"\\(NUMBER [^)]+ \\)\",\n",
    "        \"SIZE\": r\"\\(SIZE [^)]+ \\)\",\n",
    "        \"STYLE\": r\"\\(STYLE [^)]+ \\)\",\n",
    "        \"TOPPING\": r\"\\(TOPPING [^)]+ \\)\",\n",
    "        \"COMPLEX_TOPPING\": r\"\\(COMPLEX_TOPPING .*?\\)\",\n",
    "        \"QUANTITY\": r\"\\(QUANTITY [^)]+ \\)\",\n",
    "        \"NOT\": r\"\\(NOT \\(TOPPING [^)]+ \\)\\)\"\n",
    "    }\n",
    "\n",
    "    # Generate labels for tokens based on patterns\n",
    "    for tag, pattern in tag_patterns.items():\n",
    "        matches = re.finditer(pattern, top_decoupled)\n",
    "        for match in matches:\n",
    "            span_text = match.group(0)\n",
    "            span_tokens = tokenize(span_text)\n",
    "            for token in tokens:\n",
    "                if token in span_tokens:\n",
    "                    idx = tokens.index(token)\n",
    "                    labels[idx] = tag\n",
    "\n",
    "    return labels\n",
    "\n",
    "def generate_training_data(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generates training data for the BiLSTM model.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON file.\n",
    "        output_file (str): Path to save the generated training data.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            src = record[\"train.SRC\"]\n",
    "            top_decoupled = record[\"train.TOP-DECOUPLED\"]\n",
    "\n",
    "            labels = extract_labels(src, top_decoupled)\n",
    "            training_instance = {\n",
    "                \"text\": src,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "            outfile.write(json.dumps(training_instance) + \"\\n\")\n",
    "\n",
    "# File paths\n",
    "input_file = \"../dataset/PIZZA_train.json\"\n",
    "output_file = \"../dataset/training_data_bilstm.json\"\n",
    "\n",
    "# Generate and save the training data\n",
    "generate_training_data(input_file, output_file)\n",
    "\n",
    "print(f\"Training data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ORDER', 'O'),\n",
       " ('i', 'O'),\n",
       " ('need', 'O'),\n",
       " ('PIZZAORDER', 'O'),\n",
       " ('NUMBER', 'O'),\n",
       " ('a', 'B-NUMBER'),\n",
       " ('SIZE', 'O'),\n",
       " ('medium', 'B-SIZE'),\n",
       " ('TOPPING', 'O'),\n",
       " ('ham', 'B-TOPPING'),\n",
       " ('and', 'O'),\n",
       " ('TOPPING', 'O'),\n",
       " ('pineapple', 'B-TOPPING'),\n",
       " ('pizza', 'O'),\n",
       " ('and', 'O'),\n",
       " ('DRINKORDER', 'O'),\n",
       " ('NUMBER', 'O'),\n",
       " ('a', 'B-NUMBER'),\n",
       " ('SIZE', 'O'),\n",
       " ('small', 'B-SIZE'),\n",
       " ('DRINKTYPE', 'O'),\n",
       " ('iced', 'B-DRINKTYPE'),\n",
       " ('tea', 'I-DRINKTYPE')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    # Extract tokens: parentheses or sequences of non-whitespace, non-parenthesis characters.\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    # Parse tokens into a nested list structure\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "\n",
    "# Keys that indicate entities we want to label\n",
    "ENTITY_KEYS = {\n",
    "    \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\", \"COMPLEX_TOPPING\", \"QUANTITY\",\n",
    "    \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\"\n",
    "}\n",
    "\n",
    "# Keys that indicate higher-level structures (orders)\n",
    "ORDER_KEYS = {\"PIZZAORDER\", \"DRINKORDER\"}\n",
    "\n",
    "def label_entity_tokens(values, entity_type, negated=False):\n",
    "    \"\"\"\n",
    "    Given a list of tokens (values) inside an entity,\n",
    "    return a list of (token, label) pairs with B-/I- tagging.\n",
    "\n",
    "    entity_type: e.g. \"NUMBER\", \"TOPPING\", etc.\n",
    "    negated: boolean, if True we prepend \"NEG_\" to the entity_type\n",
    "    \"\"\"\n",
    "    if negated:\n",
    "        prefix = \"NEG_\"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "\n",
    "    labels = []\n",
    "    for i, val in enumerate(values):\n",
    "        tag = \"B-\" + prefix + entity_type if i == 0 else \"I-\" + prefix + entity_type\n",
    "        labels.append((val, tag))\n",
    "    return labels\n",
    "\n",
    "def flatten_nested_structure(structure, order_context=None, negated=False):\n",
    "    \"\"\"\n",
    "    Recursively descend through the parsed structure and produce (token, label) pairs.\n",
    "    All tokens in structure are from the same tokenization as the original input.\n",
    "\n",
    "    order_context: tracks if we are inside PIZZAORDER or DRINKORDER\n",
    "    negated: tracks if we are inside a NOT block\n",
    "    \"\"\"\n",
    "    if not isinstance(structure, list):\n",
    "        # It's a single token (string)\n",
    "        # If it's not an entity value token, it's just structure or unknown => O\n",
    "        # We'll label it as O\n",
    "        return [(structure, \"O\")]\n",
    "\n",
    "    # structure is a list\n",
    "    # The first element might be a key like ORDER, PIZZAORDER, NUMBER, etc., or might be nested.\n",
    "    if len(structure) == 0:\n",
    "        return []\n",
    "\n",
    "    first = structure[0]\n",
    "    if isinstance(first, list):\n",
    "        # This means we don't have a key at the start; just nested lists.\n",
    "        # Flatten them recursively.\n",
    "        result = []\n",
    "        for elem in structure:\n",
    "            result.extend(flatten_nested_structure(elem, order_context=order_context, negated=negated))\n",
    "        return result\n",
    "    else:\n",
    "        # first is a token (string)\n",
    "        key = first\n",
    "        \n",
    "        if key == \"VOLUME\":\n",
    "            key = \"SIZE\"\n",
    "\n",
    "        if key == \"ORDER\":\n",
    "            # Just go through its children\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=None, negated=False))\n",
    "            return result\n",
    "\n",
    "        elif key in ORDER_KEYS:\n",
    "            # Entering a pizza or drink order. We label the key itself as O.\n",
    "            # Inside this, we find its fields.\n",
    "            new_order_context = key  # \"PIZZAORDER\" or \"DRINKORDER\"\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=new_order_context, negated=negated))\n",
    "            return result\n",
    "\n",
    "        elif key == \"NOT\":\n",
    "            # Negation block. We set negated=True inside this.\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=order_context, negated=True))\n",
    "            return result\n",
    "\n",
    "        elif key in ENTITY_KEYS:\n",
    "            # This is an entity. Extract its values (non-list tokens).\n",
    "            # Example: (NUMBER one), (TOPPING cheese), (STYLE thin crust)\n",
    "            # We'll consider everything after key that is a token as part of this entity's value.\n",
    "            result = [(key, \"O\")]\n",
    "            entity_tokens = []\n",
    "            for elem in structure[1:]:\n",
    "                if isinstance(elem, list):\n",
    "                    # Complex entity may have nested (e.g. COMPLEX_TOPPING)\n",
    "                    # Flatten it and extract from subentities\n",
    "                    sub_result = flatten_nested_structure(elem, order_context=order_context, negated=negated)\n",
    "                    # sub_result might contain multiple tokens. They are already labeled, but we want them labeled as part of this entity.\n",
    "                    # Actually, for COMPLEX_TOPPING, we have QUANTITY and TOPPING inside it.\n",
    "                    # It's safer to just pass through and let sub-entities handle their own labeling.\n",
    "                    result.extend(sub_result)\n",
    "                else:\n",
    "                    # It's a direct token value\n",
    "                    entity_tokens.append(elem)\n",
    "\n",
    "            # If entity_tokens are present directly under this key, label them:\n",
    "            if entity_tokens:\n",
    "                # The entity_type should reflect the key.\n",
    "                # For COMPLEX_TOPPING, we actually break it down into QUANTITY and TOPPING subfields,\n",
    "                # but if there's a direct token (there shouldn't be normally), treat it similarly.\n",
    "                # Usually COMPLEX_TOPPING breaks down into more entities. If so, entity_tokens here might be empty.\n",
    "                entity_type = key\n",
    "                # label these entity tokens according to entity_type\n",
    "                labeled = label_entity_tokens(entity_tokens, entity_type, negated=negated)\n",
    "                result.extend(labeled)\n",
    "\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            # This is not a recognized key. It's probably a token or extra word not fitting into a known entity.\n",
    "            # Label as O.\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=order_context, negated=negated))\n",
    "            return result\n",
    "\n",
    "def post_process_complex_and_neg(result):\n",
    "    \"\"\"\n",
    "    After the first pass, we might have labeled QUANTITY and TOPPING inside a NOT or COMPLEX_TOPPING as if they were separate.\n",
    "    But we've already handled negation inline.\n",
    "    This step might be optional if we've handled negation tagging inline.\n",
    "    \"\"\"\n",
    "\n",
    "    # In this implementation, we already handle negation inline, so no additional step needed.\n",
    "    return result\n",
    "\n",
    "def label_input(text):\n",
    "    tokens = tokenize(text)\n",
    "    parsed = parse_tokens(tokens)\n",
    "\n",
    "    # Flatten and label\n",
    "    labeled = []\n",
    "    for elem in parsed:\n",
    "        labeled.extend(flatten_nested_structure(elem, order_context=None, negated=False))\n",
    "\n",
    "    # Post-process if needed\n",
    "    labeled = post_process_complex_and_neg(labeled)\n",
    "\n",
    "    return labeled\n",
    "\n",
    "# Example:\n",
    "input_str = \"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE medium ) (TOPPING ham ) and (TOPPING pineapple ) pizza ) and (DRINKORDER (NUMBER a ) (VOLUME small ) (DRINKTYPE iced tea ) ) )\"\n",
    "\n",
    "labeled_output = label_input(input_str)\n",
    "labeled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'O'), ('need', 'O'), ('a', 'B-NUMBER'), ('medium', 'B-SIZE'), ('ham', 'B-TOPPING'), ('and', 'O'), ('pineapple', 'B-TOPPING'), ('pizza', 'O'), ('and', 'O'), ('a', 'B-NUMBER'), ('small', 'B-SIZE'), ('iced', 'B-DRINKTYPE'), ('tea', 'I-DRINKTYPE')]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"i need a medium ham and pineapple pizza and a small iced tea\"\n",
    "\n",
    "# Convert labeled_output to a dictionary for quick lookup\n",
    "label_dict = {tok.lower(): lbl for tok, lbl in labeled_output}\n",
    "\n",
    "input_tokens = input_text.split()\n",
    "labeled_input = []\n",
    "\n",
    "for token in input_tokens:\n",
    "    # Get label from label_dict if token is found, otherwise 'O'\n",
    "    token_label = label_dict.get(token.lower(), 'O')\n",
    "    labeled_input.append((token, token_label))\n",
    "\n",
    "print(labeled_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
