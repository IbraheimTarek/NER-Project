{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "import re\n",
    "\n",
    "def tokenize(sentence: str) -> List[str]:\n",
    "    \"\"\"Tokenizes a sentence into words.\"\"\"\n",
    "    return sentence.split()\n",
    "\n",
    "def extract_labels(src: str, top_decoupled: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts labels for each token in the input sentence based on the TOP-DECOUPLED field.\n",
    "\n",
    "    Args:\n",
    "        src (str): Input sentence.\n",
    "        top_decoupled (str): Decoupled TOP structure.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of labels for each token in the input.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(src)\n",
    "    labels = [\"NONE\"] * len(tokens)\n",
    "\n",
    "    # Map of tags to patterns\n",
    "    tag_patterns = {\n",
    "        \"NUMBER\": r\"\\(NUMBER [^)]+ \\)\",\n",
    "        \"SIZE\": r\"\\(SIZE [^)]+ \\)\",\n",
    "        \"STYLE\": r\"\\(STYLE [^)]+ \\)\",\n",
    "        \"TOPPING\": r\"\\(TOPPING [^)]+ \\)\",\n",
    "        \"COMPLEX_TOPPING\": r\"\\(COMPLEX_TOPPING .*?\\)\",\n",
    "        \"QUANTITY\": r\"\\(QUANTITY [^)]+ \\)\",\n",
    "        \"NOT\": r\"\\(NOT \\(TOPPING [^)]+ \\)\\)\"\n",
    "    }\n",
    "\n",
    "    # Generate labels for tokens based on patterns\n",
    "    for tag, pattern in tag_patterns.items():\n",
    "        matches = re.finditer(pattern, top_decoupled)\n",
    "        for match in matches:\n",
    "            span_text = match.group(0)\n",
    "            span_tokens = tokenize(span_text)\n",
    "            for token in tokens:\n",
    "                if token in span_tokens:\n",
    "                    idx = tokens.index(token)\n",
    "                    labels[idx] = tag\n",
    "\n",
    "    return labels\n",
    "\n",
    "def generate_training_data(input_file: str, output_file: str):\n",
    "    \"\"\"\n",
    "    Generates training data for the BiLSTM model.\n",
    "\n",
    "    Args:\n",
    "        input_file (str): Path to the input JSON file.\n",
    "        output_file (str): Path to save the generated training data.\n",
    "    \"\"\"\n",
    "    with open(input_file, \"r\") as infile, open(output_file, \"w\") as outfile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            src = record[\"train.SRC\"]\n",
    "            top_decoupled = record[\"train.TOP-DECOUPLED\"]\n",
    "\n",
    "            labels = extract_labels(src, top_decoupled)\n",
    "            training_instance = {\n",
    "                \"text\": src,\n",
    "                \"labels\": labels\n",
    "            }\n",
    "            outfile.write(json.dumps(training_instance) + \"\\n\")\n",
    "\n",
    "# # File paths\n",
    "# input_file = \"../dataset/PIZZA_train.json\"\n",
    "# output_file = \"../dataset/training_data_bilstm.json\"\n",
    "\n",
    "# # Generate and save the training data\n",
    "# generate_training_data(input_file, output_file)\n",
    "\n",
    "# print(f\"Training data saved to {output_file}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entity recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('i', 'O'),\n",
       " ('need', 'O'),\n",
       " ('a', 'B-NUMBER'),\n",
       " ('medium', 'B-SIZE'),\n",
       " ('ham', 'B-TOPPING'),\n",
       " ('and', 'O'),\n",
       " ('pineapple', 'B-TOPPING'),\n",
       " ('pizza', 'O'),\n",
       " ('and', 'O'),\n",
       " ('a', 'B-NUMBER'),\n",
       " ('small', 'B-SIZE'),\n",
       " ('iced', 'B-DRINKTYPE'),\n",
       " ('tea', 'I-DRINKTYPE')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    # Extract tokens: parentheses or sequences of non-whitespace, non-parenthesis characters.\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    # Parse tokens into a nested list structure\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "\n",
    "# Keys that indicate entities we want to label\n",
    "ENTITY_KEYS = {\n",
    "    \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\", \"COMPLEX_TOPPING\", \"QUANTITY\",\n",
    "    \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\"\n",
    "}\n",
    "\n",
    "# Keys that indicate higher-level structures (orders)\n",
    "ORDER_KEYS = {\"PIZZAORDER\", \"DRINKORDER\"}\n",
    "\n",
    "def label_entity_tokens(values, entity_type, negated=False):\n",
    "    \"\"\"\n",
    "    Given a list of tokens (values) inside an entity,\n",
    "    return a list of (token, label) pairs with B-/I- tagging.\n",
    "\n",
    "    entity_type: e.g. \"NUMBER\", \"TOPPING\", etc.\n",
    "    negated: boolean, if True we prepend \"NEG_\" to the entity_type\n",
    "    \"\"\"\n",
    "    if negated:\n",
    "        prefix = \"NEG_\"\n",
    "    else:\n",
    "        prefix = \"\"\n",
    "\n",
    "    labels = []\n",
    "    for i, val in enumerate(values):\n",
    "        tag = \"B-\" + prefix + entity_type if i == 0 else \"I-\" + prefix + entity_type\n",
    "        labels.append((val, tag))\n",
    "    return labels\n",
    "\n",
    "def flatten_nested_structure(structure, order_context=None, negated=False):\n",
    "    \"\"\"\n",
    "    Recursively descend through the parsed structure and produce (token, label) pairs.\n",
    "    All tokens in structure are from the same tokenization as the original input.\n",
    "\n",
    "    order_context: tracks if we are inside PIZZAORDER or DRINKORDER\n",
    "    negated: tracks if we are inside a NOT block\n",
    "    \"\"\"\n",
    "    if not isinstance(structure, list):\n",
    "        # It's a single token (string)\n",
    "        # If it's not an entity value token, it's just structure or unknown => O\n",
    "        # We'll label it as O\n",
    "        return [(structure, \"O\")]\n",
    "\n",
    "    # structure is a list\n",
    "    # The first element might be a key like ORDER, PIZZAORDER, NUMBER, etc., or might be nested.\n",
    "    if len(structure) == 0:\n",
    "        return []\n",
    "\n",
    "    first = structure[0]\n",
    "    if isinstance(first, list):\n",
    "        # This means we don't have a key at the start; just nested lists.\n",
    "        # Flatten them recursively.\n",
    "        result = []\n",
    "        for elem in structure:\n",
    "            result.extend(flatten_nested_structure(elem, order_context=order_context, negated=negated))\n",
    "        return result\n",
    "    else:\n",
    "        # first is a token (string)\n",
    "        key = first\n",
    "        \n",
    "        if key == \"VOLUME\":\n",
    "            key = \"SIZE\"\n",
    "\n",
    "        if key == \"ORDER\":\n",
    "            # Just go through its children\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=None, negated=False))\n",
    "            return result\n",
    "\n",
    "        elif key in ORDER_KEYS:\n",
    "            # Entering a pizza or drink order. We label the key itself as O.\n",
    "            # Inside this, we find its fields.\n",
    "            new_order_context = key  # \"PIZZAORDER\" or \"DRINKORDER\"\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=new_order_context, negated=negated))\n",
    "            return result\n",
    "\n",
    "        elif key == \"NOT\":\n",
    "            # Negation block. We set negated=True inside this.\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=order_context, negated=True))\n",
    "            return result\n",
    "\n",
    "        elif key in ENTITY_KEYS:\n",
    "            # This is an entity. Extract its values (non-list tokens).\n",
    "            # Example: (NUMBER one), (TOPPING cheese), (STYLE thin crust)\n",
    "            # We'll consider everything after key that is a token as part of this entity's value.\n",
    "            result = [(key, \"O\")]\n",
    "            entity_tokens = []\n",
    "            for elem in structure[1:]:\n",
    "                if isinstance(elem, list):\n",
    "                    # Complex entity may have nested (e.g. COMPLEX_TOPPING)\n",
    "                    # Flatten it and extract from subentities\n",
    "                    sub_result = flatten_nested_structure(elem, order_context=order_context, negated=negated)\n",
    "                    # sub_result might contain multiple tokens. They are already labeled, but we want them labeled as part of this entity.\n",
    "                    # Actually, for COMPLEX_TOPPING, we have QUANTITY and TOPPING inside it.\n",
    "                    # It's safer to just pass through and let sub-entities handle their own labeling.\n",
    "                    result.extend(sub_result)\n",
    "                else:\n",
    "                    # It's a direct token value\n",
    "                    entity_tokens.append(elem)\n",
    "\n",
    "            # If entity_tokens are present directly under this key, label them:\n",
    "            if entity_tokens:\n",
    "                # The entity_type should reflect the key.\n",
    "                # For COMPLEX_TOPPING, we actually break it down into QUANTITY and TOPPING subfields,\n",
    "                # but if there's a direct token (there shouldn't be normally), treat it similarly.\n",
    "                # Usually COMPLEX_TOPPING breaks down into more entities. If so, entity_tokens here might be empty.\n",
    "                entity_type = key\n",
    "                # label these entity tokens according to entity_type\n",
    "                labeled = label_entity_tokens(entity_tokens, entity_type, negated=negated)\n",
    "                result.extend(labeled)\n",
    "\n",
    "            return result\n",
    "\n",
    "        else:\n",
    "            # This is not a recognized key. It's probably a token or extra word not fitting into a known entity.\n",
    "            # Label as O.\n",
    "            result = [(key, \"O\")]\n",
    "            for elem in structure[1:]:\n",
    "                result.extend(flatten_nested_structure(elem, order_context=order_context, negated=negated))\n",
    "            return result\n",
    "\n",
    "def post_process_complex_and_neg(result):\n",
    "    \"\"\"\n",
    "    After the first pass, we might have labeled QUANTITY and TOPPING inside a NOT or COMPLEX_TOPPING as if they were separate.\n",
    "    But we've already handled negation inline.\n",
    "    This step might be optional if we've handled negation tagging inline.\n",
    "    \"\"\"\n",
    "\n",
    "    # In this implementation, we already handle negation inline, so no additional step needed.\n",
    "    return result\n",
    "\n",
    "def label_input(text):\n",
    "    tokens = tokenize(text)\n",
    "    parsed = parse_tokens(tokens)\n",
    "\n",
    "    # Flatten and label\n",
    "    labeled = []\n",
    "    for elem in parsed:\n",
    "        labeled.extend(flatten_nested_structure(elem, order_context=None, negated=False))\n",
    "\n",
    "    # Post-process if needed\n",
    "    labeled = post_process_complex_and_neg(labeled)\n",
    "    \n",
    "    # remove all tuples where the first element is a ENTITY_KEY or ORDER/PizzaOrder/DrinkOrder\n",
    "    labeled = [x for x in labeled if x[0] not in ENTITY_KEYS and x[0] not in ORDER_KEYS and x[0] != \"ORDER\"]\n",
    "\n",
    "    return labeled\n",
    "\n",
    "# Example:\n",
    "input_str = \"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE medium ) (TOPPING ham ) and (TOPPING pineapple ) pizza ) and (DRINKORDER (NUMBER a ) (VOLUME small ) (DRINKTYPE iced tea ) ) )\"\n",
    "\n",
    "labeled_output = label_input(input_str)\n",
    "labeled_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the number of tokens in the largest sentence in the dev set\n",
    "# max_len = 0\n",
    "# with open(\"../dataset/PIZZA_train.json\", \"r\") as f:\n",
    "#     for line in f:\n",
    "#         record = json.loads(line)\n",
    "#         src = record[\"train.SRC\"]\n",
    "#         tokens = tokenize(src)\n",
    "#         if len(tokens) > max_len:\n",
    "#             max_len = len(tokens)\n",
    "            \n",
    "# print(max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the input to the output and map them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'O'), ('would', 'O'), ('like', 'O'), ('a', 'B-NUMBER'), ('pizza', 'O'), ('with', 'O'), ('onions', 'O'), ('and', 'O'), ('ham', 'B-TOPPING'), ('and', 'O'), ('tuna', 'O'), ('please', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "input_text = \"i would like a pizza with onions and ham and tuna please\"\n",
    "\n",
    "# Convert labeled_output to a dictionary mapping tokens to a list of (label, order_number)\n",
    "label_dict = defaultdict(list)\n",
    "for tok, lbl in labeled_output:\n",
    "    label_dict[tok.lower()].append(lbl)\n",
    "\n",
    "input_tokens = input_text.split()\n",
    "labeled_input = []\n",
    "\n",
    "# Track used labels to avoid reusing the same label for the same word\n",
    "used_labels = defaultdict(int)\n",
    "\n",
    "for token in input_tokens:\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in label_dict:\n",
    "        # Get the next unused label for this token\n",
    "        label_index = used_labels[token_lower]\n",
    "        if label_index < len(label_dict[token_lower]):\n",
    "            token_label = label_dict[token_lower][label_index]\n",
    "            used_labels[token_lower] += 1  # Mark this label as used\n",
    "        else:\n",
    "            token_label = 'O'  # Default if all labels are used\n",
    "    else:\n",
    "        token_label = 'O'  # Default for unmatched tokens\n",
    "\n",
    "    labeled_input.append((token, token_label))\n",
    "\n",
    "print(labeled_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B-DRINKTYPE': 1, 'I-DRINKTYPE': 2, 'B-SIZE': 3, 'I-SIZE': 4, 'B-NUMBER': 5, 'I-NUMBER': 6, 'B-CONTAINERTYPE': 7, 'I-CONTAINERTYPE': 8, 'B-COMPLEX_TOPPING': 9, 'I-COMPLEX_TOPPING': 10, 'B-VOLUME': 11, 'I-VOLUME': 12, 'B-TOPPING': 13, 'I-TOPPING': 14, 'B-QUANTITY': 15, 'I-QUANTITY': 16, 'B-STYLE': 17, 'I-STYLE': 18, 'O': 19}\n"
     ]
    }
   ],
   "source": [
    "ENTITY_KEYS = {\n",
    "    \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\", \"COMPLEX_TOPPING\", \"QUANTITY\",\n",
    "    \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\"\n",
    "}\n",
    "\n",
    "# Define a mapping for entity keys to numerical labels\n",
    "LABEL_MAP = {'B-DRINKTYPE': 1, 'I-DRINKTYPE': 2, 'B-SIZE': 3, 'I-SIZE': 4, 'B-NUMBER': 5, 'I-NUMBER': 6, 'B-CONTAINERTYPE': 7, 'I-CONTAINERTYPE': 8, 'B-COMPLEX_TOPPING': 9, 'I-COMPLEX_TOPPING': 10, 'B-VOLUME': 11, 'I-VOLUME': 12, 'B-TOPPING': 13, 'I-TOPPING': 14, 'B-QUANTITY': 15, 'I-QUANTITY': 16, 'B-STYLE': 17, 'I-STYLE': 18, 'O': 19}\n",
    "\n",
    "def transform_to_labels(labeled_output):\n",
    "    numerical_labels = [LABEL_MAP.get(label, LABEL_MAP[\"O\"]) for _, label in labeled_output]\n",
    "    return numerical_labels\n",
    "\n",
    "\n",
    "def process_pizza_train(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        for line in infile:\n",
    "            record = json.loads(line)\n",
    "            src_text = record[\"train.SRC\"]\n",
    "            src_top = record[\"train.TOP\"]\n",
    "            labeled_output = label_input(src_top)\n",
    "            numerical_labels = transform_to_labels(labeled_output)\n",
    "            training_instance = {\n",
    "                \"text\": src_text,\n",
    "                \"labels\": numerical_labels\n",
    "            }\n",
    "            outfile.write(json.dumps(training_instance) + \"\\n\")\n",
    "\n",
    "# Paths to the input and output files\n",
    "input_file = \"../dataset/PIZZA_train.json\"\n",
    "output_file = \"../dataset/PIZZA_train_model2.json\"\n",
    "\n",
    "# Process the data\n",
    "process_pizza_train(input_file, output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('a', 'PIZZAORDER', 1), ('medium', 'PIZZAORDER', 1), ('ham', 'PIZZAORDER', 1), ('and', 'PIZZAORDER', 1), ('pineapple', 'PIZZAORDER', 1), ('pizza', 'PIZZAORDER', 1), ('a', 'DRINKORDER', 2), ('small', 'DRINKORDER', 2), ('iced', 'DRINKORDER', 2), ('tea', 'DRINKORDER', 2)]\n"
     ]
    }
   ],
   "source": [
    "def extract_orders(structure, order_index=1):\n",
    "    \"\"\"\n",
    "    Given the nested parsed structure, find all orders and their tokens.\n",
    "    Return a list of tuples (word, order_type, order_sequence_number).\n",
    "    order_index is used to track the sequence number of orders encountered so far.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    if not isinstance(structure, list) or len(structure) == 0:\n",
    "        return results, order_index\n",
    "\n",
    "    first = structure[0]\n",
    "    if isinstance(first, list):\n",
    "        # first is a list, recursively process each sub-element\n",
    "        for elem in structure:\n",
    "            sub_results, order_index = extract_orders(elem, order_index)\n",
    "            results.extend(sub_results)\n",
    "        return results, order_index\n",
    "\n",
    "    # Check if first is a recognized order key\n",
    "    if isinstance(first, str) and first in ORDER_KEYS:\n",
    "        # We found a new order. Determine the order type.\n",
    "        order_type = \"PIZZAORDER\" if first == \"PIZZAORDER\" else \"DRINKORDER\"\n",
    "        \n",
    "        # We have encountered a new order, so use the current order_index as its sequence\n",
    "        current_order_sequence = order_index\n",
    "        order_index += 1  # increment for the next order\n",
    "\n",
    "        content_tokens = []\n",
    "\n",
    "        # Collect tokens from this order block (ignoring structural keys)\n",
    "        for elem in structure[1:]:\n",
    "            content_tokens.extend(collect_tokens(elem))\n",
    "\n",
    "        # Create tuples (word, order_type, order_sequence)\n",
    "        for tok in content_tokens:\n",
    "            results.append((tok, order_type, current_order_sequence))\n",
    "\n",
    "        return results, order_index\n",
    "    else:\n",
    "        # Not an ORDER key, recursively check elements\n",
    "        for elem in structure:\n",
    "            sub_results, order_index = extract_orders(elem, order_index)\n",
    "            results.extend(sub_results)\n",
    "        return results, order_index\n",
    "\n",
    "def collect_tokens(node):\n",
    "    \"\"\"\n",
    "    Collect all non-structural tokens from a node (which could be nested lists).\n",
    "    \"\"\"\n",
    "    collected = []\n",
    "    if isinstance(node, list):\n",
    "        for sub in node:\n",
    "            sub_tokens = collect_tokens(sub)\n",
    "            collected.extend(sub_tokens)\n",
    "    else:\n",
    "        # node is a token (string)\n",
    "        if node not in [\"(\", \")\"] and not is_structural_key(node):\n",
    "            collected.append(node)\n",
    "    return collected\n",
    "\n",
    "def is_structural_key(token):\n",
    "    # Keys for structure, not actual content words\n",
    "    return token in [\n",
    "        \"ORDER\",\"PIZZAORDER\",\"DRINKORDER\",\"NUMBER\",\"SIZE\",\"STYLE\",\"TOPPING\",\n",
    "        \"COMPLEX_TOPPING\",\"QUANTITY\",\"VOLUME\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"NOT\"\n",
    "    ]\n",
    "\n",
    "# Example input\n",
    "input_str = \"(ORDER i need (PIZZAORDER (NUMBER a ) (SIZE medium ) (TOPPING ham ) and (TOPPING pineapple ) pizza ) and (DRINKORDER (NUMBER a ) (VOLUME small ) (DRINKTYPE iced tea ) ) )\"\n",
    "\n",
    "tokens = tokenize(input_str)\n",
    "parsed = parse_tokens(tokens)\n",
    "\n",
    "order_info, _ = extract_orders(parsed)\n",
    "print(order_info)\n",
    "# Example output might be something like:\n",
    "# [('i', 'pizza', 1), ('need', 'pizza', 1), ('and', 'pizza', 1), ('pizza', 'pizza', 1),\n",
    "#  ('and', 'drink', 2), ('iced', 'drink', 2), ('tea', 'drink', 2)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('i', 'O', None), ('need', 'O', None), ('a', 'PIZZAORDER', 1), ('medium', 'PIZZAORDER', 1), ('ham', 'PIZZAORDER', 1), ('and', 'PIZZAORDER', 1), ('pineapple', 'PIZZAORDER', 1), ('pizza', 'PIZZAORDER', 1), ('and', 'O', None), ('a', 'DRINKORDER', 2), ('small', 'DRINKORDER', 2), ('iced', 'DRINKORDER', 2), ('tea', 'DRINKORDER', 2)]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "input_text = \"i need a medium ham and pineapple pizza and a small iced tea\"\n",
    "\n",
    "# Convert labeled_output to a dictionary mapping tokens to a list of (label, order_number)\n",
    "label_dict = defaultdict(list)\n",
    "for tok, lbl, num in order_info:\n",
    "    label_dict[tok.lower()].append((lbl, num))\n",
    "\n",
    "input_tokens = input_text.split()\n",
    "labeled_input = []\n",
    "\n",
    "# Track used labels to avoid reusing the same label for the same word\n",
    "used_labels = defaultdict(int)\n",
    "\n",
    "for token in input_tokens:\n",
    "    token_lower = token.lower()\n",
    "    if token_lower in label_dict:\n",
    "        # Get the next unused label for this token\n",
    "        label_index = used_labels[token_lower]\n",
    "        if label_index < len(label_dict[token_lower]):\n",
    "            token_label, sequence_number = label_dict[token_lower][label_index]\n",
    "            used_labels[token_lower] += 1  # Mark this label as used\n",
    "        else:\n",
    "            token_label, sequence_number = 'O', None  # Default if all labels are used\n",
    "    else:\n",
    "        token_label, sequence_number = 'O', None  # Default for unmatched tokens\n",
    "\n",
    "    labeled_input.append((token, token_label, sequence_number))\n",
    "\n",
    "print(labeled_input)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
