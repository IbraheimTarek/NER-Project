{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records so far...\n",
      "Processed 20000 records so far...\n",
      "Processed 30000 records so far...\n",
      "Processed 40000 records so far...\n",
      "Processed 50000 records so far...\n",
      "Processed 60000 records so far...\n",
      "Processed 70000 records so far...\n",
      "Processed 80000 records so far...\n",
      "Processed 90000 records so far...\n",
      "Processed 100000 records so far...\n",
      "Processed 110000 records so far...\n",
      "Processed 120000 records so far...\n",
      "Processed 130000 records so far...\n",
      "Processed 140000 records so far...\n",
      "Processed 150000 records so far...\n",
      "Processed 160000 records so far...\n",
      "Processed 170000 records so far...\n",
      "Processed 180000 records so far...\n",
      "Processed 190000 records so far...\n",
      "Processed 200000 records so far...\n",
      "Processed 210000 records so far...\n",
      "Processed 220000 records so far...\n",
      "Processed 230000 records so far...\n",
      "Processed 240000 records so far...\n",
      "Processed 250000 records so far...\n",
      "Processed 260000 records so far...\n",
      "Processed 270000 records so far...\n",
      "Processed 280000 records so far...\n",
      "Processed 290000 records so far...\n",
      "Processed 300000 records so far...\n",
      "Processed 310000 records so far...\n",
      "Processed 320000 records so far...\n",
      "Processed 330000 records so far...\n",
      "Processed 340000 records so far...\n",
      "Processed 350000 records so far...\n",
      "Processed 360000 records so far...\n",
      "Processed 370000 records so far...\n",
      "Processed 380000 records so far...\n",
      "Processed 390000 records so far...\n",
      "Processed 400000 records so far...\n",
      "Processed 410000 records so far...\n",
      "Processed 420000 records so far...\n",
      "Processed 430000 records so far...\n",
      "Processed 440000 records so far...\n",
      "Processed 450000 records so far...\n",
      "Processed 460000 records so far...\n",
      "Processed 470000 records so far...\n",
      "Processed 480000 records so far...\n",
      "Processed 490000 records so far...\n",
      "Processed 500000 records so far...\n",
      "Processed 510000 records so far...\n",
      "Processed 520000 records so far...\n",
      "Processed 530000 records so far...\n",
      "Processed 540000 records so far...\n",
      "Processed 550000 records so far...\n",
      "Processed 560000 records so far...\n",
      "Processed 570000 records so far...\n",
      "Processed 580000 records so far...\n",
      "Processed 590000 records so far...\n",
      "Processed 600000 records so far...\n",
      "Processed 610000 records so far...\n",
      "Processed 620000 records so far...\n",
      "Processed 630000 records so far...\n",
      "Processed 640000 records so far...\n",
      "Processed 650000 records so far...\n",
      "Processed 660000 records so far...\n",
      "Processed 670000 records so far...\n",
      "Processed 680000 records so far...\n",
      "Processed 690000 records so far...\n",
      "Processed 700000 records so far...\n",
      "Processed 710000 records so far...\n",
      "Processed 720000 records so far...\n",
      "Processed 730000 records so far...\n",
      "Processed 740000 records so far...\n",
      "Processed 750000 records so far...\n",
      "Processed 760000 records so far...\n",
      "Processed 770000 records so far...\n",
      "Processed 780000 records so far...\n",
      "Processed 790000 records so far...\n",
      "Processed 800000 records so far...\n",
      "Processed 810000 records so far...\n",
      "Processed 820000 records so far...\n",
      "Processed 830000 records so far...\n",
      "Processed 840000 records so far...\n",
      "Processed 850000 records so far...\n",
      "Processed 860000 records so far...\n",
      "Processed 870000 records so far...\n",
      "Processed 880000 records so far...\n",
      "Processed 890000 records so far...\n",
      "Processed 900000 records so far...\n",
      "Processed 910000 records so far...\n",
      "Processed 920000 records so far...\n",
      "Processed 930000 records so far...\n",
      "Processed 940000 records so far...\n",
      "Processed 950000 records so far...\n",
      "Processed 960000 records so far...\n",
      "Processed 970000 records so far...\n",
      "Processed 980000 records so far...\n",
      "Processed 990000 records so far...\n",
      "Processed 1000000 records so far...\n",
      "Processed 1010000 records so far...\n",
      "Processed 1020000 records so far...\n",
      "Processed 1030000 records so far...\n",
      "Processed 1040000 records so far...\n",
      "Processed 1050000 records so far...\n",
      "Processed 1060000 records so far...\n",
      "Processed 1070000 records so far...\n",
      "Processed 1080000 records so far...\n",
      "Processed 1090000 records so far...\n",
      "Processed 1100000 records so far...\n",
      "Processed 1110000 records so far...\n",
      "Processed 1120000 records so far...\n",
      "Processed 1130000 records so far...\n",
      "Processed 1140000 records so far...\n",
      "Processed 1150000 records so far...\n",
      "Processed 1160000 records so far...\n",
      "Processed 1170000 records so far...\n",
      "Processed 1180000 records so far...\n",
      "Processed 1190000 records so far...\n",
      "Processed 1200000 records so far...\n",
      "Processed 1210000 records so far...\n",
      "Processed 1220000 records so far...\n",
      "Processed 1230000 records so far...\n",
      "Processed 1240000 records so far...\n",
      "Processed 1250000 records so far...\n",
      "Processed 1260000 records so far...\n",
      "Processed 1270000 records so far...\n",
      "Processed 1280000 records so far...\n",
      "Processed 1290000 records so far...\n",
      "Processed 1300000 records so far...\n",
      "Processed 1310000 records so far...\n",
      "Processed 1320000 records so far...\n",
      "Processed 1330000 records so far...\n",
      "Processed 1340000 records so far...\n",
      "Processed 1350000 records so far...\n",
      "Processed 1360000 records so far...\n",
      "Processed 1370000 records so far...\n",
      "Processed 1380000 records so far...\n",
      "Processed 1390000 records so far...\n",
      "Processed 1400000 records so far...\n",
      "Processed 1410000 records so far...\n",
      "Processed 1420000 records so far...\n",
      "Processed 1430000 records so far...\n",
      "Processed 1440000 records so far...\n",
      "Processed 1450000 records so far...\n",
      "Processed 1460000 records so far...\n",
      "Processed 1470000 records so far...\n",
      "Processed 1480000 records so far...\n",
      "Processed 1490000 records so far...\n",
      "Processed 1500000 records so far...\n",
      "Processed 1510000 records so far...\n",
      "Processed 1520000 records so far...\n",
      "Processed 1530000 records so far...\n",
      "Processed 1540000 records so far...\n",
      "Processed 1550000 records so far...\n",
      "Processed 1560000 records so far...\n",
      "Processed 1570000 records so far...\n",
      "Processed 1580000 records so far...\n",
      "Processed 1590000 records so far...\n",
      "Processed 1600000 records so far...\n",
      "Processed 1610000 records so far...\n",
      "Processed 1620000 records so far...\n",
      "Processed 1630000 records so far...\n",
      "Processed 1640000 records so far...\n",
      "Processed 1650000 records so far...\n",
      "Processed 1660000 records so far...\n",
      "Processed 1670000 records so far...\n",
      "Processed 1680000 records so far...\n",
      "Processed 1690000 records so far...\n",
      "Processed 1700000 records so far...\n",
      "Processed 1710000 records so far...\n",
      "Processed 1720000 records so far...\n",
      "Processed 1730000 records so far...\n",
      "Processed 1740000 records so far...\n",
      "Processed 1750000 records so far...\n",
      "Processed 1760000 records so far...\n",
      "Processed 1770000 records so far...\n",
      "Processed 1780000 records so far...\n",
      "Processed 1790000 records so far...\n",
      "Processed 1800000 records so far...\n",
      "Processed 1810000 records so far...\n",
      "Processed 1820000 records so far...\n",
      "Processed 1830000 records so far...\n",
      "Processed 1840000 records so far...\n",
      "Processed 1850000 records so far...\n",
      "Processed 1860000 records so far...\n",
      "Processed 1870000 records so far...\n",
      "Processed 1880000 records so far...\n",
      "Processed 1890000 records so far...\n",
      "Processed 1900000 records so far...\n",
      "Processed 1910000 records so far...\n",
      "Processed 1920000 records so far...\n",
      "Processed 1930000 records so far...\n",
      "Processed 1940000 records so far...\n",
      "Processed 1950000 records so far...\n",
      "Processed 1960000 records so far...\n",
      "Processed 1970000 records so far...\n",
      "Processed 1980000 records so far...\n",
      "Processed 1990000 records so far...\n",
      "Processed 2000000 records so far...\n",
      "Processed 2010000 records so far...\n",
      "Processed 2020000 records so far...\n",
      "Processed 2030000 records so far...\n",
      "Processed 2040000 records so far...\n",
      "Processed 2050000 records so far...\n",
      "Processed 2060000 records so far...\n",
      "Processed 2070000 records so far...\n",
      "Processed 2080000 records so far...\n",
      "Processed 2090000 records so far...\n",
      "Processed 2100000 records so far...\n",
      "Processed 2110000 records so far...\n",
      "Processed 2120000 records so far...\n",
      "Processed 2130000 records so far...\n",
      "Processed 2140000 records so far...\n",
      "Processed 2150000 records so far...\n",
      "Processed 2160000 records so far...\n",
      "Processed 2170000 records so far...\n",
      "Processed 2180000 records so far...\n",
      "Processed 2190000 records so far...\n",
      "Processed 2200000 records so far...\n",
      "Processed 2210000 records so far...\n",
      "Processed 2220000 records so far...\n",
      "Processed 2230000 records so far...\n",
      "Processed 2240000 records so far...\n",
      "Processed 2250000 records so far...\n",
      "Processed 2260000 records so far...\n",
      "Processed 2270000 records so far...\n",
      "Processed 2280000 records so far...\n",
      "Processed 2290000 records so far...\n",
      "Processed 2300000 records so far...\n",
      "Processed 2310000 records so far...\n",
      "Processed 2320000 records so far...\n",
      "Processed 2330000 records so far...\n",
      "Processed 2340000 records so far...\n",
      "Processed 2350000 records so far...\n",
      "Processed 2360000 records so far...\n",
      "Processed 2370000 records so far...\n",
      "Processed 2380000 records so far...\n",
      "Processed 2390000 records so far...\n",
      "Processed 2400000 records so far...\n",
      "Processed 2410000 records so far...\n",
      "Processed 2420000 records so far...\n",
      "Processed 2430000 records so far...\n",
      "Processed 2440000 records so far...\n",
      "Processed 2450000 records so far...\n"
     ]
    }
   ],
   "source": [
    "train_path = '../dataset/PIZZA_train.json'\n",
    "test_path = '../dataset/PIZZA_dev.json'\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                data.append(record)\n",
    "                \n",
    "                # Process in chunks of 10,000 records\n",
    "                if i > 0 and i % 10000 == 0:\n",
    "                    print(f\"Processed {i} records so far...\")\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Convert remaining data to DataFrame\n",
    "data = read_data(train_path)\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "data = read_data(test_path)\n",
    "if data:\n",
    "    dev = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hima\\AppData\\Roaming\\Python\\Python311\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  2064,  1045,  2031,  1037,  2312, 22861,  4160,  2766, 15960,\n",
      "           102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "{'input_ids': tensor([[  101,  1006,  2344,  2064,  1045,  2031,  1006, 10733,  8551,  2121,\n",
      "          1006,  2193,  1037,  1007,  1006,  2946,  2312,  1007,  1006, 22286,\n",
      "         22861,  4160,  2766, 15960,  1007,  1007,  1007,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Example: Using a pre-trained tokenizer like BERT\n",
    "src_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tgt_tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "src_text = \"can i have a large bbq pulled pork\"\n",
    "tgt_text = \"(ORDER can i have (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\"\n",
    "\n",
    "# Tokenize the input and output\n",
    "src_tokens = src_tokenizer(src_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "tgt_tokens = tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "print(src_tokens)\n",
    "print(tgt_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[\"train.SRC\"][idx]\n",
    "        tgt_text = self.data[\"train.TOP\"][idx]\n",
    "\n",
    "        src_tokens = self.src_tokenizer(src_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "        tgt_tokens = self.tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"src_input_ids\": src_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"src_attention_mask\": src_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"tgt_input_ids\": tgt_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"tgt_attention_mask\": tgt_tokens[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "\n",
    "\n",
    "train_dataset = PizzaDataset(df, src_tokenizer, tgt_tokenizer)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([  101,  2312, 11345,  2007,  2665, 11565,  1998,  2007,  4469, 27233,\n",
       "         26534,  3490,   102,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'src_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'tgt_input_ids': tensor([  101,  1006,  2344,  1006, 10733,  8551,  2121,  1006,  2946,  2312,\n",
       "          1007, 11345,  2007,  1006, 22286,  2665, 11565,  1007,  1998,  2007,\n",
       "          1006,  3375,  1035, 22286,  1006, 11712,  4469,  1007,  1006, 22286,\n",
       "         27233, 26534,  3490,  1007,  1007,  1007,  1007,   102,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'tgt_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevPizzaDataset(Dataset):\n",
    "    def __init__(self, data, src_tokenizer, tgt_tokenizer):\n",
    "        self.data = data\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[\"dev.SRC\"][idx]\n",
    "        tgt_text = self.data[\"dev.TOP\"][idx]\n",
    "\n",
    "        src_tokens = self.src_tokenizer(src_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "        tgt_tokens = self.tgt_tokenizer(tgt_text, return_tensors=\"pt\", padding=\"max_length\", max_length=50, truncation=True)\n",
    "\n",
    "        return {\n",
    "            \"src_input_ids\": src_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"src_attention_mask\": src_tokens[\"attention_mask\"].squeeze(0),\n",
    "            \"tgt_input_ids\": tgt_tokens[\"input_ids\"].squeeze(0),\n",
    "            \"tgt_attention_mask\": tgt_tokens[\"attention_mask\"].squeeze(0),\n",
    "        }\n",
    "val_dataset = DevPizzaDataset(dev, src_tokenizer, tgt_tokenizer)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'src_input_ids': tensor([  101,  1045,  1005,  1040,  2066,  2000,  2344,  1037,  2312, 20949,\n",
       "          1998, 11565, 10733,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'src_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0]),\n",
       " 'tgt_input_ids': tensor([  101,  1006,  2344,  1045,  1005,  1040,  2066,  2000,  2344,  1006,\n",
       "         10733,  8551,  2121,  1006,  2193,  1037,  1007,  1006,  2946,  2312,\n",
       "          1007,  1006, 22286, 20949,  1007,  1998,  1006, 22286, 11565,  1007,\n",
       "         10733,  1007,  1007,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]),\n",
       " 'tgt_attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset.__getitem__(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset.__getitem__(3)['src_input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM-Based Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, vocab_size, num_layers=1):\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.bidirectional = True\n",
    "        self.num_directions = 2 if self.bidirectional else 1\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers,\n",
    "                            bidirectional=self.bidirectional, batch_first=True)\n",
    "\n",
    "    def forward(self, src, src_lengths):\n",
    "        embedded = self.embedding(src)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # Initialize hidden and cell states\n",
    "        batch_size = src.size(0)\n",
    "        h0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim).to(src.device)\n",
    "        c0 = torch.zeros(self.num_layers * self.num_directions, batch_size, self.hidden_dim).to(src.device)\n",
    "\n",
    "        # Pack the sequences for the LSTM\n",
    "        src_lengths = src_lengths.cpu()  # Move lengths to CPU\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(embedded, src_lengths, batch_first=True, enforce_sorted=False)\n",
    "\n",
    "        # Pass through the LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(packed, (h0, c0))\n",
    "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs, batch_first=True)\n",
    "\n",
    "        # Return all hidden states (both forward and backward)\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "\n",
    "\n",
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, output_dim, vocab_size, num_layers=1):\n",
    "        super(LSTMDecoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        input = input.unsqueeze(1)  # (batch_size, 1)\n",
    "        embedded = self.embedding(input)  # (batch_size, 1, embedding_dim)\n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc_out(output.squeeze(1))  # (batch_size, vocab_size)\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, src_lengths, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        # Tensor to store decoder outputs\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "\n",
    "        # Encode source sequence\n",
    "        encoder_outputs, hidden, cell = self.encoder(src, src_lengths)\n",
    "\n",
    "        # Reshape hidden and cell states for the decoder\n",
    "        hidden = hidden.view(self.encoder.num_layers, batch_size, -1)\n",
    "        cell = cell.view(self.encoder.num_layers, batch_size, -1)\n",
    "\n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = tgt[:, 0]\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)  # Get the highest predicted token\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs\n",
    "\n",
    "def train_model(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(dataloader)\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training Progress\", unit=\"batch\", leave=True)\n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        src = batch[\"src_input_ids\"].to(device)\n",
    "        src_lengths = (batch[\"src_attention_mask\"] != 0).sum(dim=1).cpu()\n",
    "        tgt = batch[\"tgt_input_ids\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, src_lengths, tgt)\n",
    "        # Reshape for loss computation\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / (batch_idx + 1)\n",
    "        progress_bar.set_description(f\"Training Progress: Batch {batch_idx + 1}/{total_batches}, Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "def evaluate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            src = batch[\"src_input_ids\"].to(device)\n",
    "            src_lengths = (batch[\"src_attention_mask\"] != 0).sum(dim=1).to(device)\n",
    "            tgt = batch[\"tgt_input_ids\"].to(device)\n",
    "\n",
    "            output = model(src, src_lengths, tgt, teacher_forcing_ratio=0)  # No teacher forcing during evaluation\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            tgt = tgt[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: Batch 1768/38382, Avg Loss: 1.3373:   5%|â–         | 1768/38382 [27:46<9:35:16,  1.06batch/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters())\n\u001b[0;32m     13\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss(ignore_index\u001b[38;5;241m=\u001b[39mtgt_tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m---> 16\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_model(model, train_dataloader, optimizer, criterion, DEVICE)\n\u001b[0;32m     17\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m evaluate_model(model, val_dataloader, criterion, DEVICE)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[9], line 107\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(model, dataloader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m    105\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 107\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    108\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    109\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Progress: Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_idx\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_batches\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Avg Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mavg_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "INPUT_DIM = len(src_tokenizer.vocab)\n",
    "OUTPUT_DIM = len(tgt_tokenizer.vocab)\n",
    "EMBEDDING_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "encoder = BiLSTMEncoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, INPUT_DIM, NUM_LAYERS).to(DEVICE)\n",
    "decoder = LSTMDecoder(EMBEDDING_DIM, HIDDEN_DIM * 2, OUTPUT_DIM, OUTPUT_DIM, NUM_LAYERS).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=tgt_tokenizer.pad_token_id)\n",
    "\n",
    "\n",
    "train_loss = train_model(model, train_dataloader, optimizer, criterion, DEVICE)\n",
    "val_loss = evaluate_model(model, val_dataloader, criterion, DEVICE)\n",
    "print(f\"Train Loss: {train_loss:.3f}, Val Loss: {val_loss:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, src, src_lengths, max_len=50):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        encoder_outputs, hidden, cell = model.encoder(src, src_lengths)\n",
    "        input = torch.tensor([tgt_tokenizer.bos_token_id]).to(model.device)\n",
    "\n",
    "        outputs = []\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input, hidden, cell)\n",
    "            top1 = output.argmax(1).item()\n",
    "            outputs.append(top1)\n",
    "            if top1 == tgt_tokenizer.eos_token_id:\n",
    "                break\n",
    "            input = torch.tensor([top1]).to(model.device)\n",
    "\n",
    "        return tgt_tokenizer.decode(outputs, skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, \"../weights/transformer_Bilstm.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
