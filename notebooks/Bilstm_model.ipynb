{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, num_layers=3, dropout=0.5):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        \n",
    "        # Bidirectional LSTM layers\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=embedding_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            bidirectional=True,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, 512)\n",
    "        self.fc2 = nn.Linear(512, 512)\n",
    "        self.fc_out = nn.Linear(512, output_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Embedding layer\n",
    "        embedded = self.embedding(x)  # (batch_size, seq_len, embedding_dim)\n",
    "\n",
    "        # BiLSTM layers\n",
    "        lstm_out, _ = self.bilstm(embedded)  # (batch_size, seq_len, hidden_dim * 2)\n",
    "\n",
    "        # TimeDistributed fully connected layers\n",
    "        output = self.fc1(lstm_out)  # (batch_size, seq_len, 512)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = self.fc2(output)  # (batch_size, seq_len, 512)\n",
    "        output = F.relu(output)\n",
    "        output = self.dropout(output)\n",
    "\n",
    "        output = self.fc_out(output)  # (batch_size, seq_len, output_dim)\n",
    "        return F.log_softmax(output, dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records so far...\n",
      "Processed 20000 records so far...\n",
      "Processed 30000 records so far...\n",
      "Processed 40000 records so far...\n",
      "Processed 50000 records so far...\n",
      "Processed 60000 records so far...\n",
      "Processed 70000 records so far...\n",
      "Processed 80000 records so far...\n",
      "Processed 90000 records so far...\n",
      "Processed 100000 records so far...\n",
      "Processed 110000 records so far...\n",
      "Processed 120000 records so far...\n",
      "Processed 130000 records so far...\n",
      "Processed 140000 records so far...\n",
      "Processed 150000 records so far...\n",
      "Processed 160000 records so far...\n",
      "Processed 170000 records so far...\n",
      "Processed 180000 records so far...\n",
      "Processed 190000 records so far...\n",
      "Processed 200000 records so far...\n",
      "Processed 210000 records so far...\n",
      "Processed 220000 records so far...\n",
      "Processed 230000 records so far...\n",
      "Processed 240000 records so far...\n",
      "Processed 250000 records so far...\n",
      "Processed 260000 records so far...\n",
      "Processed 270000 records so far...\n",
      "Processed 280000 records so far...\n",
      "Processed 290000 records so far...\n",
      "Processed 300000 records so far...\n",
      "Processed 310000 records so far...\n",
      "Processed 320000 records so far...\n",
      "Processed 330000 records so far...\n",
      "Processed 340000 records so far...\n",
      "Processed 350000 records so far...\n",
      "Processed 360000 records so far...\n",
      "Processed 370000 records so far...\n",
      "Processed 380000 records so far...\n",
      "Processed 390000 records so far...\n",
      "Processed 400000 records so far...\n",
      "Processed 410000 records so far...\n",
      "Processed 420000 records so far...\n",
      "Processed 430000 records so far...\n",
      "Processed 440000 records so far...\n",
      "Processed 450000 records so far...\n",
      "Processed 460000 records so far...\n",
      "Processed 470000 records so far...\n",
      "Processed 480000 records so far...\n",
      "Processed 490000 records so far...\n",
      "Processed 500000 records so far...\n",
      "Processed 510000 records so far...\n",
      "Processed 520000 records so far...\n",
      "Processed 530000 records so far...\n",
      "Processed 540000 records so far...\n",
      "Processed 550000 records so far...\n",
      "Processed 560000 records so far...\n",
      "Processed 570000 records so far...\n",
      "Processed 580000 records so far...\n",
      "Processed 590000 records so far...\n",
      "Processed 600000 records so far...\n",
      "Processed 610000 records so far...\n",
      "Processed 620000 records so far...\n",
      "Processed 630000 records so far...\n",
      "Processed 640000 records so far...\n",
      "Processed 650000 records so far...\n",
      "Processed 660000 records so far...\n",
      "Processed 670000 records so far...\n",
      "Processed 680000 records so far...\n",
      "Processed 690000 records so far...\n",
      "Processed 700000 records so far...\n",
      "Processed 710000 records so far...\n",
      "Processed 720000 records so far...\n",
      "Processed 730000 records so far...\n",
      "Processed 740000 records so far...\n",
      "Processed 750000 records so far...\n",
      "Processed 760000 records so far...\n",
      "Processed 770000 records so far...\n",
      "Processed 780000 records so far...\n",
      "Processed 790000 records so far...\n",
      "Processed 800000 records so far...\n",
      "Processed 810000 records so far...\n",
      "Processed 820000 records so far...\n",
      "Processed 830000 records so far...\n",
      "Processed 840000 records so far...\n",
      "Processed 850000 records so far...\n",
      "Processed 860000 records so far...\n",
      "Processed 870000 records so far...\n",
      "Processed 880000 records so far...\n",
      "Processed 890000 records so far...\n",
      "Processed 900000 records so far...\n",
      "Processed 910000 records so far...\n",
      "Processed 920000 records so far...\n",
      "Processed 930000 records so far...\n",
      "Processed 940000 records so far...\n",
      "Processed 950000 records so far...\n",
      "Processed 960000 records so far...\n",
      "Processed 970000 records so far...\n",
      "Processed 980000 records so far...\n",
      "Processed 990000 records so far...\n",
      "Processed 1000000 records so far...\n",
      "Processed 1010000 records so far...\n",
      "Processed 1020000 records so far...\n",
      "Processed 1030000 records so far...\n",
      "Processed 1040000 records so far...\n",
      "Processed 1050000 records so far...\n",
      "Processed 1060000 records so far...\n",
      "Processed 1070000 records so far...\n",
      "Processed 1080000 records so far...\n",
      "Processed 1090000 records so far...\n",
      "Processed 1100000 records so far...\n",
      "Processed 1110000 records so far...\n",
      "Processed 1120000 records so far...\n",
      "Processed 1130000 records so far...\n",
      "Processed 1140000 records so far...\n",
      "Processed 1150000 records so far...\n",
      "Processed 1160000 records so far...\n",
      "Processed 1170000 records so far...\n",
      "Processed 1180000 records so far...\n",
      "Processed 1190000 records so far...\n",
      "Processed 1200000 records so far...\n",
      "Processed 1210000 records so far...\n",
      "Processed 1220000 records so far...\n",
      "Processed 1230000 records so far...\n",
      "Processed 1240000 records so far...\n",
      "Processed 1250000 records so far...\n",
      "Processed 1260000 records so far...\n",
      "Processed 1270000 records so far...\n",
      "Processed 1280000 records so far...\n",
      "Processed 1290000 records so far...\n",
      "Processed 1300000 records so far...\n",
      "Processed 1310000 records so far...\n",
      "Processed 1320000 records so far...\n",
      "Processed 1330000 records so far...\n",
      "Processed 1340000 records so far...\n",
      "Processed 1350000 records so far...\n",
      "Processed 1360000 records so far...\n",
      "Processed 1370000 records so far...\n",
      "Processed 1380000 records so far...\n",
      "Processed 1390000 records so far...\n",
      "Processed 1400000 records so far...\n",
      "Processed 1410000 records so far...\n",
      "Processed 1420000 records so far...\n",
      "Processed 1430000 records so far...\n",
      "Processed 1440000 records so far...\n",
      "Processed 1450000 records so far...\n",
      "Processed 1460000 records so far...\n",
      "Processed 1470000 records so far...\n",
      "Processed 1480000 records so far...\n",
      "Processed 1490000 records so far...\n",
      "Processed 1500000 records so far...\n",
      "Processed 1510000 records so far...\n",
      "Processed 1520000 records so far...\n",
      "Processed 1530000 records so far...\n",
      "Processed 1540000 records so far...\n",
      "Processed 1550000 records so far...\n",
      "Processed 1560000 records so far...\n",
      "Processed 1570000 records so far...\n",
      "Processed 1580000 records so far...\n",
      "Processed 1590000 records so far...\n",
      "Processed 1600000 records so far...\n",
      "Processed 1610000 records so far...\n",
      "Processed 1620000 records so far...\n",
      "Processed 1630000 records so far...\n",
      "Processed 1640000 records so far...\n",
      "Processed 1650000 records so far...\n",
      "Processed 1660000 records so far...\n",
      "Processed 1670000 records so far...\n",
      "Processed 1680000 records so far...\n",
      "Processed 1690000 records so far...\n",
      "Processed 1700000 records so far...\n",
      "Processed 1710000 records so far...\n",
      "Processed 1720000 records so far...\n",
      "Processed 1730000 records so far...\n",
      "Processed 1740000 records so far...\n",
      "Processed 1750000 records so far...\n",
      "Processed 1760000 records so far...\n",
      "Processed 1770000 records so far...\n",
      "Processed 1780000 records so far...\n",
      "Processed 1790000 records so far...\n",
      "Processed 1800000 records so far...\n",
      "Processed 1810000 records so far...\n",
      "Processed 1820000 records so far...\n",
      "Processed 1830000 records so far...\n",
      "Processed 1840000 records so far...\n",
      "Processed 1850000 records so far...\n",
      "Processed 1860000 records so far...\n",
      "Processed 1870000 records so far...\n",
      "Processed 1880000 records so far...\n",
      "Processed 1890000 records so far...\n",
      "Processed 1900000 records so far...\n",
      "Processed 1910000 records so far...\n",
      "Processed 1920000 records so far...\n",
      "Processed 1930000 records so far...\n",
      "Processed 1940000 records so far...\n",
      "Processed 1950000 records so far...\n",
      "Processed 1960000 records so far...\n",
      "Processed 1970000 records so far...\n",
      "Processed 1980000 records so far...\n",
      "Processed 1990000 records so far...\n",
      "Processed 2000000 records so far...\n",
      "Processed 2010000 records so far...\n",
      "Processed 2020000 records so far...\n",
      "Processed 2030000 records so far...\n",
      "Processed 2040000 records so far...\n",
      "Processed 2050000 records so far...\n",
      "Processed 2060000 records so far...\n",
      "Processed 2070000 records so far...\n",
      "Processed 2080000 records so far...\n",
      "Processed 2090000 records so far...\n",
      "Processed 2100000 records so far...\n",
      "Processed 2110000 records so far...\n",
      "Processed 2120000 records so far...\n",
      "Processed 2130000 records so far...\n",
      "Processed 2140000 records so far...\n",
      "Processed 2150000 records so far...\n",
      "Processed 2160000 records so far...\n",
      "Processed 2170000 records so far...\n",
      "Processed 2180000 records so far...\n",
      "Processed 2190000 records so far...\n",
      "Processed 2200000 records so far...\n",
      "Processed 2210000 records so far...\n",
      "Processed 2220000 records so far...\n",
      "Processed 2230000 records so far...\n",
      "Processed 2240000 records so far...\n",
      "Processed 2250000 records so far...\n",
      "Processed 2260000 records so far...\n",
      "Processed 2270000 records so far...\n",
      "Processed 2280000 records so far...\n",
      "Processed 2290000 records so far...\n",
      "Processed 2300000 records so far...\n",
      "Processed 2310000 records so far...\n",
      "Processed 2320000 records so far...\n",
      "Processed 2330000 records so far...\n",
      "Processed 2340000 records so far...\n",
      "Processed 2350000 records so far...\n",
      "Processed 2360000 records so far...\n",
      "Processed 2370000 records so far...\n",
      "Processed 2380000 records so far...\n",
      "Processed 2390000 records so far...\n",
      "Processed 2400000 records so far...\n",
      "Processed 2410000 records so far...\n",
      "Processed 2420000 records so far...\n",
      "Processed 2430000 records so far...\n",
      "Processed 2440000 records so far...\n",
      "Processed 2450000 records so far...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "from utils.data_preprocessing import preprocess_text\n",
    "from utils.feature_extraction import bag_of_words, tfidf_features, extract_embeddings\n",
    "\n",
    "train_path = '../dataset/PIZZA_train.json'\n",
    "test_path = '../dataset/PIZZA_dev.json'\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                data.append(record)\n",
    "                \n",
    "                # Process in chunks of 10,000 records\n",
    "                if i > 0 and i % 10000 == 0:\n",
    "                    print(f\"Processed {i} records so far...\")\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Convert remaining data to DataFrame\n",
    "data = read_data(train_path)\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "data = read_data(test_path)\n",
    "if data:\n",
    "    dev = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2456446\n",
      "i'd like a pizza with hot pepper pecorino cheese and parmesan without thin crust\n",
      "(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING HOT_PEPPERS ) (TOPPING PECORINO_CHEESE ) (TOPPING PARMESAN_CHEESE ) (NOT (STYLE THIN_CRUST ) ) ) )\n",
      "i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage\n"
     ]
    }
   ],
   "source": [
    "X_train = df['train.SRC']\n",
    "y_train = df['train.EXR']\n",
    "X_test = dev['dev.SRC']\n",
    "y_test = dev['dev.EXR']\n",
    "print(len(df))\n",
    "print(X_train[2456445])\n",
    "print(y_train[2456445])\n",
    "print(dev['dev.SRC'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d pizza hot pepper pecoricheese parmesan thin crust\n"
     ]
    }
   ],
   "source": [
    "X_train = [\" \".join(preprocess_text(text)) for text in X_train]\n",
    "X_test = [\" \".join(preprocess_text(text)) for text in X_test]\n",
    "print(X_train[2456445])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize_output(output):\n",
    "    \"\"\"\n",
    "    Tokenizes the structured output into meaningful tokens.\n",
    "    Example:\n",
    "        Input: \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\"\n",
    "        Output: [\"(ORDER\", \"(PIZZAORDER\", \"(NUMBER\", \"a\", \"(SIZE\", \"large\", \"(TOPPING\", \"bbq\", \"pulled\", \"pork\", \")\", \")\", \")\", \")\"]\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"\\(|\\)|\\w+|[^\\s()]+\", output)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(outputs):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from tokenized outputs.\n",
    "    \"\"\"\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}  # Special tokens\n",
    "    i = 2\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = i\n",
    "                i += 1\n",
    "    return vocab\n",
    "def encode_outputs(outputs, vocab):\n",
    "    \"\"\"\n",
    "    Encodes tokenized outputs into sequences of integers.\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)\n",
    "        sequence = [vocab[\"<SOS>\"]] + [vocab[token] for token in tokens if token in vocab] + [vocab[\"<EOS>\"]]\n",
    "        encoded.append(sequence)\n",
    "    return encoded\n",
    "\n",
    "def pad_sequences_to_fixed_length(sequences, max_len):\n",
    "    \"\"\"\n",
    "    Pads sequences to a fixed length.\n",
    "    \"\"\"\n",
    "    return pad_sequences(sequences, maxlen=max_len, padding=\"post\", value=0)\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of integers back into the structured output string.\n",
    "    \"\"\"\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}  # Reverse the vocabulary\n",
    "    tokens = [inv_vocab[idx] for idx in sequence if idx > 0]  # Ignore <PAD> tokens\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x2a71961c090>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "glove_vectors = api.load(\"glove-wiki-gigaword-100\")  # 100-dimension GloVe\n",
    "glove_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def prepare_data(\n",
    "    X_train, y_train, X_test, y_test, feature_type=\"bow\", glove_vectors=None, max_len=20\n",
    "):\n",
    "\n",
    "    vectorizer = None\n",
    "\n",
    "    # Feature Extraction for X_train and X_test\n",
    "    if feature_type == \"bow\":\n",
    "        X_train_processed, vectorizer = bag_of_words(X_train)\n",
    "        X_train_processed = X_train_processed.toarray()\n",
    "        X_test_processed = vectorizer.transform(X_test).toarray()\n",
    "    elif feature_type == \"tfidf\":\n",
    "        X_train_processed, vectorizer = tfidf_features(X_train)\n",
    "        X_train_processed = X_train_processed.toarray()\n",
    "        X_test_processed = vectorizer.transform(X_test).toarray()\n",
    "    elif feature_type == \"embeddings\":\n",
    "        if not glove_vectors:\n",
    "            raise ValueError(\"GloVe vectors must be provided for embeddings.\")\n",
    "        X_train_tokenized = [sentence.split() for sentence in X_train]\n",
    "        X_test_tokenized = [sentence.split() for sentence in X_test]\n",
    "        X_train_processed = extract_embeddings(X_train_tokenized)\n",
    "        X_test_processed = extract_embeddings(X_test_tokenized)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature type. Choose 'bow', 'tfidf', or 'embeddings'.\")\n",
    "\n",
    "    vocab = build_vocab(y_train)  # Build vocabulary from training outputs\n",
    "    y_train_encoded = encode_outputs(y_train, vocab)  # Encode training outputs\n",
    "    y_test_encoded = encode_outputs(y_test, vocab)  # Encode testing outputs\n",
    "    y_train_processed = pad_sequences_to_fixed_length(y_train_encoded, max_len)\n",
    "    y_test_processed = pad_sequences_to_fixed_length(y_test_encoded, max_len)\n",
    "\n",
    "\n",
    "    return (\n",
    "        X_train_processed,\n",
    "        X_test_processed,\n",
    "        y_train_processed,\n",
    "        y_test_processed,\n",
    "        vectorizer,\n",
    "        vocab,  # Return vocabulary for decoding\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "GloVe vectors must be provided for embeddings.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[70], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X_train_processed, X_test_processed, y_train_processed, y_test_processed, vectorizer, vocab \u001b[38;5;241m=\u001b[39m prepare_data( X_train, y_train, X_test, y_test, feature_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_len\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m)\n",
      "Cell \u001b[1;32mIn[69], line 18\u001b[0m, in \u001b[0;36mprepare_data\u001b[1;34m(X_train, y_train, X_test, y_test, feature_type, glove_vectors, max_len)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m feature_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membeddings\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m glove_vectors:\n\u001b[1;32m---> 18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGloVe vectors must be provided for embeddings.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m     X_train_tokenized \u001b[38;5;241m=\u001b[39m [sentence\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m X_train]\n\u001b[0;32m     20\u001b[0m     X_test_tokenized \u001b[38;5;241m=\u001b[39m [sentence\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m X_test]\n",
      "\u001b[1;31mValueError\u001b[0m: GloVe vectors must be provided for embeddings."
     ]
    }
   ],
   "source": [
    "X_train_processed, X_test_processed, y_train_processed, y_test_processed, vectorizer, vocab = prepare_data( X_train, y_train, X_test, y_test, feature_type=\"embeddings\", max_len=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2456446, 797), (2456446, 300))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape, y_train_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "183"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_processed.shape[1]  # Vocabulary size\n",
    "embedding_dim = 25  # Dimension of embedding vectors\n",
    "hidden_dim = 256  # Hidden state size for LSTM\n",
    "output_dim = y_train_processed.shape[1]  # Number of output classes\n",
    "num_layers = 3  # Number of BiLSTM layers\n",
    "dropout = 0.5  # Dropout probability\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BiLSTMModel(input_dim, embedding_dim, hidden_dim, output_dim, num_layers, dropout).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7831149848 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y_train_processed, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# Same for targets\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     y_train_processed \u001b[38;5;241m=\u001b[39m y_train_processed\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[1;32m---> 24\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m SequenceDataset(X_train_processed, y_train_processed)\n\u001b[0;32m     25\u001b[0m test_dataset \u001b[38;5;241m=\u001b[39m SequenceDataset(X_test_processed, y_test_processed)\n\u001b[0;32m     27\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m  \u001b[38;5;66;03m# Adjust based GPU ;-;  memory\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 6\u001b[0m, in \u001b[0;36mSequenceDataset.__init__\u001b[1;34m(self, inputs, targets)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, targets):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(inputs, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32) \n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtargets \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(targets, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:114] data. DefaultCPUAllocator: not enough memory: you tried to allocate 7831149848 bytes."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = torch.tensor(inputs, dtype=torch.float32) \n",
    "        self.targets = torch.tensor(targets, dtype=torch.long) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            \"src_input_ids\": self.inputs[idx],\n",
    "            \"tgt_input_ids\": self.targets[idx]\n",
    "        }\n",
    "\n",
    "# if hasattr(X_train_processed, \"toarray\"):  # Check if it's a sparse matrix\n",
    "#     X_train_processed = X_train_processed.toarray()\n",
    "\n",
    "# if hasattr(y_train_processed, \"toarray\"):  # Same for targets\n",
    "#     y_train_processed = y_train_processed.toarray()\n",
    "    \n",
    "train_dataset = SequenceDataset(X_train_processed, y_train_processed)\n",
    "test_dataset = SequenceDataset(X_test_processed, y_test_processed)\n",
    "\n",
    "batch_size = 128  # Adjust based GPU ;-;  memory\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()  # Use for multi-class classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(10):  # Number of epochs\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(train_dataloader)\n",
    "    progress_bar = tqdm(train_dataloader, desc=\"Training Progress\", unit=\"batch\", leave=True)\n",
    "    for batch in train_dataloader:  # Assuming a DataLoader is used for batches\n",
    "        src = batch[\"src_input_ids\"].to(device)  # Input tokens\n",
    "        tgt = batch[\"tgt_input_ids\"].to(device)  # Target tokens\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src)  # Forward pass\n",
    "\n",
    "        # Reshape outputs and targets for loss computation\n",
    "        output = output.view(-1, output_dim)\n",
    "        tgt = tgt.view(-1)\n",
    "\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / len(train_dataloader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    sample_input = torch.tensor([[1, 2, 3, 4, 5]], dtype=torch.long).to(device)  # Example input\n",
    "    output = model(sample_input)\n",
    "    predicted_classes = torch.argmax(output, dim=-1)  # Get class with highest probability\n",
    "    print(predicted_classes)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
