{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    # Extract tokens: parentheses or sequences of non-whitespace, non-parenthesis characters.\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    # Parse tokens into a nested list structure\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "\n",
    "def normalize_structure(tree):\n",
    "    if not isinstance(tree, list):\n",
    "        return None\n",
    "\n",
    "    def is_key(token):\n",
    "        return token in [\n",
    "            \"ORDER\", \"PIZZAORDER\", \"DRINKORDER\", \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\",\n",
    "            \"COMPLEX_TOPPING\", \"QUANTITY\", \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"NOT\"\n",
    "        ]\n",
    "\n",
    "    # Clean the list by keeping sublists and tokens as-is for further analysis\n",
    "    cleaned = []\n",
    "    for el in tree:\n",
    "        cleaned.append(el)\n",
    "\n",
    "    if len(cleaned) > 0 and isinstance(cleaned[0], str) and is_key(cleaned[0]):\n",
    "        key = cleaned[0]\n",
    "        if key == \"ORDER\":\n",
    "            pizzaorders = []\n",
    "            drinkorders = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    if \"PIZZAORDER\" in node:\n",
    "                        if isinstance(node[\"PIZZAORDER\"], list):\n",
    "                            pizzaorders.extend(node[\"PIZZAORDER\"])\n",
    "                        else:\n",
    "                            pizzaorders.append(node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in node:\n",
    "                        if isinstance(node[\"DRINKORDER\"], list):\n",
    "                            drinkorders.extend(node[\"DRINKORDER\"])\n",
    "                        else:\n",
    "                            drinkorders.append(node[\"DRINKORDER\"])\n",
    "                    if node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                        pizzaorders.append(node)\n",
    "                    if node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                        drinkorders.append(node)\n",
    "            result = {}\n",
    "            if pizzaorders:\n",
    "                result[\"PIZZAORDER\"] = pizzaorders\n",
    "            if drinkorders:\n",
    "                result[\"DRINKORDER\"] = drinkorders\n",
    "            if result:\n",
    "                return {\"ORDER\": result}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "        elif key == \"PIZZAORDER\":\n",
    "            number = None\n",
    "            size = None\n",
    "            style = None\n",
    "            toppings = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"SIZE\":\n",
    "                        size = node[\"VALUE\"]\n",
    "                    elif t == \"STYLE\":\n",
    "                        style = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        toppings.append(node)\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if size is not None:\n",
    "                result[\"SIZE\"] = size\n",
    "            if style is not None:\n",
    "                result[\"STYLE\"] = style\n",
    "            if toppings:\n",
    "                result[\"AllTopping\"] = toppings\n",
    "            # Mark type internally, will remove later\n",
    "            result[\"TYPE\"] = \"PIZZAORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key == \"DRINKORDER\":\n",
    "            number = None\n",
    "            volume = None\n",
    "            drinktype = None\n",
    "            containertype = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"VOLUME\":\n",
    "                        volume = node[\"VALUE\"]\n",
    "                    elif t == \"DRINKTYPE\":\n",
    "                        drinktype = node[\"VALUE\"]\n",
    "                    elif t == \"CONTAINERTYPE\":\n",
    "                        containertype = node[\"VALUE\"]\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if volume is not None:\n",
    "                result[\"VOLUME\"] = volume\n",
    "            if drinktype is not None:\n",
    "                result[\"DRINKTYPE\"] = drinktype\n",
    "            if containertype is not None:\n",
    "                result[\"CONTAINERTYPE\"] = containertype\n",
    "            result[\"TYPE\"] = \"DRINKORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key in [\"NUMBER\",\"SIZE\",\"STYLE\",\"VOLUME\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"QUANTITY\"]:\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            value_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": key,\n",
    "                \"VALUE\": value_str\n",
    "            }\n",
    "\n",
    "        elif key == \"TOPPING\":\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            topping_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": None,\n",
    "                \"Topping\": topping_str\n",
    "            }\n",
    "\n",
    "        elif key == \"COMPLEX_TOPPING\":\n",
    "            quantity = None\n",
    "            topping = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"QUANTITY\":\n",
    "                        quantity = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        topping = node[\"Topping\"]\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": quantity,\n",
    "                \"Topping\": topping\n",
    "            }\n",
    "\n",
    "        elif key == \"NOT\":\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict) and node.get(\"TYPE\") == \"TOPPING\":\n",
    "                    node[\"NOT\"] = True\n",
    "                    if \"Quantity\" not in node:\n",
    "                        node[\"Quantity\"] = None\n",
    "                    return node\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Try to parse sublists and combine orders found\n",
    "        combined_order = {\"PIZZAORDER\": [], \"DRINKORDER\": []}\n",
    "        found_order = False\n",
    "\n",
    "        for el in cleaned:\n",
    "            node = normalize_structure(el)\n",
    "            if isinstance(node, dict):\n",
    "                if \"ORDER\" in node:\n",
    "                    found_order = True\n",
    "                    order_node = node[\"ORDER\"]\n",
    "                    if \"PIZZAORDER\" in order_node:\n",
    "                        combined_order[\"PIZZAORDER\"].extend(order_node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in order_node:\n",
    "                        combined_order[\"DRINKORDER\"].extend(order_node[\"DRINKORDER\"])\n",
    "                elif node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"PIZZAORDER\"].append(node)\n",
    "                elif node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"DRINKORDER\"].append(node)\n",
    "\n",
    "        if found_order:\n",
    "            final = {}\n",
    "            if combined_order[\"PIZZAORDER\"]:\n",
    "                final[\"PIZZAORDER\"] = combined_order[\"PIZZAORDER\"]\n",
    "            if combined_order[\"DRINKORDER\"]:\n",
    "                final[\"DRINKORDER\"] = combined_order[\"DRINKORDER\"]\n",
    "            return {\"ORDER\": final} if final else {}\n",
    "\n",
    "        return None\n",
    "\n",
    "def remove_type_keys(obj):\n",
    "    # Recursively remove \"TYPE\" keys from all dictionaries\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(\"TYPE\", None)\n",
    "        for k, v in obj.items():\n",
    "            remove_type_keys(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            remove_type_keys(item)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenize(text)\n",
    "    parsed = parse_tokens(tokens)\n",
    "    result = normalize_structure(parsed)\n",
    "    remove_type_keys(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, bidirectional=True, batch_first=True)\n",
    "        self.fc = nn.Linear(hid_dim*2, hid_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, src):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "        # outputs: [batch, src_len, hid_dim*2]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        \n",
    "        # hidden and cell are from both directions\n",
    "        # We can combine them:\n",
    "        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n",
    "        \n",
    "        # cell: just take last layer's cell states (similar to hidden)\n",
    "        # If you want, you can do similar combination for cell:\n",
    "        cell = torch.zeros_like(hidden)  # for simplicity\n",
    "        \n",
    "        return outputs, (hidden.unsqueeze(0), cell.unsqueeze(0))\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(emb_dim, hid_dim, batch_first=True)\n",
    "        self.out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, input_step, hidden, cell):\n",
    "        # input_step: [batch]\n",
    "        # hidden, cell: [1, batch, hid_dim]\n",
    "        input_step = input_step.unsqueeze(1) # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input_step)) # [batch, 1, emb_dim]\n",
    "        outputs, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        # outputs: [batch, 1, hid_dim]\n",
    "        pred = self.out(outputs.squeeze(1)) # [batch, output_dim]\n",
    "        return pred, hidden, cell\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, tgt_vocab):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.tgt_vocab = tgt_vocab\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # tgt: [batch, tgt_len]\n",
    "        batch_size = src.size(0)\n",
    "        tgt_len = tgt.size(1)\n",
    "        outputs = torch.zeros(batch_size, tgt_len, len(self.tgt_vocab)).to(src.device)\n",
    "\n",
    "        enc_outputs, (hidden, cell) = self.encoder(src)\n",
    "\n",
    "        input_step = tgt[:,0] # <SOS>\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            pred, hidden, cell = self.decoder(input_step, hidden, cell)\n",
    "            outputs[:, t, :] = pred\n",
    "            # Teacher forcing\n",
    "            teacher_force = True if torch.rand(1).item() < teacher_forcing_ratio else False\n",
    "            top1 = pred.argmax(1)\n",
    "            input_step = tgt[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_input(text):\n",
    "    return text.strip().split()\n",
    "\n",
    "def build_vocab(dataset, min_freq=1):\n",
    "    # Collect frequencies\n",
    "    freq = {}\n",
    "    for src_tokens, tgt_tokens in dataset:\n",
    "        for t in src_tokens:\n",
    "            freq[t] = freq.get(t, 0) + 1\n",
    "        for t in tgt_tokens:\n",
    "            freq[t] = freq.get(t, 0) + 1\n",
    "    \n",
    "    # Create vocab\n",
    "    vocab = {\"<PAD>\":0, \"<SOS>\":1, \"<EOS>\":2, \"<UNK>\":3}\n",
    "    idx = len(vocab)\n",
    "    for token, count in freq.items():\n",
    "        if count >= min_freq and token not in vocab:\n",
    "            vocab[token] = idx\n",
    "            idx += 1\n",
    "    return vocab\n",
    "\n",
    "def numericalize(tokens, vocab):\n",
    "    return [vocab.get(t, vocab[\"<UNK>\"]) for t in tokens]\n",
    "\n",
    "def pad_batch(sequences, pad_idx):\n",
    "    # Pad a list of sequences to the max length in the batch\n",
    "    max_len = max(len(seq) for seq in sequences)\n",
    "    padded = []\n",
    "    for seq in sequences:\n",
    "        padded.append(seq + [pad_idx]*(max_len - len(seq)))\n",
    "    return padded\n",
    "\n",
    "def collate_fn(batch, src_vocab, tgt_vocab):\n",
    "    # batch: list of tuples (src_tokens, tgt_tokens)\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    # Numericalize\n",
    "    src_seqs_num = [numericalize(s, src_vocab) for s in src_seqs]\n",
    "    tgt_seqs_num = [[tgt_vocab[\"<SOS>\"]]+numericalize(t, tgt_vocab)+[tgt_vocab[\"<EOS>\"]] for t in tgt_seqs]\n",
    "\n",
    "    src_padded = pad_batch(src_seqs_num, src_vocab[\"<PAD>\"])\n",
    "    tgt_padded = pad_batch(tgt_seqs_num, tgt_vocab[\"<PAD>\"])\n",
    "\n",
    "    src_tensor = torch.tensor(src_padded, dtype=torch.long)\n",
    "    tgt_tensor = torch.tensor(tgt_padded, dtype=torch.long)\n",
    "    return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    total_batches = len(dataloader)\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    \n",
    "    for batch_idx, batch in enumerate(progress_bar):\n",
    "        src, tgt = batch\n",
    "        src = src.to(model.device)\n",
    "        tgt = tgt.to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)  # output: [batch, tgt_len, output_dim]\n",
    "        \n",
    "        # Reshape for loss calculation\n",
    "        output_dim = output.shape[-1]\n",
    "        # Exclude the <SOS> token at the start\n",
    "        output = output[:, 1:].reshape(-1, output_dim)  \n",
    "        tgt = tgt[:, 1:].reshape(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        avg_loss = epoch_loss / (batch_idx + 1)\n",
    "        progress_bar.set_description(f\"Training Progress: Batch {batch_idx + 1}/{total_batches}, Avg Loss: {avg_loss:.8f}\")\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def greedy_decode(model, src_tokens, src_vocab, tgt_vocab, max_len=50):\n",
    "    model.eval()\n",
    "    src_ids = [src_vocab.get(t, src_vocab[\"<UNK>\"]) for t in src_tokens]\n",
    "    src_tensor = torch.tensor(src_ids, dtype=torch.long).unsqueeze(0) # [1, src_len]\n",
    "    src_tensor = src_tensor.to(model.device)  # Move input to the same device as model\n",
    "\n",
    "    with torch.no_grad():\n",
    "        enc_outputs, (hidden, cell) = model.encoder(src_tensor)\n",
    "\n",
    "    input_step = torch.tensor([tgt_vocab[\"<SOS>\"]], dtype=torch.long).to(model.device)\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        with torch.no_grad():\n",
    "            pred, hidden, cell = model.decoder(input_step, hidden, cell)\n",
    "        top1 = pred.argmax(1)\n",
    "        token_id = top1.item()\n",
    "        if token_id == tgt_vocab[\"<EOS>\"]:\n",
    "            break\n",
    "        # Convert token_id back to token\n",
    "        decoded_tokens.append([k for k,v in tgt_vocab.items() if v==token_id][0])\n",
    "        input_step = top1\n",
    "\n",
    "    return \" \".join(decoded_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, data, src_field, tgt_field):\n",
    "\n",
    "        self.data = data\n",
    "        self.src_field = src_field\n",
    "        self.tgt_field = tgt_field\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src_text = self.data[idx][self.src_field]\n",
    "        tgt_text = self.data[idx][self.tgt_field]\n",
    "        src_tokens = tokenize(src_text)\n",
    "        tgt_tokens = tokenize(tgt_text)\n",
    "        return src_tokens, tgt_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_json_path = \"../dataset/PIZZA_train.json\"  \n",
    "dev_jsonl_path = \"../dataset/PIZZA_dev.json\" \n",
    "train_data = pd.read_json(train_json_path, lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = train_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = PizzaDataset(data_list, \"train.SRC\", \"train.TOP-DECOUPLED\")\n",
    "dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "all_pairs = [dataset[i] for i in range(len(dataset))]\n",
    "src_vocab = build_vocab(all_pairs, min_freq=1)\n",
    "tgt_vocab = src_vocab  #  use one vocab; we may separate if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=True, collate_fn=lambda b: collate_fn(b, src_vocab, tgt_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "# Define model parameters\n",
    "INPUT_DIM = len(src_vocab)\n",
    "OUTPUT_DIM = len(tgt_vocab)\n",
    "EMB_DIM = 128\n",
    "HID_DIM = 128\n",
    "PAD_IDX = src_vocab[\"<PAD>\"]\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "encoder = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, PAD_IDX)\n",
    "decoder = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, PAD_IDX)\n",
    "model = Seq2Seq(encoder, decoder, tgt_vocab)\n",
    "model = model.to(device)\n",
    "model.device = device\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_json_path = \"../dataset/PIZZA_dev.json\" \n",
    "dev_data = pd.read_json(dev_json_path, lines=True)\n",
    "dev_data = dev_data.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fix maxlen\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_parentheses(s):\n",
    "    # Tokenize parentheses and other tokens\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "\n",
    "    stack = []\n",
    "    cleaned = []\n",
    "    open_count = 0\n",
    "    \n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            open_count += 1\n",
    "            cleaned.append(token)\n",
    "        elif token == ')':\n",
    "            # Only add a closing parenthesis if there is a corresponding open\n",
    "            if open_count > 0:\n",
    "                cleaned.append(token)\n",
    "                open_count -= 1\n",
    "            # If no open, skip this token\n",
    "        else:\n",
    "            # It's a word/token, just add it\n",
    "            cleaned.append(token)\n",
    "\n",
    "    return \" \".join(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_json(pred, gold):\n",
    "    def compare_nested_dicts(pred_dict, gold_dict, match_count, total_count):\n",
    "        \"\"\"\n",
    "        Recursively compare two dictionaries and count matches and total attributes.\n",
    "        \"\"\"\n",
    "        # Ensure both inputs are dictionaries\n",
    "        if not isinstance(pred_dict, dict) or not isinstance(gold_dict, dict):\n",
    "            return match_count, total_count\n",
    "\n",
    "        for key in gold_dict:\n",
    "            if key in pred_dict:\n",
    "                if isinstance(gold_dict[key], dict) and isinstance(pred_dict[key], dict):\n",
    "                    # Recursively compare nested dictionaries\n",
    "                    match_count, total_count = compare_nested_dicts(\n",
    "                        pred_dict[key], gold_dict[key], match_count, total_count\n",
    "                    )\n",
    "                elif isinstance(gold_dict[key], list) and isinstance(pred_dict[key], list):\n",
    "                    # Compare lists element-wise\n",
    "                    match_count, total_count = compare_nested_lists(\n",
    "                        pred_dict[key], gold_dict[key], match_count, total_count\n",
    "                    )\n",
    "                else:\n",
    "                    # Compare values directly\n",
    "                    total_count += 1\n",
    "                    if gold_dict[key] == pred_dict[key]:\n",
    "                        match_count += 1\n",
    "            else:\n",
    "                # Key missing in prediction\n",
    "                total_count += 1\n",
    "\n",
    "        return match_count, total_count\n",
    "\n",
    "    def compare_nested_lists(pred_list, gold_list, match_count, total_count):\n",
    "        \"\"\"\n",
    "        Compare two lists element-wise or by subset (assuming gold is ground truth).\n",
    "        \"\"\"\n",
    "        if not isinstance(pred_list, list) or not isinstance(gold_list, list):\n",
    "            return match_count, total_count\n",
    "\n",
    "        for gold_item in gold_list:\n",
    "            matched = False\n",
    "            for pred_item in pred_list:\n",
    "                # Compare each item in the lists (assuming dictionaries)\n",
    "                if isinstance(gold_item, dict) and isinstance(pred_item, dict):\n",
    "                    sub_match_count, sub_total_count = compare_nested_dicts(\n",
    "                        pred_item, gold_item, 0, 0\n",
    "                    )\n",
    "                    if sub_match_count == sub_total_count:\n",
    "                        matched = True\n",
    "                        break\n",
    "                elif gold_item == pred_item:\n",
    "                    matched = True\n",
    "                    break\n",
    "            total_count += 1\n",
    "            if matched:\n",
    "                match_count += 1\n",
    "\n",
    "        return match_count, total_count\n",
    "\n",
    "    # Initialize match and total counts\n",
    "    match_count = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # Start comparison\n",
    "    match_count, total_count = compare_nested_dicts(pred, gold, match_count, total_count)\n",
    "\n",
    "    # Return percentage of correct matches\n",
    "    accuracy = (match_count / total_count) * 100 if total_count > 0 else 0\n",
    "    return accuracy, match_count, total_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sequences = len(dev_data)\n",
    "correct_sequences = 0\n",
    "pred_list = []\n",
    "tgt_list = []\n",
    "accuracy= match_count = total_count = 0\n",
    "for sample in dev_data:\n",
    "    pred = greedy_decode(model, tokenize_input(sample[\"dev.SRC\"]), src_vocab, tgt_vocab)\n",
    "    print(\"SRC:\", sample[\"dev.SRC\"])\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", sample[\"dev.TOP\"])\n",
    "    pred = preprocess(clean_parentheses(pred))\n",
    "    tgt = preprocess(sample[\"dev.TOP\"])\n",
    "    print(\"PRED:\", pred)\n",
    "    print(\"GOLD:\", tgt)\n",
    "    accuracy_1, match_count_1, total_count_1 = compare_json(pred, tgt)\n",
    "    accuracy += accuracy_1\n",
    "    match_count += match_count_1\n",
    "    total_count += total_count_1\n",
    "    if pred == tgt:\n",
    "        # print(pred)\n",
    "        # print(tgt)\n",
    "        correct_sequences += 1\n",
    "    # else:\n",
    "    #     print(pred)\n",
    "    #     print(tgt)\n",
    "print(f\"Correct {correct_sequences}, Total {total_sequences}\")\n",
    "sequence_accuracy = correct_sequences / total_sequences if total_sequences > 0 else 0\n",
    "sequence_accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy/total_count * 100, match_count, total_count"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
