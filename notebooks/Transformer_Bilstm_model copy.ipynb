{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def tokenize(s):\n",
    "    # Extract tokens: parentheses or sequences of non-whitespace, non-parenthesis characters.\n",
    "    tokens = re.findall(r'\\(|\\)|[^\\s()]+', s)\n",
    "    return tokens\n",
    "\n",
    "def parse_tokens(tokens):\n",
    "    # Parse tokens into a nested list structure\n",
    "    stack = []\n",
    "    current_list = []\n",
    "    for token in tokens:\n",
    "        if token == '(':\n",
    "            stack.append(current_list)\n",
    "            current_list = []\n",
    "        elif token == ')':\n",
    "            finished = current_list\n",
    "            current_list = stack.pop()\n",
    "            current_list.append(finished)\n",
    "        else:\n",
    "            current_list.append(token)\n",
    "    return current_list\n",
    "\n",
    "def normalize_structure(tree):\n",
    "    if not isinstance(tree, list):\n",
    "        return None\n",
    "\n",
    "    def is_key(token):\n",
    "        return token in [\n",
    "            \"ORDER\", \"PIZZAORDER\", \"DRINKORDER\", \"NUMBER\", \"SIZE\", \"STYLE\", \"TOPPING\",\n",
    "            \"COMPLEX_TOPPING\", \"QUANTITY\", \"VOLUME\", \"DRINKTYPE\", \"CONTAINERTYPE\", \"NOT\"\n",
    "        ]\n",
    "\n",
    "    # Clean the list by keeping sublists and tokens as-is for further analysis\n",
    "    cleaned = []\n",
    "    for el in tree:\n",
    "        cleaned.append(el)\n",
    "\n",
    "    if len(cleaned) > 0 and isinstance(cleaned[0], str) and is_key(cleaned[0]):\n",
    "        key = cleaned[0]\n",
    "        if key == \"ORDER\":\n",
    "            pizzaorders = []\n",
    "            drinkorders = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    if \"PIZZAORDER\" in node:\n",
    "                        if isinstance(node[\"PIZZAORDER\"], list):\n",
    "                            pizzaorders.extend(node[\"PIZZAORDER\"])\n",
    "                        else:\n",
    "                            pizzaorders.append(node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in node:\n",
    "                        if isinstance(node[\"DRINKORDER\"], list):\n",
    "                            drinkorders.extend(node[\"DRINKORDER\"])\n",
    "                        else:\n",
    "                            drinkorders.append(node[\"DRINKORDER\"])\n",
    "                    if node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                        pizzaorders.append(node)\n",
    "                    if node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                        drinkorders.append(node)\n",
    "            result = {}\n",
    "            if pizzaorders:\n",
    "                result[\"PIZZAORDER\"] = pizzaorders\n",
    "            if drinkorders:\n",
    "                result[\"DRINKORDER\"] = drinkorders\n",
    "            if result:\n",
    "                return {\"ORDER\": result}\n",
    "            else:\n",
    "                return {}\n",
    "\n",
    "        elif key == \"PIZZAORDER\":\n",
    "            number = None\n",
    "            size = None\n",
    "            style = None\n",
    "            toppings = []\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"SIZE\":\n",
    "                        size = node[\"VALUE\"]\n",
    "                    elif t == \"STYLE\":\n",
    "                        style = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        toppings.append(node)\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if size is not None:\n",
    "                result[\"SIZE\"] = size\n",
    "            if style is not None:\n",
    "                result[\"STYLE\"] = style\n",
    "            if toppings:\n",
    "                result[\"AllTopping\"] = toppings\n",
    "            # Mark type internally, will remove later\n",
    "            result[\"TYPE\"] = \"PIZZAORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key == \"DRINKORDER\":\n",
    "            number = None\n",
    "            volume = None\n",
    "            drinktype = None\n",
    "            containertype = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"NUMBER\":\n",
    "                        number = node[\"VALUE\"]\n",
    "                    elif t == \"VOLUME\":\n",
    "                        volume = node[\"VALUE\"]\n",
    "                    elif t == \"DRINKTYPE\":\n",
    "                        drinktype = node[\"VALUE\"]\n",
    "                    elif t == \"CONTAINERTYPE\":\n",
    "                        containertype = node[\"VALUE\"]\n",
    "            result = {}\n",
    "            if number is not None:\n",
    "                result[\"NUMBER\"] = number\n",
    "            if volume is not None:\n",
    "                result[\"VOLUME\"] = volume\n",
    "            if drinktype is not None:\n",
    "                result[\"DRINKTYPE\"] = drinktype\n",
    "            if containertype is not None:\n",
    "                result[\"CONTAINERTYPE\"] = containertype\n",
    "            result[\"TYPE\"] = \"DRINKORDER\"\n",
    "            return result\n",
    "\n",
    "        elif key in [\"NUMBER\",\"SIZE\",\"STYLE\",\"VOLUME\",\"DRINKTYPE\",\"CONTAINERTYPE\",\"QUANTITY\"]:\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            value_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": key,\n",
    "                \"VALUE\": value_str\n",
    "            }\n",
    "\n",
    "        elif key == \"TOPPING\":\n",
    "            values = []\n",
    "            for el in cleaned[1:]:\n",
    "                if isinstance(el, str):\n",
    "                    values.append(el)\n",
    "            topping_str = \" \".join(values).strip()\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": None,\n",
    "                \"Topping\": topping_str\n",
    "            }\n",
    "\n",
    "        elif key == \"COMPLEX_TOPPING\":\n",
    "            quantity = None\n",
    "            topping = None\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict):\n",
    "                    t = node.get(\"TYPE\")\n",
    "                    if t == \"QUANTITY\":\n",
    "                        quantity = node[\"VALUE\"]\n",
    "                    elif t == \"TOPPING\":\n",
    "                        topping = node[\"Topping\"]\n",
    "            return {\n",
    "                \"TYPE\": \"TOPPING\",\n",
    "                \"NOT\": False,\n",
    "                \"Quantity\": quantity,\n",
    "                \"Topping\": topping\n",
    "            }\n",
    "\n",
    "        elif key == \"NOT\":\n",
    "            for sub in cleaned[1:]:\n",
    "                node = normalize_structure(sub)\n",
    "                if isinstance(node, dict) and node.get(\"TYPE\") == \"TOPPING\":\n",
    "                    node[\"NOT\"] = True\n",
    "                    if \"Quantity\" not in node:\n",
    "                        node[\"Quantity\"] = None\n",
    "                    return node\n",
    "            return None\n",
    "\n",
    "    else:\n",
    "        # Try to parse sublists and combine orders found\n",
    "        combined_order = {\"PIZZAORDER\": [], \"DRINKORDER\": []}\n",
    "        found_order = False\n",
    "\n",
    "        for el in cleaned:\n",
    "            node = normalize_structure(el)\n",
    "            if isinstance(node, dict):\n",
    "                if \"ORDER\" in node:\n",
    "                    found_order = True\n",
    "                    order_node = node[\"ORDER\"]\n",
    "                    if \"PIZZAORDER\" in order_node:\n",
    "                        combined_order[\"PIZZAORDER\"].extend(order_node[\"PIZZAORDER\"])\n",
    "                    if \"DRINKORDER\" in order_node:\n",
    "                        combined_order[\"DRINKORDER\"].extend(order_node[\"DRINKORDER\"])\n",
    "                elif node.get(\"TYPE\") == \"PIZZAORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"PIZZAORDER\"].append(node)\n",
    "                elif node.get(\"TYPE\") == \"DRINKORDER\":\n",
    "                    found_order = True\n",
    "                    combined_order[\"DRINKORDER\"].append(node)\n",
    "\n",
    "        if found_order:\n",
    "            final = {}\n",
    "            if combined_order[\"PIZZAORDER\"]:\n",
    "                final[\"PIZZAORDER\"] = combined_order[\"PIZZAORDER\"]\n",
    "            if combined_order[\"DRINKORDER\"]:\n",
    "                final[\"DRINKORDER\"] = combined_order[\"DRINKORDER\"]\n",
    "            return {\"ORDER\": final} if final else {}\n",
    "\n",
    "        return None\n",
    "\n",
    "def remove_type_keys(obj):\n",
    "    # Recursively remove \"TYPE\" keys from all dictionaries\n",
    "    if isinstance(obj, dict):\n",
    "        obj.pop(\"TYPE\", None)\n",
    "        for k, v in obj.items():\n",
    "            remove_type_keys(v)\n",
    "    elif isinstance(obj, list):\n",
    "        for item in obj:\n",
    "            remove_type_keys(item)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    tokens = tokenize(text)\n",
    "    parsed = parse_tokens(tokens)\n",
    "    result = normalize_structure(parsed)\n",
    "    remove_type_keys(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=(dropout if n_layers>1 else 0), batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        # src: [batch, src_len]\n",
    "        embedded = self.dropout(self.embedding(src))  # [batch, src_len, emb_dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded) \n",
    "        return hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers=1, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hid_dim, num_layers=n_layers, dropout=(dropout if n_layers>1 else 0), batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input: [batch] single token\n",
    "        input = input.unsqueeze(1)  # [batch, 1]\n",
    "        embedded = self.dropout(self.embedding(input)) # [batch, 1, emb_dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell)) # output: [batch, 1, hid_dim]\n",
    "        prediction = self.fc_out(output.squeeze(1)) # [batch, output_dim]\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "    \n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        # src: [batch, src_len]\n",
    "        # tgt: [batch, tgt_len]\n",
    "        batch_size = src.shape[0]\n",
    "        tgt_len = tgt.shape[1]\n",
    "        tgt_vocab_size = self.decoder.fc_out.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, tgt_len, tgt_vocab_size).to(self.device)\n",
    "        \n",
    "        hidden, cell = self.encoder(src)\n",
    "        input = tgt[:,0]  # first token <sos>\n",
    "        \n",
    "        for t in range(1, tgt_len):\n",
    "            # Teacher forcing\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[:, t, :] = output\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = tgt[:, t] if teacher_force else top1\n",
    "        \n",
    "        return outputs\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts, min_freq=2, special_tokens=[\"<pad>\", \"<sos>\", \"<eos>\", \"<unk>\"]):\n",
    "    word_freq = {}\n",
    "    for text in texts:\n",
    "        for tok in text.split():\n",
    "            word_freq[tok] = word_freq.get(tok, 0) + 1\n",
    "    # Build vocab\n",
    "    vocab = special_tokens[:]\n",
    "    for w, freq in sorted(word_freq.items(), key=lambda x: x[1], reverse=True):\n",
    "        if freq >= min_freq and w not in special_tokens:\n",
    "            vocab.append(w)\n",
    "    word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "    idx2word = {i: w for w, i in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "def numericalize(text, word2idx, max_len, sos_token=\"<sos>\", eos_token=\"<eos>\", pad_token=\"<pad>\"):\n",
    "    tokens = text.split()\n",
    "    tokens = [sos_token] + tokens + [eos_token]\n",
    "    # truncate or pad\n",
    "    if len(tokens) > max_len:\n",
    "        tokens = tokens[:max_len]\n",
    "        tokens[-1] = eos_token  # ensure ends with EOS\n",
    "    else:\n",
    "        tokens = tokens + [pad_token] * (max_len - len(tokens))\n",
    "    indices = [word2idx.get(t, word2idx[\"<unk>\"]) for t in tokens]\n",
    "    return indices\n",
    "\n",
    "def collate_fn(batch, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len):\n",
    "    srcs, tgts = zip(*batch)\n",
    "    src_indices = [numericalize(s, src_word2idx, max_src_len) for s in srcs]\n",
    "    tgt_indices = [numericalize(t, tgt_word2idx, max_tgt_len) for t in tgts]\n",
    "    \n",
    "    src_tensor = torch.tensor(src_indices, dtype=torch.long)\n",
    "    tgt_tensor = torch.tensor(tgt_indices, dtype=torch.long)\n",
    "    \n",
    "    return src_tensor, tgt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, optimizer, criterion, clip=1):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "    for src, tgt in progress_bar:\n",
    "        src = src.to(model.device)\n",
    "        tgt = tgt.to(model.device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt)\n",
    "        \n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:,1:].contiguous().view(-1, output_dim)\n",
    "        tgt = tgt[:,1:].contiguous().view(-1)\n",
    "        \n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        progress_bar.set_postfix({\"loss\": f\"{loss.item():.3f}\"})\n",
    "        \n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def evaluate_loss(model, dataloader, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=\"Evaluating\", leave=False)\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in progress_bar:\n",
    "            src = src.to(model.device)\n",
    "            tgt = tgt.to(model.device)\n",
    "            output = model(src, tgt, teacher_forcing_ratio=0)\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:,1:].contiguous().view(-1, output_dim)\n",
    "            tgt = tgt[:,1:].contiguous().view(-1)\n",
    "            \n",
    "            loss = criterion(output, tgt)\n",
    "            epoch_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"val_loss\": f\"{loss.item():.3f}\"})\n",
    "    return epoch_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def generate_output(model, src_tokens, tgt_word2idx, idx2tgt):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden, cell = model.encoder(src_tokens)\n",
    "        sos_idx = tgt_word2idx[\"<sos>\"]\n",
    "        eos_idx = tgt_word2idx[\"<eos>\"]\n",
    "        input_token = torch.tensor([sos_idx], device=model.device)\n",
    "        decoded_tokens = []\n",
    "        max_tgt_len = src_tokens.shape[1]*2  # heuristic limit\n",
    "\n",
    "        for _ in range(max_tgt_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "            top1 = output.argmax(1)\n",
    "            if top1.item() == eos_idx:\n",
    "                break\n",
    "            decoded_tokens.append(top1.item())\n",
    "            input_token = top1\n",
    "        \n",
    "        decoded_text = \" \".join([idx2tgt[i] for i in decoded_tokens])\n",
    "        return decoded_text\n",
    "\n",
    "def exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, idx2tgt, max_src_len, batch_size=32):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False)\n",
    "    for srcs, tgts in dev_loader:\n",
    "        # Convert srcs to tensors\n",
    "        src_indices = [numericalize(s, src_word2idx, max_src_len) for s in srcs]\n",
    "        src_tensor = torch.tensor(src_indices, dtype=torch.long, device=model.device)\n",
    "\n",
    "        # Decode each sample in the batch\n",
    "        for i in range(len(srcs)):\n",
    "            single_src = src_tensor[i].unsqueeze(0)  # [1, max_src_len]\n",
    "            predicted_text = generate_output(model, single_src, tgt_word2idx, idx2tgt)\n",
    "            # Convert both predicted and reference TOP into JSON using preprocess\n",
    "            pred_json = preprocess(predicted_text)\n",
    "            ref_json = preprocess(tgts[i])\n",
    "            if pred_json == ref_json:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "    \n",
    "    return correct / total if total > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PizzaDataset(Dataset):\n",
    "    def __init__(self, jsonl_path,SRC_NAME, TOP_NAME, max_samples=None, max_src_len=128, max_tgt_len=256):\n",
    "        self.src_texts = []\n",
    "        self.tgt_texts = []\n",
    "        self.max_src_len = max_src_len\n",
    "        self.max_tgt_len = max_tgt_len\n",
    "        \n",
    "        with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "            for i, line in enumerate(f):\n",
    "                if max_samples is not None and i >= max_samples:\n",
    "                    break\n",
    "                entry = json.loads(line.strip())\n",
    "                src = entry.get(SRC_NAME, \"\").strip()\n",
    "                tgt = entry.get(TOP_NAME, \"\").strip()\n",
    "                if src and tgt:\n",
    "                    self.src_texts.append(src)\n",
    "                    self.tgt_texts.append(tgt)\n",
    "                    \n",
    "        # Shuffle is not applied here for dev datasets.\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.src_texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.src_texts[idx], self.tgt_texts[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_jsonl_path = \"../dataset/PIZZA_train.json\"  \n",
    "dev_jsonl_path = \"../dataset/PIZZA_dev.json\"      \n",
    "\n",
    "train_dataset = PizzaDataset(train_jsonl_path,\"train.SRC\", \"train.TOP-DECOUPLED\", max_samples=50000) \n",
    "dev_dataset = PizzaDataset(dev_jsonl_path,\"dev.SRC\", \"dev.TOP\", max_samples=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('large pie with green pepper and with extra peperonni',\n",
       " '(ORDER (PIZZAORDER (SIZE large ) (TOPPING green pepper ) (COMPLEX_TOPPING (QUANTITY extra ) (TOPPING peperonni ) ) ) )')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vocabularies from train data\n",
    "src_word2idx, src_idx2word = build_vocab(train_dataset.src_texts, min_freq=2)\n",
    "tgt_word2idx, tgt_idx2word = build_vocab(train_dataset.tgt_texts, min_freq=2)\n",
    "\n",
    "pad_idx = tgt_word2idx[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0,\n",
       "  '<sos>': 1,\n",
       "  '<eos>': 2,\n",
       "  '<unk>': 3,\n",
       "  'with': 4,\n",
       "  'and': 5,\n",
       "  'pie': 6,\n",
       "  'a': 7,\n",
       "  'cheese': 8,\n",
       "  'i': 9,\n",
       "  'pizza': 10,\n",
       "  'one': 11,\n",
       "  'party': 12,\n",
       "  'american': 13,\n",
       "  'sized': 14,\n",
       "  'size': 15,\n",
       "  'extra': 16,\n",
       "  '-': 17,\n",
       "  'pepper': 18,\n",
       "  'can': 19,\n",
       "  'have': 20,\n",
       "  'peperonni': 21,\n",
       "  'banana': 22,\n",
       "  \"i'd\": 23,\n",
       "  'like': 24,\n",
       "  'peppers': 25,\n",
       "  'personal': 26,\n",
       "  'without': 27,\n",
       "  'green': 28,\n",
       "  'large': 29,\n",
       "  'want': 30,\n",
       "  'of': 31,\n",
       "  'need': 32,\n",
       "  'mozzarella': 33,\n",
       "  'lunch': 34,\n",
       "  'sauce': 35,\n",
       "  'lot': 36,\n",
       "  'pecorino': 37,\n",
       "  'any': 38,\n",
       "  'yellow': 39,\n",
       "  'no': 40,\n",
       "  'stuffed': 41,\n",
       "  'high': 42,\n",
       "  'rise': 43,\n",
       "  'dough': 44,\n",
       "  'crust': 45,\n",
       "  'meatball': 46,\n",
       "  'crusts': 47,\n",
       "  'medium': 48,\n",
       "  'little': 49,\n",
       "  'olive': 50,\n",
       "  'chicken': 51,\n",
       "  'regular': 52,\n",
       "  'bit': 53,\n",
       "  'peppperoni': 54,\n",
       "  'pepperoni': 55,\n",
       "  'big': 56,\n",
       "  'glaze': 57,\n",
       "  'olives': 58,\n",
       "  'roasted': 59,\n",
       "  'small': 60,\n",
       "  'meat': 61,\n",
       "  'caramelized': 62,\n",
       "  'meatlover': 63,\n",
       "  'onions': 64,\n",
       "  'red': 65,\n",
       "  'vegan': 66,\n",
       "  'onion': 67,\n",
       "  'dried': 68,\n",
       "  'peperronni': 69,\n",
       "  'buffalo': 70,\n",
       "  'thick': 71,\n",
       "  'balsamic': 72,\n",
       "  'bacon': 73,\n",
       "  'bbq': 74,\n",
       "  'new': 75,\n",
       "  'barbecue': 76,\n",
       "  'tomato': 77,\n",
       "  'yorker': 78,\n",
       "  'arugula': 79,\n",
       "  'kalamata': 80,\n",
       "  'balzamic': 81,\n",
       "  'garlic': 82,\n",
       "  'tomatoes': 83,\n",
       "  'pestos': 84,\n",
       "  'feta': 85,\n",
       "  'low': 86,\n",
       "  'fat': 87,\n",
       "  'ranch': 88,\n",
       "  'pulled': 89,\n",
       "  'pork': 90,\n",
       "  'beef': 91,\n",
       "  'cherry': 92,\n",
       "  'grilled': 93,\n",
       "  'just': 94,\n",
       "  'peperoni': 95,\n",
       "  'fried': 96,\n",
       "  'pesto': 97,\n",
       "  'hot': 98,\n",
       "  'ham': 99,\n",
       "  'all': 100,\n",
       "  'hams': 101,\n",
       "  'peperroni': 102,\n",
       "  'artichoke': 103,\n",
       "  'mushrooms': 104,\n",
       "  'bay': 105,\n",
       "  'leaves': 106,\n",
       "  'combination': 107,\n",
       "  'sausage': 108,\n",
       "  'broccoli': 109,\n",
       "  'artichokes': 110,\n",
       "  'carrots': 111,\n",
       "  'peppperonis': 112,\n",
       "  'alfredo': 113,\n",
       "  'pickles': 114,\n",
       "  'the': 115,\n",
       "  'everything': 116,\n",
       "  'pineapple': 117,\n",
       "  'parmesan': 118,\n",
       "  'not': 119,\n",
       "  'applewood': 120,\n",
       "  'apple': 121,\n",
       "  'wood': 122,\n",
       "  'chickens': 123,\n",
       "  'black': 124,\n",
       "  'bean': 125,\n",
       "  'cumin': 126,\n",
       "  'powder': 127,\n",
       "  'cheddar': 128,\n",
       "  'basil': 129,\n",
       "  'ground': 130,\n",
       "  'pepperonis': 131,\n",
       "  'bacons': 132,\n",
       "  'chorizo': 133,\n",
       "  'anchovies': 134,\n",
       "  'anchovy': 135,\n",
       "  'cheeseburger': 136,\n",
       "  'mozarella': 137,\n",
       "  'beans': 138,\n",
       "  'brocoli': 139,\n",
       "  'chorrizo': 140,\n",
       "  'gluten': 141,\n",
       "  'free': 142,\n",
       "  'carrot': 143,\n",
       "  'much': 144,\n",
       "  'sourdough': 145,\n",
       "  'deepdish': 146,\n",
       "  'vegetarian': 147,\n",
       "  'pea': 148,\n",
       "  'peperonis': 149,\n",
       "  'keto': 150,\n",
       "  'napolitana': 151,\n",
       "  'thin': 152,\n",
       "  'oregano': 153,\n",
       "  'peas': 154,\n",
       "  'mushroom': 155,\n",
       "  'lovers': 156,\n",
       "  'every': 157,\n",
       "  'style': 158,\n",
       "  'pickle': 159,\n",
       "  'gluten-free': 160,\n",
       "  'lover': 161,\n",
       "  'cauliflower': 162,\n",
       "  'oil': 163,\n",
       "  'spiced': 164,\n",
       "  'margarita': 165,\n",
       "  'neapolitan': 166,\n",
       "  'chicago': 167,\n",
       "  'parsley': 168,\n",
       "  'tiny': 169,\n",
       "  'spicy': 170,\n",
       "  'lots': 171,\n",
       "  'tuna': 172,\n",
       "  'salami': 173,\n",
       "  'white': 174,\n",
       "  'spinach': 175,\n",
       "  'jalapeno': 176,\n",
       "  'veggie': 177,\n",
       "  'shrimps': 178,\n",
       "  'topping': 179,\n",
       "  'vegetables': 180,\n",
       "  'works': 181,\n",
       "  'york': 182,\n",
       "  'margherita': 183,\n",
       "  'shrimp': 184,\n",
       "  'toppings': 185,\n",
       "  'meatlovers': 186,\n",
       "  'mexican': 187,\n",
       "  'hawaiian': 188,\n",
       "  'deep': 189,\n",
       "  'dish': 190,\n",
       "  'veggies': 191,\n",
       "  'ricotta': 192,\n",
       "  'italian': 193,\n",
       "  'only': 194,\n",
       "  'supreme': 195,\n",
       "  'mediterranean': 196,\n",
       "  'pineapples': 197,\n",
       "  'many': 198,\n",
       "  'rosemary': 199,\n",
       "  'pineaples': 200,\n",
       "  'jalapenos': 201,\n",
       "  'pineaple': 202,\n",
       "  'flake': 203,\n",
       "  'lettuce': 204,\n",
       "  'flakes': 205},\n",
       " {0: '<pad>',\n",
       "  1: '<sos>',\n",
       "  2: '<eos>',\n",
       "  3: '<unk>',\n",
       "  4: 'with',\n",
       "  5: 'and',\n",
       "  6: 'pie',\n",
       "  7: 'a',\n",
       "  8: 'cheese',\n",
       "  9: 'i',\n",
       "  10: 'pizza',\n",
       "  11: 'one',\n",
       "  12: 'party',\n",
       "  13: 'american',\n",
       "  14: 'sized',\n",
       "  15: 'size',\n",
       "  16: 'extra',\n",
       "  17: '-',\n",
       "  18: 'pepper',\n",
       "  19: 'can',\n",
       "  20: 'have',\n",
       "  21: 'peperonni',\n",
       "  22: 'banana',\n",
       "  23: \"i'd\",\n",
       "  24: 'like',\n",
       "  25: 'peppers',\n",
       "  26: 'personal',\n",
       "  27: 'without',\n",
       "  28: 'green',\n",
       "  29: 'large',\n",
       "  30: 'want',\n",
       "  31: 'of',\n",
       "  32: 'need',\n",
       "  33: 'mozzarella',\n",
       "  34: 'lunch',\n",
       "  35: 'sauce',\n",
       "  36: 'lot',\n",
       "  37: 'pecorino',\n",
       "  38: 'any',\n",
       "  39: 'yellow',\n",
       "  40: 'no',\n",
       "  41: 'stuffed',\n",
       "  42: 'high',\n",
       "  43: 'rise',\n",
       "  44: 'dough',\n",
       "  45: 'crust',\n",
       "  46: 'meatball',\n",
       "  47: 'crusts',\n",
       "  48: 'medium',\n",
       "  49: 'little',\n",
       "  50: 'olive',\n",
       "  51: 'chicken',\n",
       "  52: 'regular',\n",
       "  53: 'bit',\n",
       "  54: 'peppperoni',\n",
       "  55: 'pepperoni',\n",
       "  56: 'big',\n",
       "  57: 'glaze',\n",
       "  58: 'olives',\n",
       "  59: 'roasted',\n",
       "  60: 'small',\n",
       "  61: 'meat',\n",
       "  62: 'caramelized',\n",
       "  63: 'meatlover',\n",
       "  64: 'onions',\n",
       "  65: 'red',\n",
       "  66: 'vegan',\n",
       "  67: 'onion',\n",
       "  68: 'dried',\n",
       "  69: 'peperronni',\n",
       "  70: 'buffalo',\n",
       "  71: 'thick',\n",
       "  72: 'balsamic',\n",
       "  73: 'bacon',\n",
       "  74: 'bbq',\n",
       "  75: 'new',\n",
       "  76: 'barbecue',\n",
       "  77: 'tomato',\n",
       "  78: 'yorker',\n",
       "  79: 'arugula',\n",
       "  80: 'kalamata',\n",
       "  81: 'balzamic',\n",
       "  82: 'garlic',\n",
       "  83: 'tomatoes',\n",
       "  84: 'pestos',\n",
       "  85: 'feta',\n",
       "  86: 'low',\n",
       "  87: 'fat',\n",
       "  88: 'ranch',\n",
       "  89: 'pulled',\n",
       "  90: 'pork',\n",
       "  91: 'beef',\n",
       "  92: 'cherry',\n",
       "  93: 'grilled',\n",
       "  94: 'just',\n",
       "  95: 'peperoni',\n",
       "  96: 'fried',\n",
       "  97: 'pesto',\n",
       "  98: 'hot',\n",
       "  99: 'ham',\n",
       "  100: 'all',\n",
       "  101: 'hams',\n",
       "  102: 'peperroni',\n",
       "  103: 'artichoke',\n",
       "  104: 'mushrooms',\n",
       "  105: 'bay',\n",
       "  106: 'leaves',\n",
       "  107: 'combination',\n",
       "  108: 'sausage',\n",
       "  109: 'broccoli',\n",
       "  110: 'artichokes',\n",
       "  111: 'carrots',\n",
       "  112: 'peppperonis',\n",
       "  113: 'alfredo',\n",
       "  114: 'pickles',\n",
       "  115: 'the',\n",
       "  116: 'everything',\n",
       "  117: 'pineapple',\n",
       "  118: 'parmesan',\n",
       "  119: 'not',\n",
       "  120: 'applewood',\n",
       "  121: 'apple',\n",
       "  122: 'wood',\n",
       "  123: 'chickens',\n",
       "  124: 'black',\n",
       "  125: 'bean',\n",
       "  126: 'cumin',\n",
       "  127: 'powder',\n",
       "  128: 'cheddar',\n",
       "  129: 'basil',\n",
       "  130: 'ground',\n",
       "  131: 'pepperonis',\n",
       "  132: 'bacons',\n",
       "  133: 'chorizo',\n",
       "  134: 'anchovies',\n",
       "  135: 'anchovy',\n",
       "  136: 'cheeseburger',\n",
       "  137: 'mozarella',\n",
       "  138: 'beans',\n",
       "  139: 'brocoli',\n",
       "  140: 'chorrizo',\n",
       "  141: 'gluten',\n",
       "  142: 'free',\n",
       "  143: 'carrot',\n",
       "  144: 'much',\n",
       "  145: 'sourdough',\n",
       "  146: 'deepdish',\n",
       "  147: 'vegetarian',\n",
       "  148: 'pea',\n",
       "  149: 'peperonis',\n",
       "  150: 'keto',\n",
       "  151: 'napolitana',\n",
       "  152: 'thin',\n",
       "  153: 'oregano',\n",
       "  154: 'peas',\n",
       "  155: 'mushroom',\n",
       "  156: 'lovers',\n",
       "  157: 'every',\n",
       "  158: 'style',\n",
       "  159: 'pickle',\n",
       "  160: 'gluten-free',\n",
       "  161: 'lover',\n",
       "  162: 'cauliflower',\n",
       "  163: 'oil',\n",
       "  164: 'spiced',\n",
       "  165: 'margarita',\n",
       "  166: 'neapolitan',\n",
       "  167: 'chicago',\n",
       "  168: 'parsley',\n",
       "  169: 'tiny',\n",
       "  170: 'spicy',\n",
       "  171: 'lots',\n",
       "  172: 'tuna',\n",
       "  173: 'salami',\n",
       "  174: 'white',\n",
       "  175: 'spinach',\n",
       "  176: 'jalapeno',\n",
       "  177: 'veggie',\n",
       "  178: 'shrimps',\n",
       "  179: 'topping',\n",
       "  180: 'vegetables',\n",
       "  181: 'works',\n",
       "  182: 'york',\n",
       "  183: 'margherita',\n",
       "  184: 'shrimp',\n",
       "  185: 'toppings',\n",
       "  186: 'meatlovers',\n",
       "  187: 'mexican',\n",
       "  188: 'hawaiian',\n",
       "  189: 'deep',\n",
       "  190: 'dish',\n",
       "  191: 'veggies',\n",
       "  192: 'ricotta',\n",
       "  193: 'italian',\n",
       "  194: 'only',\n",
       "  195: 'supreme',\n",
       "  196: 'mediterranean',\n",
       "  197: 'pineapples',\n",
       "  198: 'many',\n",
       "  199: 'rosemary',\n",
       "  200: 'pineaples',\n",
       "  201: 'jalapenos',\n",
       "  202: 'pineaple',\n",
       "  203: 'flake',\n",
       "  204: 'lettuce',\n",
       "  205: 'flakes'})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_word2idx, src_idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'<pad>': 0,\n",
       "  '<sos>': 1,\n",
       "  '<eos>': 2,\n",
       "  '<unk>': 3,\n",
       "  ')': 4,\n",
       "  '(TOPPING': 5,\n",
       "  '(ORDER': 6,\n",
       "  '(PIZZAORDER': 7,\n",
       "  '(SIZE': 8,\n",
       "  '(NUMBER': 9,\n",
       "  'a': 10,\n",
       "  'cheese': 11,\n",
       "  'one': 12,\n",
       "  'party': 13,\n",
       "  '(COMPLEX_TOPPING': 14,\n",
       "  '(QUANTITY': 15,\n",
       "  'american': 16,\n",
       "  'sized': 17,\n",
       "  '(STYLE': 18,\n",
       "  'size': 19,\n",
       "  'extra': 20,\n",
       "  '-': 21,\n",
       "  '(NOT': 22,\n",
       "  'pepper': 23,\n",
       "  'peperonni': 24,\n",
       "  'banana': 25,\n",
       "  'peppers': 26,\n",
       "  'personal': 27,\n",
       "  'green': 28,\n",
       "  'large': 29,\n",
       "  'of': 30,\n",
       "  'mozzarella': 31,\n",
       "  'lunch': 32,\n",
       "  'sauce': 33,\n",
       "  'lot': 34,\n",
       "  'pecorino': 35,\n",
       "  'yellow': 36,\n",
       "  'stuffed': 37,\n",
       "  'high': 38,\n",
       "  'rise': 39,\n",
       "  'dough': 40,\n",
       "  'crust': 41,\n",
       "  'meatball': 42,\n",
       "  'crusts': 43,\n",
       "  'medium': 44,\n",
       "  'little': 45,\n",
       "  'olive': 46,\n",
       "  'chicken': 47,\n",
       "  'regular': 48,\n",
       "  'bit': 49,\n",
       "  'peppperoni': 50,\n",
       "  'pepperoni': 51,\n",
       "  'big': 52,\n",
       "  'glaze': 53,\n",
       "  'olives': 54,\n",
       "  'roasted': 55,\n",
       "  'small': 56,\n",
       "  'meat': 57,\n",
       "  'caramelized': 58,\n",
       "  'meatlover': 59,\n",
       "  'onions': 60,\n",
       "  'red': 61,\n",
       "  'vegan': 62,\n",
       "  'onion': 63,\n",
       "  'dried': 64,\n",
       "  'peperronni': 65,\n",
       "  'buffalo': 66,\n",
       "  'thick': 67,\n",
       "  'balsamic': 68,\n",
       "  'bacon': 69,\n",
       "  'bbq': 70,\n",
       "  'new': 71,\n",
       "  'barbecue': 72,\n",
       "  'tomato': 73,\n",
       "  'yorker': 74,\n",
       "  'arugula': 75,\n",
       "  'kalamata': 76,\n",
       "  'balzamic': 77,\n",
       "  'garlic': 78,\n",
       "  'tomatoes': 79,\n",
       "  'pestos': 80,\n",
       "  'feta': 81,\n",
       "  'low': 82,\n",
       "  'fat': 83,\n",
       "  'ranch': 84,\n",
       "  'pulled': 85,\n",
       "  'pork': 86,\n",
       "  'beef': 87,\n",
       "  'cherry': 88,\n",
       "  'grilled': 89,\n",
       "  'just': 90,\n",
       "  'peperoni': 91,\n",
       "  'fried': 92,\n",
       "  'pesto': 93,\n",
       "  'hot': 94,\n",
       "  'ham': 95,\n",
       "  'all': 96,\n",
       "  'hams': 97,\n",
       "  'peperroni': 98,\n",
       "  'artichoke': 99,\n",
       "  'mushrooms': 100,\n",
       "  'bay': 101,\n",
       "  'leaves': 102,\n",
       "  'combination': 103,\n",
       "  'sausage': 104,\n",
       "  'broccoli': 105,\n",
       "  'artichokes': 106,\n",
       "  'carrots': 107,\n",
       "  'peppperonis': 108,\n",
       "  'alfredo': 109,\n",
       "  'pickles': 110,\n",
       "  'the': 111,\n",
       "  'everything': 112,\n",
       "  'pineapple': 113,\n",
       "  'parmesan': 114,\n",
       "  'not': 115,\n",
       "  'applewood': 116,\n",
       "  'apple': 117,\n",
       "  'wood': 118,\n",
       "  'chickens': 119,\n",
       "  'black': 120,\n",
       "  'bean': 121,\n",
       "  'cumin': 122,\n",
       "  'powder': 123,\n",
       "  'cheddar': 124,\n",
       "  'basil': 125,\n",
       "  'ground': 126,\n",
       "  'pepperonis': 127,\n",
       "  'bacons': 128,\n",
       "  'chorizo': 129,\n",
       "  'anchovies': 130,\n",
       "  'anchovy': 131,\n",
       "  'cheeseburger': 132,\n",
       "  'mozarella': 133,\n",
       "  'beans': 134,\n",
       "  'brocoli': 135,\n",
       "  'chorrizo': 136,\n",
       "  'gluten': 137,\n",
       "  'free': 138,\n",
       "  'carrot': 139,\n",
       "  'much': 140,\n",
       "  'sourdough': 141,\n",
       "  'deepdish': 142,\n",
       "  'vegetarian': 143,\n",
       "  'pea': 144,\n",
       "  'peperonis': 145,\n",
       "  'keto': 146,\n",
       "  'napolitana': 147,\n",
       "  'thin': 148,\n",
       "  'oregano': 149,\n",
       "  'peas': 150,\n",
       "  'mushroom': 151,\n",
       "  'lovers': 152,\n",
       "  'every': 153,\n",
       "  'style': 154,\n",
       "  'pickle': 155,\n",
       "  'gluten-free': 156,\n",
       "  'lover': 157,\n",
       "  'cauliflower': 158,\n",
       "  'oil': 159,\n",
       "  'spiced': 160,\n",
       "  'margarita': 161,\n",
       "  'neapolitan': 162,\n",
       "  'chicago': 163,\n",
       "  'parsley': 164,\n",
       "  'tiny': 165,\n",
       "  'spicy': 166,\n",
       "  'lots': 167,\n",
       "  'tuna': 168,\n",
       "  'salami': 169,\n",
       "  'white': 170,\n",
       "  'spinach': 171,\n",
       "  'jalapeno': 172,\n",
       "  'veggie': 173,\n",
       "  'shrimps': 174,\n",
       "  'topping': 175,\n",
       "  'vegetables': 176,\n",
       "  'with': 177,\n",
       "  'works': 178,\n",
       "  'york': 179,\n",
       "  'margherita': 180,\n",
       "  'shrimp': 181,\n",
       "  'toppings': 182,\n",
       "  'meatlovers': 183,\n",
       "  'mexican': 184,\n",
       "  'hawaiian': 185,\n",
       "  'deep': 186,\n",
       "  'dish': 187,\n",
       "  'veggies': 188,\n",
       "  'ricotta': 189,\n",
       "  'italian': 190,\n",
       "  'only': 191,\n",
       "  'supreme': 192,\n",
       "  'mediterranean': 193,\n",
       "  'pineapples': 194,\n",
       "  'many': 195,\n",
       "  'rosemary': 196,\n",
       "  'pineaples': 197,\n",
       "  'jalapenos': 198,\n",
       "  'pineaple': 199,\n",
       "  'flake': 200,\n",
       "  'lettuce': 201,\n",
       "  'flakes': 202},\n",
       " {0: '<pad>',\n",
       "  1: '<sos>',\n",
       "  2: '<eos>',\n",
       "  3: '<unk>',\n",
       "  4: ')',\n",
       "  5: '(TOPPING',\n",
       "  6: '(ORDER',\n",
       "  7: '(PIZZAORDER',\n",
       "  8: '(SIZE',\n",
       "  9: '(NUMBER',\n",
       "  10: 'a',\n",
       "  11: 'cheese',\n",
       "  12: 'one',\n",
       "  13: 'party',\n",
       "  14: '(COMPLEX_TOPPING',\n",
       "  15: '(QUANTITY',\n",
       "  16: 'american',\n",
       "  17: 'sized',\n",
       "  18: '(STYLE',\n",
       "  19: 'size',\n",
       "  20: 'extra',\n",
       "  21: '-',\n",
       "  22: '(NOT',\n",
       "  23: 'pepper',\n",
       "  24: 'peperonni',\n",
       "  25: 'banana',\n",
       "  26: 'peppers',\n",
       "  27: 'personal',\n",
       "  28: 'green',\n",
       "  29: 'large',\n",
       "  30: 'of',\n",
       "  31: 'mozzarella',\n",
       "  32: 'lunch',\n",
       "  33: 'sauce',\n",
       "  34: 'lot',\n",
       "  35: 'pecorino',\n",
       "  36: 'yellow',\n",
       "  37: 'stuffed',\n",
       "  38: 'high',\n",
       "  39: 'rise',\n",
       "  40: 'dough',\n",
       "  41: 'crust',\n",
       "  42: 'meatball',\n",
       "  43: 'crusts',\n",
       "  44: 'medium',\n",
       "  45: 'little',\n",
       "  46: 'olive',\n",
       "  47: 'chicken',\n",
       "  48: 'regular',\n",
       "  49: 'bit',\n",
       "  50: 'peppperoni',\n",
       "  51: 'pepperoni',\n",
       "  52: 'big',\n",
       "  53: 'glaze',\n",
       "  54: 'olives',\n",
       "  55: 'roasted',\n",
       "  56: 'small',\n",
       "  57: 'meat',\n",
       "  58: 'caramelized',\n",
       "  59: 'meatlover',\n",
       "  60: 'onions',\n",
       "  61: 'red',\n",
       "  62: 'vegan',\n",
       "  63: 'onion',\n",
       "  64: 'dried',\n",
       "  65: 'peperronni',\n",
       "  66: 'buffalo',\n",
       "  67: 'thick',\n",
       "  68: 'balsamic',\n",
       "  69: 'bacon',\n",
       "  70: 'bbq',\n",
       "  71: 'new',\n",
       "  72: 'barbecue',\n",
       "  73: 'tomato',\n",
       "  74: 'yorker',\n",
       "  75: 'arugula',\n",
       "  76: 'kalamata',\n",
       "  77: 'balzamic',\n",
       "  78: 'garlic',\n",
       "  79: 'tomatoes',\n",
       "  80: 'pestos',\n",
       "  81: 'feta',\n",
       "  82: 'low',\n",
       "  83: 'fat',\n",
       "  84: 'ranch',\n",
       "  85: 'pulled',\n",
       "  86: 'pork',\n",
       "  87: 'beef',\n",
       "  88: 'cherry',\n",
       "  89: 'grilled',\n",
       "  90: 'just',\n",
       "  91: 'peperoni',\n",
       "  92: 'fried',\n",
       "  93: 'pesto',\n",
       "  94: 'hot',\n",
       "  95: 'ham',\n",
       "  96: 'all',\n",
       "  97: 'hams',\n",
       "  98: 'peperroni',\n",
       "  99: 'artichoke',\n",
       "  100: 'mushrooms',\n",
       "  101: 'bay',\n",
       "  102: 'leaves',\n",
       "  103: 'combination',\n",
       "  104: 'sausage',\n",
       "  105: 'broccoli',\n",
       "  106: 'artichokes',\n",
       "  107: 'carrots',\n",
       "  108: 'peppperonis',\n",
       "  109: 'alfredo',\n",
       "  110: 'pickles',\n",
       "  111: 'the',\n",
       "  112: 'everything',\n",
       "  113: 'pineapple',\n",
       "  114: 'parmesan',\n",
       "  115: 'not',\n",
       "  116: 'applewood',\n",
       "  117: 'apple',\n",
       "  118: 'wood',\n",
       "  119: 'chickens',\n",
       "  120: 'black',\n",
       "  121: 'bean',\n",
       "  122: 'cumin',\n",
       "  123: 'powder',\n",
       "  124: 'cheddar',\n",
       "  125: 'basil',\n",
       "  126: 'ground',\n",
       "  127: 'pepperonis',\n",
       "  128: 'bacons',\n",
       "  129: 'chorizo',\n",
       "  130: 'anchovies',\n",
       "  131: 'anchovy',\n",
       "  132: 'cheeseburger',\n",
       "  133: 'mozarella',\n",
       "  134: 'beans',\n",
       "  135: 'brocoli',\n",
       "  136: 'chorrizo',\n",
       "  137: 'gluten',\n",
       "  138: 'free',\n",
       "  139: 'carrot',\n",
       "  140: 'much',\n",
       "  141: 'sourdough',\n",
       "  142: 'deepdish',\n",
       "  143: 'vegetarian',\n",
       "  144: 'pea',\n",
       "  145: 'peperonis',\n",
       "  146: 'keto',\n",
       "  147: 'napolitana',\n",
       "  148: 'thin',\n",
       "  149: 'oregano',\n",
       "  150: 'peas',\n",
       "  151: 'mushroom',\n",
       "  152: 'lovers',\n",
       "  153: 'every',\n",
       "  154: 'style',\n",
       "  155: 'pickle',\n",
       "  156: 'gluten-free',\n",
       "  157: 'lover',\n",
       "  158: 'cauliflower',\n",
       "  159: 'oil',\n",
       "  160: 'spiced',\n",
       "  161: 'margarita',\n",
       "  162: 'neapolitan',\n",
       "  163: 'chicago',\n",
       "  164: 'parsley',\n",
       "  165: 'tiny',\n",
       "  166: 'spicy',\n",
       "  167: 'lots',\n",
       "  168: 'tuna',\n",
       "  169: 'salami',\n",
       "  170: 'white',\n",
       "  171: 'spinach',\n",
       "  172: 'jalapeno',\n",
       "  173: 'veggie',\n",
       "  174: 'shrimps',\n",
       "  175: 'topping',\n",
       "  176: 'vegetables',\n",
       "  177: 'with',\n",
       "  178: 'works',\n",
       "  179: 'york',\n",
       "  180: 'margherita',\n",
       "  181: 'shrimp',\n",
       "  182: 'toppings',\n",
       "  183: 'meatlovers',\n",
       "  184: 'mexican',\n",
       "  185: 'hawaiian',\n",
       "  186: 'deep',\n",
       "  187: 'dish',\n",
       "  188: 'veggies',\n",
       "  189: 'ricotta',\n",
       "  190: 'italian',\n",
       "  191: 'only',\n",
       "  192: 'supreme',\n",
       "  193: 'mediterranean',\n",
       "  194: 'pineapples',\n",
       "  195: 'many',\n",
       "  196: 'rosemary',\n",
       "  197: 'pineaples',\n",
       "  198: 'jalapenos',\n",
       "  199: 'pineaple',\n",
       "  200: 'flake',\n",
       "  201: 'lettuce',\n",
       "  202: 'flakes'})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_word2idx, tgt_idx2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "max_src_len = 128\n",
    "max_tgt_len = 256\n",
    "\n",
    "# Split train into train/val\n",
    "train_size = int(0.9 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_data, val_data = torch.utils.data.random_split(train_dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                            collate_fn=lambda b: collate_fn(b, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len))\n",
    "val_loader = DataLoader(val_data, batch_size=batch_size, shuffle=False, \n",
    "                        collate_fn=lambda b: collate_fn(b, src_word2idx, tgt_word2idx, max_src_len, max_tgt_len))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model hyperparameters\n",
    "input_dim = len(src_word2idx)\n",
    "output_dim = len(tgt_word2idx)\n",
    "emb_dim = 256\n",
    "hid_dim = 512\n",
    "n_layers = 2\n",
    "dropout = 0.5\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "encoder = Encoder(input_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "decoder = Decoder(output_dim, emb_dim, hid_dim, n_layers, dropout)\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Train Loss: 1.335 | Val Loss: 1.862 | Dev Exact Match: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "pop from empty list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m evaluate_loss(model, val_loader, criterion)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# Compute exact match accuracy on dev set\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     exact_match \u001b[38;5;241m=\u001b[39m exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Val Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Dev Exact Match: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexact_match\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Final evaluation on dev\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 83\u001b[0m, in \u001b[0;36mexact_match_accuracy\u001b[1;34m(model, dev_dataset, src_word2idx, tgt_word2idx, idx2tgt, max_src_len, batch_size)\u001b[0m\n\u001b[0;32m     81\u001b[0m predicted_text \u001b[38;5;241m=\u001b[39m generate_output(model, single_src, tgt_word2idx, idx2tgt)\n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Convert both predicted and reference TOP into JSON using preprocess\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m pred_json \u001b[38;5;241m=\u001b[39m preprocess(predicted_text)\n\u001b[0;32m     84\u001b[0m ref_json \u001b[38;5;241m=\u001b[39m preprocess(tgts[i])\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_json \u001b[38;5;241m==\u001b[39m ref_json:\n",
      "Cell \u001b[1;32mIn[1], line 228\u001b[0m, in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocess\u001b[39m(text):\n\u001b[0;32m    227\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m tokenize(text)\n\u001b[1;32m--> 228\u001b[0m     parsed \u001b[38;5;241m=\u001b[39m parse_tokens(tokens)\n\u001b[0;32m    229\u001b[0m     result \u001b[38;5;241m=\u001b[39m normalize_structure(parsed)\n\u001b[0;32m    230\u001b[0m     remove_type_keys(result)\n",
      "Cell \u001b[1;32mIn[1], line 19\u001b[0m, in \u001b[0;36mparse_tokens\u001b[1;34m(tokens)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m token \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     18\u001b[0m     finished \u001b[38;5;241m=\u001b[39m current_list\n\u001b[1;32m---> 19\u001b[0m     current_list \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     20\u001b[0m     current_list\u001b[38;5;241m.\u001b[39mappend(finished)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mIndexError\u001b[0m: pop from empty list"
     ]
    }
   ],
   "source": [
    "\n",
    "n_epochs = 5\n",
    "for epoch in range(n_epochs):\n",
    "    train_loss = train_model(model, train_loader, optimizer, criterion)\n",
    "    val_loss = evaluate_loss(model, val_loader, criterion)\n",
    "\n",
    "    # Compute exact match accuracy on dev set\n",
    "    exact_match = exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{n_epochs} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f} | Dev Exact Match: {exact_match:.2%}\")\n",
    "\n",
    "# Final evaluation on dev\n",
    "exact_match_final = exact_match_accuracy(model, dev_dataset, src_word2idx, tgt_word2idx, tgt_idx2word, max_src_len)\n",
    "print(\"Final Dev Exact Match Accuracy:\", exact_match_final)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
