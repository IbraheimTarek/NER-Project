{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hima\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import json\n",
    "sys.path.append(\"..\")\n",
    "from utils.data_preprocessing import preprocess_text\n",
    "from utils.feature_extraction import bag_of_words, tfidf_features, extract_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 records so far...\n",
      "Processed 20000 records so far...\n",
      "Processed 30000 records so far...\n",
      "Processed 40000 records so far...\n",
      "Processed 50000 records so far...\n",
      "Processed 60000 records so far...\n",
      "Processed 70000 records so far...\n",
      "Processed 80000 records so far...\n",
      "Processed 90000 records so far...\n",
      "Processed 100000 records so far...\n",
      "Processed 110000 records so far...\n",
      "Processed 120000 records so far...\n",
      "Processed 130000 records so far...\n",
      "Processed 140000 records so far...\n",
      "Processed 150000 records so far...\n",
      "Processed 160000 records so far...\n",
      "Processed 170000 records so far...\n",
      "Processed 180000 records so far...\n",
      "Processed 190000 records so far...\n",
      "Processed 200000 records so far...\n",
      "Processed 210000 records so far...\n",
      "Processed 220000 records so far...\n",
      "Processed 230000 records so far...\n",
      "Processed 240000 records so far...\n",
      "Processed 250000 records so far...\n",
      "Processed 260000 records so far...\n",
      "Processed 270000 records so far...\n",
      "Processed 280000 records so far...\n",
      "Processed 290000 records so far...\n",
      "Processed 300000 records so far...\n",
      "Processed 310000 records so far...\n",
      "Processed 320000 records so far...\n",
      "Processed 330000 records so far...\n",
      "Processed 340000 records so far...\n",
      "Processed 350000 records so far...\n",
      "Processed 360000 records so far...\n",
      "Processed 370000 records so far...\n",
      "Processed 380000 records so far...\n",
      "Processed 390000 records so far...\n",
      "Processed 400000 records so far...\n",
      "Processed 410000 records so far...\n",
      "Processed 420000 records so far...\n",
      "Processed 430000 records so far...\n",
      "Processed 440000 records so far...\n",
      "Processed 450000 records so far...\n",
      "Processed 460000 records so far...\n",
      "Processed 470000 records so far...\n",
      "Processed 480000 records so far...\n",
      "Processed 490000 records so far...\n",
      "Processed 500000 records so far...\n",
      "Processed 510000 records so far...\n",
      "Processed 520000 records so far...\n",
      "Processed 530000 records so far...\n",
      "Processed 540000 records so far...\n",
      "Processed 550000 records so far...\n",
      "Processed 560000 records so far...\n",
      "Processed 570000 records so far...\n",
      "Processed 580000 records so far...\n",
      "Processed 590000 records so far...\n",
      "Processed 600000 records so far...\n",
      "Processed 610000 records so far...\n",
      "Processed 620000 records so far...\n",
      "Processed 630000 records so far...\n",
      "Processed 640000 records so far...\n",
      "Processed 650000 records so far...\n",
      "Processed 660000 records so far...\n",
      "Processed 670000 records so far...\n",
      "Processed 680000 records so far...\n",
      "Processed 690000 records so far...\n",
      "Processed 700000 records so far...\n",
      "Processed 710000 records so far...\n",
      "Processed 720000 records so far...\n",
      "Processed 730000 records so far...\n",
      "Processed 740000 records so far...\n",
      "Processed 750000 records so far...\n",
      "Processed 760000 records so far...\n",
      "Processed 770000 records so far...\n",
      "Processed 780000 records so far...\n",
      "Processed 790000 records so far...\n",
      "Processed 800000 records so far...\n",
      "Processed 810000 records so far...\n",
      "Processed 820000 records so far...\n",
      "Processed 830000 records so far...\n",
      "Processed 840000 records so far...\n",
      "Processed 850000 records so far...\n",
      "Processed 860000 records so far...\n",
      "Processed 870000 records so far...\n",
      "Processed 880000 records so far...\n",
      "Processed 890000 records so far...\n",
      "Processed 900000 records so far...\n",
      "Processed 910000 records so far...\n",
      "Processed 920000 records so far...\n",
      "Processed 930000 records so far...\n",
      "Processed 940000 records so far...\n",
      "Processed 950000 records so far...\n",
      "Processed 960000 records so far...\n",
      "Processed 970000 records so far...\n",
      "Processed 980000 records so far...\n",
      "Processed 990000 records so far...\n",
      "Processed 1000000 records so far...\n",
      "Processed 1010000 records so far...\n",
      "Processed 1020000 records so far...\n",
      "Processed 1030000 records so far...\n",
      "Processed 1040000 records so far...\n",
      "Processed 1050000 records so far...\n",
      "Processed 1060000 records so far...\n",
      "Processed 1070000 records so far...\n",
      "Processed 1080000 records so far...\n",
      "Processed 1090000 records so far...\n",
      "Processed 1100000 records so far...\n",
      "Processed 1110000 records so far...\n",
      "Processed 1120000 records so far...\n",
      "Processed 1130000 records so far...\n",
      "Processed 1140000 records so far...\n",
      "Processed 1150000 records so far...\n",
      "Processed 1160000 records so far...\n",
      "Processed 1170000 records so far...\n",
      "Processed 1180000 records so far...\n",
      "Processed 1190000 records so far...\n",
      "Processed 1200000 records so far...\n",
      "Processed 1210000 records so far...\n",
      "Processed 1220000 records so far...\n",
      "Processed 1230000 records so far...\n",
      "Processed 1240000 records so far...\n",
      "Processed 1250000 records so far...\n",
      "Processed 1260000 records so far...\n",
      "Processed 1270000 records so far...\n",
      "Processed 1280000 records so far...\n",
      "Processed 1290000 records so far...\n",
      "Processed 1300000 records so far...\n",
      "Processed 1310000 records so far...\n",
      "Processed 1320000 records so far...\n",
      "Processed 1330000 records so far...\n",
      "Processed 1340000 records so far...\n",
      "Processed 1350000 records so far...\n",
      "Processed 1360000 records so far...\n",
      "Processed 1370000 records so far...\n",
      "Processed 1380000 records so far...\n",
      "Processed 1390000 records so far...\n",
      "Processed 1400000 records so far...\n",
      "Processed 1410000 records so far...\n",
      "Processed 1420000 records so far...\n",
      "Processed 1430000 records so far...\n",
      "Processed 1440000 records so far...\n",
      "Processed 1450000 records so far...\n",
      "Processed 1460000 records so far...\n",
      "Processed 1470000 records so far...\n",
      "Processed 1480000 records so far...\n",
      "Processed 1490000 records so far...\n",
      "Processed 1500000 records so far...\n",
      "Processed 1510000 records so far...\n",
      "Processed 1520000 records so far...\n",
      "Processed 1530000 records so far...\n",
      "Processed 1540000 records so far...\n",
      "Processed 1550000 records so far...\n",
      "Processed 1560000 records so far...\n",
      "Processed 1570000 records so far...\n",
      "Processed 1580000 records so far...\n",
      "Processed 1590000 records so far...\n",
      "Processed 1600000 records so far...\n",
      "Processed 1610000 records so far...\n",
      "Processed 1620000 records so far...\n",
      "Processed 1630000 records so far...\n",
      "Processed 1640000 records so far...\n",
      "Processed 1650000 records so far...\n",
      "Processed 1660000 records so far...\n",
      "Processed 1670000 records so far...\n",
      "Processed 1680000 records so far...\n",
      "Processed 1690000 records so far...\n",
      "Processed 1700000 records so far...\n",
      "Processed 1710000 records so far...\n",
      "Processed 1720000 records so far...\n",
      "Processed 1730000 records so far...\n",
      "Processed 1740000 records so far...\n",
      "Processed 1750000 records so far...\n",
      "Processed 1760000 records so far...\n",
      "Processed 1770000 records so far...\n",
      "Processed 1780000 records so far...\n",
      "Processed 1790000 records so far...\n",
      "Processed 1800000 records so far...\n",
      "Processed 1810000 records so far...\n",
      "Processed 1820000 records so far...\n",
      "Processed 1830000 records so far...\n",
      "Processed 1840000 records so far...\n",
      "Processed 1850000 records so far...\n",
      "Processed 1860000 records so far...\n",
      "Processed 1870000 records so far...\n",
      "Processed 1880000 records so far...\n",
      "Processed 1890000 records so far...\n",
      "Processed 1900000 records so far...\n",
      "Processed 1910000 records so far...\n",
      "Processed 1920000 records so far...\n",
      "Processed 1930000 records so far...\n",
      "Processed 1940000 records so far...\n",
      "Processed 1950000 records so far...\n",
      "Processed 1960000 records so far...\n",
      "Processed 1970000 records so far...\n",
      "Processed 1980000 records so far...\n",
      "Processed 1990000 records so far...\n",
      "Processed 2000000 records so far...\n",
      "Processed 2010000 records so far...\n",
      "Processed 2020000 records so far...\n",
      "Processed 2030000 records so far...\n",
      "Processed 2040000 records so far...\n",
      "Processed 2050000 records so far...\n",
      "Processed 2060000 records so far...\n",
      "Processed 2070000 records so far...\n",
      "Processed 2080000 records so far...\n",
      "Processed 2090000 records so far...\n",
      "Processed 2100000 records so far...\n",
      "Processed 2110000 records so far...\n",
      "Processed 2120000 records so far...\n",
      "Processed 2130000 records so far...\n",
      "Processed 2140000 records so far...\n",
      "Processed 2150000 records so far...\n",
      "Processed 2160000 records so far...\n",
      "Processed 2170000 records so far...\n",
      "Processed 2180000 records so far...\n",
      "Processed 2190000 records so far...\n",
      "Processed 2200000 records so far...\n",
      "Processed 2210000 records so far...\n",
      "Processed 2220000 records so far...\n",
      "Processed 2230000 records so far...\n",
      "Processed 2240000 records so far...\n",
      "Processed 2250000 records so far...\n",
      "Processed 2260000 records so far...\n",
      "Processed 2270000 records so far...\n",
      "Processed 2280000 records so far...\n",
      "Processed 2290000 records so far...\n",
      "Processed 2300000 records so far...\n",
      "Processed 2310000 records so far...\n",
      "Processed 2320000 records so far...\n",
      "Processed 2330000 records so far...\n",
      "Processed 2340000 records so far...\n",
      "Processed 2350000 records so far...\n",
      "Processed 2360000 records so far...\n",
      "Processed 2370000 records so far...\n",
      "Processed 2380000 records so far...\n",
      "Processed 2390000 records so far...\n",
      "Processed 2400000 records so far...\n",
      "Processed 2410000 records so far...\n",
      "Processed 2420000 records so far...\n",
      "Processed 2430000 records so far...\n",
      "Processed 2440000 records so far...\n",
      "Processed 2450000 records so far...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_path = '../dataset/PIZZA_train.json'\n",
    "test_path = '../dataset/PIZZA_dev.json'\n",
    "def read_data(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                record = json.loads(line.strip())\n",
    "                data.append(record)\n",
    "                \n",
    "                # Process in chunks of 10,000 records\n",
    "                if i > 0 and i % 10000 == 0:\n",
    "                    print(f\"Processed {i} records so far...\")\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    return data\n",
    "\n",
    "# Convert remaining data to DataFrame\n",
    "data = read_data(train_path)\n",
    "if data:\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "data = read_data(test_path)\n",
    "if data:\n",
    "    dev = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2456446\n",
      "i'd like a pizza with hot pepper pecorino cheese and parmesan without thin crust\n",
      "(ORDER (PIZZAORDER (NUMBER 1 ) (TOPPING HOT_PEPPERS ) (TOPPING PECORINO_CHEESE ) (TOPPING PARMESAN_CHEESE ) (NOT (STYLE THIN_CRUST ) ) ) )\n",
      "i want to order two medium pizzas with sausage and black olives and two medium pizzas with pepperoni and extra cheese and three large pizzas with pepperoni and sausage\n"
     ]
    }
   ],
   "source": [
    "X_train = df['train.SRC']\n",
    "y_train = df['train.EXR']\n",
    "X_test = dev['dev.SRC']\n",
    "y_test = dev['dev.EXR']\n",
    "print(len(df))\n",
    "print(X_train[2456445])\n",
    "print(y_train[2456445])\n",
    "print(dev['dev.SRC'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pizza hot pepper pecoricheese parmesan without thin crust\n"
     ]
    }
   ],
   "source": [
    "X_train = [\" \".join(preprocess_text(text)) for text in X_train]\n",
    "X_test = [\" \".join(preprocess_text(text)) for text in X_test]\n",
    "print(X_train[2456445])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define model constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### not prepared encoder\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, input_dim, embedding_dim, hidden_dim):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "#         self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "    \n",
    "#     def forward(self, src):\n",
    "#         embedded = self.embedding(src)  # Shape: [batch_size, seq_len, embedding_dim]\n",
    "#         outputs, hidden = self.rnn(embedded)  # Shape: [batch_size, seq_len, hidden_dim]\n",
    "#         return hidden  # Hidden state passed to the decoder\n",
    "    \n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.fc = nn.Linear(input_dim, hidden_dim)  # Fully connected to map features to hidden space\n",
    "        self.rnn = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    def forward(self, src):\n",
    "        src = src.unsqueeze(1)  # Add sequence dimension (batch_size, seq_len=1, input_dim)\n",
    "        embedded = self.fc(src)  # (batch_size, seq_len=1, hidden_dim)\n",
    "        outputs, hidden = self.rnn(embedded)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, input, hidden):\n",
    "        input = input.unsqueeze(1)  # Add sequence dimension (for single token)\n",
    "        embedded = self.embedding(input)  # Shape: [batch_size, 1, embedding_dim]\n",
    "        output, hidden = self.rnn(embedded, hidden)  # Shape: [batch_size, 1, hidden_dim]\n",
    "        prediction = self.fc(output.squeeze(1))  # Shape: [batch_size, output_dim]\n",
    "        return prediction, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seq2Seq model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.fc.out_features\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "        hidden = self.encoder(src)\n",
    "\n",
    "        # First input to the decoder is the <sos> token\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden = self.decoder(input, hidden)\n",
    "            outputs[:, t] = output\n",
    "            # Decide whether to use teacher forcing\n",
    "            teacher_force = np.random.random() < teacher_forcing_ratio\n",
    "            top1 = output.argmax(1)\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop in pytorch as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch in iterator:\n",
    "        src = batch[\"src\"].to(DEVICE)\n",
    "        trg = batch[\"trg\"].to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # trg: [batch_size, trg_len]\n",
    "        # output: [batch_size, trg_len, output_dim]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[:, 1:].reshape(-1, output_dim)\n",
    "        trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eavluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            src = batch[\"src\"].to(DEVICE)\n",
    "            trg = batch[\"trg\"].to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, 0)  # Turn off teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[:, 1:].reshape(-1, output_dim)\n",
    "            trg = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def tokenize_output(output):\n",
    "    \"\"\"\n",
    "    Tokenizes the structured output into meaningful tokens.\n",
    "    Example:\n",
    "        Input: \"(ORDER (PIZZAORDER (NUMBER a ) (SIZE large ) (TOPPING bbq pulled pork ) ) )\"\n",
    "        Output: [\"(ORDER\", \"(PIZZAORDER\", \"(NUMBER\", \"a\", \"(SIZE\", \"large\", \"(TOPPING\", \"bbq\", \"pulled\", \"pork\", \")\", \")\", \")\", \")\"]\n",
    "    \"\"\"\n",
    "    tokens = re.findall(r\"\\(|\\)|\\w+|[^\\s()]+\", output)\n",
    "    return tokens\n",
    "\n",
    "def build_vocab(outputs):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from tokenized outputs.\n",
    "    \"\"\"\n",
    "    vocab = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2}  # Special tokens\n",
    "    i = 2\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = i\n",
    "                i += 1\n",
    "    return vocab\n",
    "def encode_outputs(outputs, vocab):\n",
    "    \"\"\"\n",
    "    Encodes tokenized outputs into sequences of integers.\n",
    "    \"\"\"\n",
    "    encoded = []\n",
    "    for output in outputs:\n",
    "        tokens = tokenize_output(output)\n",
    "        sequence = [vocab[\"<SOS>\"]] + [vocab[token] for token in tokens if token in vocab] + [vocab[\"<EOS>\"]]\n",
    "        encoded.append(sequence)\n",
    "    return encoded\n",
    "\n",
    "def pad_sequences_to_fixed_length(sequences, max_len):\n",
    "    \"\"\"\n",
    "    Pads sequences to a fixed length.\n",
    "    \"\"\"\n",
    "    return pad_sequences(sequences, maxlen=max_len, padding=\"post\", value=0)\n",
    "\n",
    "def decode_sequence(sequence, vocab):\n",
    "    \"\"\"\n",
    "    Decodes a sequence of integers back into the structured output string.\n",
    "    \"\"\"\n",
    "    inv_vocab = {v: k for k, v in vocab.items()}  # Reverse the vocabulary\n",
    "    tokens = [inv_vocab[idx] for idx in sequence if idx > 0]  # Ignore <PAD> tokens\n",
    "    return \" \".join(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prep Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def prepare_data(\n",
    "    X_train, y_train, X_test, y_test, feature_type=\"bow\", glove_vectors=None, max_len=20\n",
    "):\n",
    "\n",
    "    vectorizer = None\n",
    "\n",
    "    # Feature Extraction for X_train and X_test\n",
    "    if feature_type == \"bow\":\n",
    "        X_train_processed, vectorizer = bag_of_words(X_train)\n",
    "        X_test_processed = vectorizer.transform(X_test).toarray()\n",
    "    elif feature_type == \"tfidf\":\n",
    "        X_train_processed, vectorizer = tfidf_features(X_train)\n",
    "        X_test_processed = vectorizer.transform(X_test).toarray()\n",
    "    elif feature_type == \"embeddings\":\n",
    "        if not glove_vectors:\n",
    "            raise ValueError(\"GloVe vectors must be provided for embeddings.\")\n",
    "        X_train_tokenized = [sentence.split() for sentence in X_train]\n",
    "        X_test_tokenized = [sentence.split() for sentence in X_test]\n",
    "        X_train_processed = extract_embeddings(X_train_tokenized)\n",
    "        X_test_processed = extract_embeddings(X_test_tokenized)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid feature type. Choose 'bow', 'tfidf', or 'embeddings'.\")\n",
    "\n",
    "    vocab = build_vocab(y_train)  # Build vocabulary from training outputs\n",
    "    y_train_encoded = encode_outputs(y_train, vocab)  # Encode training outputs\n",
    "    y_test_encoded = encode_outputs(y_test, vocab)  # Encode testing outputs\n",
    "    y_train_processed = pad_sequences_to_fixed_length(y_train_encoded, max_len)\n",
    "    y_test_processed = pad_sequences_to_fixed_length(y_test_encoded, max_len)\n",
    "\n",
    "\n",
    "    return (\n",
    "        X_train_processed,\n",
    "        X_test_processed,\n",
    "        y_train_processed,\n",
    "        y_test_processed,\n",
    "        vectorizer,\n",
    "        vocab,  # Return vocabulary for decoding\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save and load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path):\n",
    "    torch.save(model.state_dict(), path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "# Load the model\n",
    "def load_model(model, path):\n",
    "    model.load_state_dict(torch.load(path))\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create dataset and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset and dataloader\n",
    "class Seq2SeqDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, src_data, trg_data):\n",
    "        self.src_data = src_data\n",
    "        self.trg_data = trg_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {\n",
    "            \"src\": torch.tensor(self.src_data[index], dtype=torch.float32),\n",
    "            \"trg\": torch.tensor(self.trg_data[index], dtype=torch.long),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_processed, X_test_processed, y_train_processed, y_test_processed, vectorizer, vocab = prepare_data( X_train, y_train, X_test, y_test, feature_type=\"bow\", max_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '(': 2,\n",
       " 'ORDER': 3,\n",
       " 'PIZZAORDER': 4,\n",
       " 'NUMBER': 5,\n",
       " '1': 6,\n",
       " ')': 7,\n",
       " 'SIZE': 8,\n",
       " 'LARGE': 9,\n",
       " 'TOPPING': 10,\n",
       " 'BBQ_PULLED_PORK': 11,\n",
       " 'GREEN_PEPPERS': 12,\n",
       " 'COMPLEX_TOPPING': 13,\n",
       " 'QUANTITY': 14,\n",
       " 'EXTRA': 15,\n",
       " 'PEPPERONI': 16,\n",
       " 'STYLE': 17,\n",
       " 'VEGETARIAN': 18,\n",
       " 'PARTY_SIZE': 19,\n",
       " 'STUFFED_CRUST': 20,\n",
       " 'AMERICAN_CHEESE': 21,\n",
       " 'MUSHROOMS': 22,\n",
       " 'PERSONAL_SIZE': 23,\n",
       " 'ARTICHOKES': 24,\n",
       " 'BANANA_PEPPERS': 25,\n",
       " 'LOW_FAT_CHEESE': 26,\n",
       " 'REGULARSIZE': 27,\n",
       " 'NOT': 28,\n",
       " 'FRIED_ONIONS': 29,\n",
       " 'LIGHT': 30,\n",
       " 'THICK_CRUST': 31,\n",
       " 'GREEN_OLIVES': 32,\n",
       " 'PESTO': 33,\n",
       " 'YELLOW_PEPPERS': 34,\n",
       " 'MEATBALLS': 35,\n",
       " 'BEANS': 36,\n",
       " 'MEAT_LOVER': 37,\n",
       " 'PECORINO_CHEESE': 38,\n",
       " 'BALSAMIC_GLAZE': 39,\n",
       " 'OLIVES': 40,\n",
       " 'CHICKEN': 41,\n",
       " 'MOZZARELLA_CHEESE': 42,\n",
       " 'SAUCE': 43,\n",
       " 'ITALIAN_SAUSAGE': 44,\n",
       " 'LUNCH_SIZE': 45,\n",
       " 'ALFREDO_CHICKEN': 46,\n",
       " 'CHEESEBURGER': 47,\n",
       " 'COMBINATION': 48,\n",
       " 'SPICED_SAUSAGE': 49,\n",
       " 'MEDITERRANEAN': 50,\n",
       " 'CARAMELIZED_ONIONS': 51,\n",
       " 'BACON': 52,\n",
       " 'CHORIZO': 53,\n",
       " 'VEGAN_PEPPERONI': 54,\n",
       " 'CHEESE': 55,\n",
       " 'BASIL': 56,\n",
       " 'FETA_CHEESE': 57,\n",
       " 'MEDIUM': 58,\n",
       " 'CUMIN': 59,\n",
       " 'PICKLES': 60,\n",
       " 'SMALL': 61,\n",
       " 'ANCHOVIES': 62,\n",
       " 'BUFFALO_CHICKEN': 63,\n",
       " 'HOT_PEPPERS': 64,\n",
       " 'CARROTS': 65,\n",
       " 'DRIED_PEPPERS': 66,\n",
       " 'ONIONS': 67,\n",
       " 'PARMESAN_CHEESE': 68,\n",
       " 'GLUTEN_FREE_CRUST': 69,\n",
       " 'GARLIC_POWDER': 70,\n",
       " 'BROCCOLI': 71,\n",
       " 'GRILLED_CHICKEN': 72,\n",
       " 'DEEP_DISH': 73,\n",
       " 'BUFFALO_MOZZARELLA': 74,\n",
       " 'ALL_TOPPINGS': 75,\n",
       " 'ARUGULA': 76,\n",
       " 'GRILLED_PINEAPPLE': 77,\n",
       " 'KALAMATA_OLIVES': 78,\n",
       " 'BEEF': 79,\n",
       " 'BBQ_SAUCE': 80,\n",
       " 'NEW_YORK_STYLE': 81,\n",
       " 'VEGAN': 82,\n",
       " 'PINEAPPLE': 83,\n",
       " 'TOMATOES': 84,\n",
       " 'HAM': 85,\n",
       " 'BBQ_CHICKEN': 86,\n",
       " 'MARGHERITA': 87,\n",
       " 'CHEDDAR_CHEESE': 88,\n",
       " 'RED_PEPPER_FLAKES': 89,\n",
       " 'WHITE_ONIONS': 90,\n",
       " 'BAY_LEAVES': 91,\n",
       " 'EXTRA_LARGE': 92,\n",
       " 'BUFFALO_SAUCE': 93,\n",
       " 'ROASTED_RED_PEPPERS': 94,\n",
       " 'CAULIFLOWER_CRUST': 95,\n",
       " 'LETTUCE': 96,\n",
       " 'SALAMI': 97,\n",
       " 'ROASTED_GREEN_PEPPERS': 98,\n",
       " 'CHEESE_LOVER': 99,\n",
       " 'NEAPOLITAN': 100,\n",
       " 'DRIED_TOMATOES': 101,\n",
       " 'SOURDOUGH_CRUST': 102,\n",
       " 'OREGANO': 103,\n",
       " 'SAUSAGE': 104,\n",
       " 'RANCH_SAUCE': 105,\n",
       " 'ROASTED_CHICKEN': 106,\n",
       " 'PEAS': 107,\n",
       " 'ALL_VEGETABLES': 108,\n",
       " 'ROASTED_TOMATOES': 109,\n",
       " 'CHERRY_TOMATOES': 110,\n",
       " 'JALAPENO_PEPPERS': 111,\n",
       " 'CHICAGO_STYLE': 112,\n",
       " 'HAWAIIAN': 113,\n",
       " 'TOMATO_SAUCE': 114,\n",
       " 'THIN_CRUST': 115,\n",
       " 'SPICY_RED_SAUCE': 116,\n",
       " 'SUPREME': 117,\n",
       " 'TUNA': 118,\n",
       " 'SHRIMPS': 119,\n",
       " 'PEPPERS': 120,\n",
       " 'RED_ONIONS': 121,\n",
       " 'PARSLEY': 122,\n",
       " 'ROSEMARY': 123,\n",
       " 'KETO_CRUST': 124,\n",
       " 'RICOTTA_CHEESE': 125,\n",
       " 'ROASTED_GARLIC': 126,\n",
       " 'MEXICAN': 127,\n",
       " 'SPINACH': 128,\n",
       " 'OLIVE_OIL': 129,\n",
       " 'RED_PEPPERS': 130,\n",
       " 'ROASTED_PEPPERS': 131,\n",
       " '2': 132,\n",
       " '3': 133,\n",
       " '4': 134,\n",
       " '5': 135,\n",
       " 'DRINKORDER': 136,\n",
       " 'VOLUME': 137,\n",
       " 'LITER': 138,\n",
       " 'DRINKTYPE': 139,\n",
       " 'ICE_TEA': 140,\n",
       " 'CONTAINERTYPE': 141,\n",
       " 'CAN': 142,\n",
       " '500': 143,\n",
       " 'ML': 144,\n",
       " 'SEVEN_UP': 145,\n",
       " 'BOTTLE': 146,\n",
       " '20': 147,\n",
       " 'FLOZ': 148,\n",
       " 'COKE': 149,\n",
       " '200': 150,\n",
       " 'MOUNTAIN_DEW': 151,\n",
       " 'LEMON_ICE_TEA': 152,\n",
       " 'DR_PEPPER': 153,\n",
       " 'SPRITE': 154,\n",
       " 'DIET_ICE_TEA': 155,\n",
       " 'COFFEE': 156,\n",
       " '8': 157,\n",
       " 'OZ': 158,\n",
       " 'PEPSI': 159,\n",
       " 'PELLEGRINO_SPARKLING_WATER': 160,\n",
       " 'DIET_COKE': 161,\n",
       " 'PINEAPPLE_SODA': 162,\n",
       " 'CHERRY_COKE': 163,\n",
       " '16': 164,\n",
       " 'FANTA': 165,\n",
       " 'CHERRY_PEPSI': 166,\n",
       " '12': 167,\n",
       " 'COKE_ZERO': 168,\n",
       " 'DIET_PEPSI': 169,\n",
       " 'DIET_SPRITE': 170,\n",
       " 'PERRIER_SPARKLING_WATER': 171,\n",
       " 'WATER': 172,\n",
       " 'GINGER_ALE': 173,\n",
       " '9': 174,\n",
       " '13': 175,\n",
       " '6': 176,\n",
       " '10': 177,\n",
       " '14': 178,\n",
       " '11': 179,\n",
       " '7': 180,\n",
       " '15': 181}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, ..., 0, 0, 0],\n",
       "       [1, 2, 3, ..., 0, 0, 0],\n",
       "       [1, 2, 3, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 2, 3, ..., 0, 0, 0],\n",
       "       [1, 2, 3, ..., 0, 0, 0],\n",
       "       [1, 2, 3, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2456446, 50)\n",
      "(2456446, 793)\n"
     ]
    }
   ],
   "source": [
    "print(y_train_processed.shape)\n",
    "print(X_train_processed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse\n",
    "import torch\n",
    "\n",
    "# Ensure X_train_processed and X_test_processed are dense\n",
    "if scipy.sparse.issparse(X_train_processed):\n",
    "    X_train_processed = X_train_processed.toarray()\n",
    "if scipy.sparse.issparse(X_test_processed):\n",
    "    X_test_processed = X_test_processed.toarray()\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_processed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_processed, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test_processed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_processed, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(793, 50)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_processed.shape[1] ,y_train_processed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Encoder.__init__() takes 3 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m dataloader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Model, optimizer, and loss\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m encoder \u001b[38;5;241m=\u001b[39m Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     12\u001b[0m decoder \u001b[38;5;241m=\u001b[39m Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Seq2Seq(encoder, decoder, DEVICE)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "\u001b[1;31mTypeError\u001b[0m: Encoder.__init__() takes 3 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "INPUT_DIM = X_train_processed.shape[1]  # Vocabulary size for input\n",
    "OUTPUT_DIM = y_train_processed.shape[1]  # Vocabulary size for output\n",
    "EMBEDDING_DIM = 512  # Dimension of word embeddings\n",
    "HIDDEN_DIM = 1024  # Hidden state size\n",
    "\n",
    "# Create dataset and dataloader\n",
    "dataset = Seq2SeqDataset(X_train_processed, y_train_processed)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Model, optimizer, and loss\n",
    "encoder = Encoder(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
    "decoder = Decoder(OUTPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM).to(DEVICE)\n",
    "model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "N_EPOCHS = 10\n",
    "CLIP = 1\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss = train(model, dataloader, optimizer, criterion, CLIP)\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "save_model(model, \"../weights/transformer.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the model again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model for inference\n",
    "loaded_model = Seq2Seq(encoder, decoder, DEVICE).to(DEVICE)\n",
    "loaded_model = load_model(loaded_model, \"../weights/transformer.pt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
